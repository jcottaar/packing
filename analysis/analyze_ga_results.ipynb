{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Statistical Analysis of GA Hyperparameter Experiments\n",
    "\n",
    "This notebook analyzes results from multiple GA runs with different hyperparameters to identify which settings work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration - Set these parameters\n",
    "# ============================================================\n",
    "\n",
    "# Which runner configuration to analyze\n",
    "runner_name = 'Baseline'  # e.g., 'Baseline', 'PopSize+Generations', 'CostWeights', etc.\n",
    "\n",
    "# Was fast_mode used?\n",
    "fast_mode = False  # Set to True if analyzing fast_mode results\n",
    "\n",
    "# Results directory\n",
    "results_dir = '../../results/many_ga/abbr/'\n",
    "\n",
    "# Generations to analyze\n",
    "generations_to_analyze = [500, 1000, 1500] if not fast_mode else [2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../core')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import dill\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "def compute_pvalue(df, modifier, outcome_var='cost_final'):\n",
    "    \"\"\"\n",
    "    Compute p-value for association between modifier and outcome using linear regression.\n",
    "    \n",
    "    This uses ordinary least squares regression and returns the p-value for the slope coefficient.\n",
    "    For binary variables, this is equivalent to a t-test.\n",
    "    \n",
    "    Returns: (p_value, test_name)\n",
    "    \"\"\"\n",
    "    df_clean = df[[modifier, outcome_var]].dropna()\n",
    "    \n",
    "    # Need at least 3 samples for meaningful statistics\n",
    "    if len(df_clean) < 3:\n",
    "        return np.nan, 'insufficient_data'\n",
    "    \n",
    "    # Check if there's any variation in the modifier\n",
    "    if df_clean[modifier].nunique() <= 1:\n",
    "        return np.nan, 'no_variation'\n",
    "    \n",
    "    try:\n",
    "        # Convert to numeric if boolean\n",
    "        X = pd.to_numeric(df_clean[modifier], errors='coerce')\n",
    "        y = df_clean[outcome_var]\n",
    "        \n",
    "        # Remove any NaN that resulted from conversion\n",
    "        mask = ~(X.isna() | y.isna())\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "        \n",
    "        if len(X) < 3:\n",
    "            return np.nan, 'insufficient_data'\n",
    "        \n",
    "        # Perform linear regression using scipy\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(X, y)\n",
    "        \n",
    "        return p_value, 'LinReg'\n",
    "        \n",
    "    except Exception as e:\n",
    "        return np.nan, 'error'\n",
    "\n",
    "def format_pvalue(p_val):\n",
    "    \"\"\"Format p-value for display\"\"\"\n",
    "    if np.isnan(p_val):\n",
    "        return 'p=N/A'\n",
    "    elif p_val < 0.001:\n",
    "        return 'p<0.001***'\n",
    "    elif p_val < 0.01:\n",
    "        return f'p={p_val:.3f}**'\n",
    "    elif p_val < 0.05:\n",
    "        return f'p={p_val:.3f}*'\n",
    "    else:\n",
    "        return f'p={p_val:.3f}'\n",
    "\n",
    "def plot_modifier_vs_outcome(ax, df, modifier, outcome_var, show_pval=True):\n",
    "    \"\"\"\n",
    "    Plot modifier vs outcome variable (cost or runtime).\n",
    "    Uses box plot for binary variables, scatter plot otherwise.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib axis\n",
    "    df : DataFrame\n",
    "    modifier : str - column name of modifier/hyperparameter\n",
    "    outcome_var : str - column name of outcome (e.g., 'cost_final', 'runtime')\n",
    "    show_pval : bool - whether to show p-value in title\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    p_val : float - p-value for the association\n",
    "    \"\"\"\n",
    "    is_numeric = pd.api.types.is_numeric_dtype(df[modifier])\n",
    "    is_boolean = df[modifier].dtype == bool or set(df[modifier].unique()).issubset({True, False, np.nan})\n",
    "    n_unique = df[modifier].nunique()\n",
    "    \n",
    "    df_plot = df[[modifier, outcome_var]].dropna()\n",
    "    \n",
    "    if len(df_plot) == 0:\n",
    "        ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "        return np.nan\n",
    "    \n",
    "    # Compute p-value using linear regression\n",
    "    p_val, test_name = compute_pvalue(df, modifier, outcome_var)\n",
    "    \n",
    "    # Use box plot only for binary variables\n",
    "    if n_unique == 2:\n",
    "        sns.boxplot(data=df_plot, x=modifier, y=outcome_var, ax=ax)\n",
    "        ax.set_xlabel(modifier, fontsize=9)\n",
    "    else:\n",
    "        # Use scatter plot for everything else\n",
    "        ax.scatter(df_plot[modifier], df_plot[outcome_var], alpha=0.6, s=50)\n",
    "        \n",
    "        # Add trend line if numeric with >2 unique values\n",
    "        if is_numeric and len(df_plot) > 1 and n_unique > 2:\n",
    "            z = np.polyfit(df_plot[modifier], df_plot[outcome_var], 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_line = np.linspace(df_plot[modifier].min(), df_plot[modifier].max(), 100)\n",
    "            ax.plot(x_line, p(x_line), \"r--\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel(modifier, fontsize=9)\n",
    "    \n",
    "    ax.set_ylabel(outcome_var.replace('_', ' ').title(), fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add p-value to title if requested\n",
    "    if show_pval:\n",
    "        title = f'{format_pvalue(p_val)}'\n",
    "        ax.set_title(title, fontsize=9)\n",
    "    \n",
    "    return p_val\n",
    "\n",
    "\n",
    "def analyze_outcome_vs_hyperparameters(df, outcome_var, outcome_name, modifier_cols):\n",
    "    \"\"\"\n",
    "    Unified function to analyze how hyperparameters affect an outcome (cost, score, or runtime).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame containing results\n",
    "    outcome_var : str - column name of outcome variable (e.g., 'cost_final', 'score_final', 'runtime')\n",
    "    outcome_name : str - human-readable name for the outcome (e.g., 'Score', 'Runtime')\n",
    "    modifier_cols : list - hyperparameter column names to analyze\n",
    "    \"\"\"\n",
    "    if len(df) == 0 or outcome_var not in df.columns or df[outcome_var].isna().all():\n",
    "        print(f\"No {outcome_name.lower()} data available for analysis\")\n",
    "        return\n",
    "    \n",
    "    # Filter out modifiers with no variation\n",
    "    modifier_cols = [col for col in modifier_cols if df[col].nunique() > 1]\n",
    "    \n",
    "    if len(modifier_cols) == 0:\n",
    "        print(f\"No hyperparameters with variation to analyze against {outcome_name.lower()}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nAnalyzing {len(modifier_cols)} hyperparameters vs {outcome_name}...\\n\")\n",
    "    \n",
    "    # Create grid of plots\n",
    "    n_cols = min(3, len(modifier_cols))\n",
    "    n_rows = (len(modifier_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_rows == 1 or n_cols == 1:\n",
    "        axes = axes.reshape(n_rows, n_cols)\n",
    "    \n",
    "    fig.suptitle(f'{outcome_name} vs Hyperparameters', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, modifier in enumerate(modifier_cols):\n",
    "        row = idx // n_cols\n",
    "        col = idx % n_cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Use helper function to create plot\n",
    "        p_val = plot_modifier_vs_outcome(ax, df, modifier, outcome_var, show_pval=True)\n",
    "        \n",
    "        # Update title to include modifier name\n",
    "        current_title = ax.get_title()\n",
    "        ax.set_title(f'{modifier}\\n{current_title}', fontsize=9)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(modifier_cols), n_rows * n_cols):\n",
    "        row = idx // n_cols\n",
    "        col = idx % n_cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"HYPERPARAMETER ASSOCIATIONS WITH {outcome_name.upper()}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for modifier in modifier_cols:\n",
    "        is_numeric = pd.api.types.is_numeric_dtype(df[modifier])\n",
    "        is_boolean = df[modifier].dtype == bool or set(df[modifier].unique()).issubset({True, False, np.nan})\n",
    "        n_unique = df[modifier].nunique()\n",
    "        \n",
    "        if is_numeric and not is_boolean and n_unique > 2:\n",
    "            corr = df[[modifier, outcome_var]].corr().iloc[0, 1]\n",
    "            p_val, test_name = compute_pvalue(df, modifier, outcome_var)\n",
    "            print(f\"\\n{modifier}:\")\n",
    "            print(f\"  Correlation: {corr:+.3f} ({format_pvalue(p_val)})\")\n",
    "        elif n_unique >= 2:\n",
    "            p_val, test_name = compute_pvalue(df, modifier, outcome_var)\n",
    "            print(f\"\\n{modifier}:\")\n",
    "            print(f\"  Association: {format_pvalue(p_val)} [{test_name}]\")\n",
    "\n",
    "print(\"Imports and helper functions complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_header",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all result files matching the runner name\n",
    "pattern = f\"{results_dir}/{runner_name}_*_*.pkl\"\n",
    "result_files = sorted(glob.glob(pattern))\n",
    "#result_files = [r for r in result_files if '_3' in r]\n",
    "\n",
    "print(f\"Found {len(result_files)} result files for '{runner_name}'\")\n",
    "print(f\"Pattern: {pattern}\")\n",
    "\n",
    "if len(result_files) == 0:\n",
    "    print(\"\\nNo files found! Available files:\")\n",
    "    all_files = glob.glob(f\"{results_dir}/*.pkl\")\n",
    "    for f in all_files[:10]:\n",
    "        print(f\"  {Path(f).name}\")\n",
    "    if len(all_files) > 10:\n",
    "        print(f\"  ... and {len(all_files)-10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834b3ef-d7e4-44b2-b59e-fb00a42b4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load all results and extract data\n",
    "data_records = []\n",
    "skipped_fast = 0\n",
    "skipped_slow = 0\n",
    "\n",
    "for filepath in result_files:\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            r = dill.load(f)\n",
    "        \n",
    "        # Skip if exception occurred\n",
    "        if r.exception is not None:\n",
    "            print(f\"Skipping {Path(filepath).name} - exception occurred\")\n",
    "            continue\n",
    "        \n",
    "        # Skip if no results\n",
    "        if r.best_costs is None:\n",
    "            print(f\"Skipping {Path(filepath).name} - no best_costs\")\n",
    "            continue\n",
    "\n",
    "        # Filter by fast_mode based on number of generations\n",
    "        n_gens = r.best_costs.shape[0]\n",
    "        is_fast_mode_result = n_gens < 100  # Fast mode typically has fewer generations\n",
    "        if fast_mode and not is_fast_mode_result:\n",
    "            skipped_slow += 1\n",
    "            continue  # Skip slow mode results when analyzing fast mode\n",
    "        if not fast_mode and is_fast_mode_result:\n",
    "            skipped_fast += 1\n",
    "            continue  # Skip fast mode results when analyzing slow mode\n",
    "\n",
    "        # Extract basic info\n",
    "        record = {\n",
    "            'seed': r.seed,\n",
    "            'runtime': r.runtime_seconds if hasattr(r, 'runtime_seconds') else np.nan,\n",
    "            'filename': Path(filepath).name,\n",
    "            'n_generations': n_gens\n",
    "        }\n",
    "        \n",
    "        # Extract modifier values (hyperparameters)\n",
    "        for key, value in r.modifier_values.items():\n",
    "            if key != 'seed':\n",
    "                record[key] = value\n",
    "        \n",
    "        # Extract costs at different generations\n",
    "        # r.best_costs shape: (n_generations, n_configs)\n",
    "        n_configs = r.best_costs.shape[1]\n",
    "        for gen in generations_to_analyze:\n",
    "            if gen < n_gens:\n",
    "                # Average cost across all configs\n",
    "                record[f'cost_gen_{gen}'] = np.mean(r.best_costs[gen, :])\n",
    "                # Also store per-config costs\n",
    "                for i_config in range(n_configs):\n",
    "                    record[f'cost_gen_{gen}_config_{i_config}'] = r.best_costs[gen, i_config]\n",
    "        \n",
    "        # Final cost\n",
    "        record['cost_final'] = np.mean(r.best_costs[-1, :])\n",
    "        record['cost_final_std'] = np.std(r.best_costs[-1, :])\n",
    "\n",
    "        # Extract scores from champions if available (pack_ga2 structure)\n",
    "        if hasattr(r.result_ga, 'ga') and hasattr(r.result_ga.ga, 'champions') and r.result_ga.ga.champions is not None:\n",
    "            champions = r.result_ga.ga.champions\n",
    "            if len(champions) > 0:\n",
    "                # Extract scores from champions (each champion has fitness tuple)\n",
    "                scores = []\n",
    "                for champion in champions:\n",
    "                    if hasattr(champion, 'fitness') and champion.fitness is not None and len(champion.fitness) > 0:\n",
    "                        # Fitness is (N_solutions, N_components) - take first solution\n",
    "                        champion_fitness = champion.fitness[0]\n",
    "                        # For fixed_h mode, fitness[0] is h value, fitness[1] is cost\n",
    "                        # For non-fixed_h mode, fitness[0] is cost\n",
    "                        # Score = area per tree = (h^2 / N_trees) or extract from fitness\n",
    "                        if hasattr(champion.phenotype, 'use_fixed_h') and champion.phenotype.use_fixed_h:\n",
    "                            h_val = float(champion_fitness[0])\n",
    "                        else:\n",
    "                            # Estimate h from phenotype\n",
    "                            h_val = float(champion.phenotype.h[0, 0].get())\n",
    "                        \n",
    "                        # Calculate score as h^2 / N_trees\n",
    "                        n_trees = champion.phenotype.N_trees\n",
    "                        score = (h_val ** 2) / n_trees\n",
    "                        scores.append(score)\n",
    "                \n",
    "                # Store average and std of scores\n",
    "                if len(scores) > 0:\n",
    "                    record['score_final'] = np.mean(scores)\n",
    "                    record['score_final_std'] = np.std(scores) if len(scores) > 1 else 0.0\n",
    "                    # Store individual scores\n",
    "                    for i_config, score in enumerate(scores):\n",
    "                        record[f'score_config_{i_config}'] = score\n",
    "                else:\n",
    "                    record['score_final'] = np.nan\n",
    "                    record['score_final_std'] = np.nan\n",
    "            else:\n",
    "                record['score_final'] = np.nan\n",
    "                record['score_final_std'] = np.nan\n",
    "        else:\n",
    "            record['score_final'] = np.nan\n",
    "            record['score_final_std'] = np.nan\n",
    "\n",
    "        data_records.append(record)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {Path(filepath).name}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_records)\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(df)} runs\")\n",
    "if skipped_fast > 0:\n",
    "    print(f\"Skipped {skipped_fast} fast-mode runs (analyzing slow mode)\")\n",
    "if skipped_slow > 0:\n",
    "    print(f\"Skipped {skipped_slow} slow-mode runs (analyzing fast mode)\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "if len(df) > 0:\n",
    "    print(f\"Generation range: {df['n_generations'].min()}-{df['n_generations'].max()}\")\n",
    "    \n",
    "    # Show score statistics if available\n",
    "    if 'score_final' in df.columns and not df['score_final'].isna().all():\n",
    "        print(f\"\\nScore statistics (legalized area per tree):\")\n",
    "        print(f\"  Mean: {df['score_final'].mean():.6f}\")\n",
    "        print(f\"  Median: {df['score_final'].median():.6f}\")\n",
    "        print(f\"  Min: {df['score_final'].min():.6f}\")\n",
    "        print(f\"  Max: {df['score_final'].max():.6f}\")\n",
    "        print(f\"  Std: {df['score_final'].std():.6f}\")\n",
    "#df = df[ ((df['seed']>2800) & (df['use_fixed_h_for_size_setup'] & df['genotype_at']==1)) & (~df['reduce_h_per_individual'] & df['alter_diversity']==1)]\n",
    "df\n",
    "#& (~df['reduce_h_per_individual'] & df['alter_diversity']==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nTotal runs: {len(df)}\")\n",
    "    print(f\"\\nRuntime statistics:\")\n",
    "    if 'runtime' in df.columns and not df['runtime'].isna().all():\n",
    "        print(f\"  Mean: {df['runtime'].mean():.1f}s\")\n",
    "        print(f\"  Median: {df['runtime'].median():.1f}s\")\n",
    "        print(f\"  Min: {df['runtime'].min():.1f}s\")\n",
    "        print(f\"  Max: {df['runtime'].max():.1f}s\")\n",
    "    else:\n",
    "        print(\"  No runtime data available\")\n",
    "    \n",
    "    print(f\"\\nFinal cost statistics:\")\n",
    "    print(f\"  Mean: {df['cost_final'].mean():.6f}\")\n",
    "    print(f\"  Median: {df['cost_final'].median():.6f}\")\n",
    "    print(f\"  Min: {df['cost_final'].min():.6f}\")\n",
    "    print(f\"  Max: {df['cost_final'].max():.6f}\")\n",
    "    print(f\"  Std: {df['cost_final'].std():.6f}\")\n",
    "    \n",
    "    # Scores statistics if available\n",
    "    if 'score_final' in df.columns and not df['score_final'].isna().all():\n",
    "        print(f\"\\nFinal score statistics (legalized area per tree):\")\n",
    "        print(f\"  Mean: {df['score_final'].mean():.6f}\")\n",
    "        print(f\"  Median: {df['score_final'].median():.6f}\")\n",
    "        print(f\"  Min: {df['score_final'].min():.6f}\")\n",
    "        print(f\"  Max: {df['score_final'].max():.6f}\")\n",
    "        print(f\"  Std: {df['score_final'].std():.6f}\")\n",
    "    \n",
    "    print(f\"\\nBest 5 runs (by final cost):\")\n",
    "    best_runs = df.nsmallest(5, 'cost_final')\n",
    "    for idx, row in best_runs.iterrows():\n",
    "        score_str = f\", score={row.get('score_final', np.nan):.6f}\" if 'score_final' in row and not pd.isna(row.get('score_final')) else \"\"\n",
    "        print(f\"  Seed {row['seed']:3d}: cost={row['cost_final']:.6f}{score_str}, runtime={row.get('runtime', np.nan):.1f}s\")\n",
    "    \n",
    "    print(f\"\\nWorst 5 runs (by final cost):\")\n",
    "    worst_runs = df.nlargest(5, 'cost_final')\n",
    "    for idx, row in worst_runs.iterrows():\n",
    "        score_str = f\", score={row.get('score_final', np.nan):.6f}\" if 'score_final' in row and not pd.isna(row.get('score_final')) else \"\"\n",
    "        print(f\"  Seed {row['seed']:3d}: cost={row['cost_final']:.6f}{score_str}, runtime={row.get('runtime', np.nan):.1f}s\")\n",
    "    \n",
    "    # Best runs by score if available\n",
    "    if 'score_final' in df.columns and not df['score_final'].isna().all():\n",
    "        print(f\"\\nBest 5 runs (by final score):\")\n",
    "        best_score_runs = df.nsmallest(5, 'score_final')\n",
    "        for idx, row in best_score_runs.iterrows():\n",
    "            print(f\"  Seed {row['seed']:3d}: score={row['score_final']:.6f}, cost={row['cost_final']:.6f}, runtime={row.get('runtime', np.nan):.1f}s\")\n",
    "    \n",
    "    # Identify modifier columns (hyperparameters)\n",
    "    modifier_cols = [col for col in df.columns if col not in \n",
    "                     ['seed', 'runtime', 'filename', 'cost_final', 'cost_final_std', 'score_final', 'score_final_std'] and \n",
    "                     not col.startswith('cost_gen_') and not col.startswith('score_trees_')]\n",
    "    \n",
    "    if len(modifier_cols) > 0:\n",
    "        print(f\"\\nHyperparameters varied: {', '.join(modifier_cols)}\")\n",
    "else:\n",
    "    print(\"No data loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convergence_header",
   "metadata": {},
   "source": [
    "## Convergence Analysis\n",
    "\n",
    "Plot costs at different generations to see convergence patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convergence_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    # Get cost columns for the specified generations\n",
    "    cost_cols = [f'cost_gen_{gen}' for gen in generations_to_analyze if f'cost_gen_{gen}' in df.columns]\n",
    "    \n",
    "    if len(cost_cols) > 0:\n",
    "        fig, axes = plt.subplots(1, len(cost_cols), figsize=(5*len(cost_cols), 5))\n",
    "        if len(cost_cols) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, col in zip(axes, cost_cols):\n",
    "            gen = int(col.split('_')[-1])\n",
    "            ax.hist(df[col], bins=30, edgecolor='black', alpha=0.7)\n",
    "            ax.axvline(df[col].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[col].mean():.5f}')\n",
    "            ax.axvline(df[col].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df[col].median():.5f}')\n",
    "            ax.set_xlabel('Cost')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_title(f'Cost Distribution at Generation {gen}')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Box plot comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        df_melt = df[cost_cols].melt(var_name='Generation', value_name='Cost')\n",
    "        df_melt['Generation'] = df_melt['Generation'].str.extract(r'(\\d+)').astype(int)\n",
    "        sns.boxplot(data=df_melt, x='Generation', y='Cost')\n",
    "        plt.title('Cost Distribution Across Generations')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Generation')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6k8yf6e4vff",
   "metadata": {},
   "source": [
    "## Score Analysis\n",
    "\n",
    "Analyze the legalized scores (area per tree) from GA output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1qs9a3h6p4w",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0 and 'score_final' in df.columns and not df['score_final'].isna().all():\n",
    "    # Get score columns for different phenotypes\n",
    "    score_cols = [col for col in df.columns if col.startswith('score_config_')]\n",
    "    \n",
    "    if len(score_cols) > 0:\n",
    "        # Create figure with multiple subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # 1. Score distribution histogram\n",
    "        ax = axes[0]\n",
    "        ax.hist(df['score_final'], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        ax.axvline(df['score_final'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean: {df[\"score_final\"].mean():.6f}')\n",
    "        ax.axvline(df['score_final'].median(), color='green', linestyle='--', linewidth=2, \n",
    "                   label=f'Median: {df[\"score_final\"].median():.6f}')\n",
    "        ax.set_xlabel('Score (Area per Tree)')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title('Score Distribution (Legalized)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Cost vs Score scatter plot\n",
    "        ax = axes[1]\n",
    "        ax.scatter(df['cost_final'], df['score_final'], alpha=0.6, s=50, color='steelblue')\n",
    "        \n",
    "        # Add correlation\n",
    "        corr = df[['cost_final', 'score_final']].corr().iloc[0, 1]\n",
    "        ax.set_xlabel('Final Cost')\n",
    "        ax.set_ylabel('Final Score (Area per Tree)')\n",
    "        ax.set_title(f'Cost vs Score (Correlation: {corr:.3f})')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(df) > 1:\n",
    "            z = np.polyfit(df['cost_final'], df['score_final'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_line = np.linspace(df['cost_final'].min(), df['cost_final'].max(), 100)\n",
    "            ax.plot(x_line, p(x_line), \"r--\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        # 3. Scores by phenotype (if multiple)\n",
    "        ax = axes[2]\n",
    "        if len(score_cols) > 1:\n",
    "            # Create box plot for different phenotypes\n",
    "            score_data = []\n",
    "            labels = []\n",
    "            for col in score_cols:\n",
    "                config_id = col.split('_')[-1]\n",
    "                score_data.append(df[col].dropna())\n",
    "                labels.append(f'Config {config_id}')\n",
    "            \n",
    "            ax.boxplot(score_data, labels=labels, patch_artist=True,\n",
    "                      boxprops=dict(facecolor='steelblue', alpha=0.7))\n",
    "            ax.set_ylabel('Score (Area per Tree)')\n",
    "            ax.set_title('Scores by phenotype')\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "        else:\n",
    "            # Single phenotype - show histogram\n",
    "            ax.hist(df[score_cols[0]], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "            ax.set_xlabel('Score')\n",
    "            ax.set_ylabel('Count')\n",
    "            config_id = score_cols[0].split('_')[-1]\n",
    "            ax.set_title(f'Score Distribution (Config {config_id})')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Runtime vs Score\n",
    "        ax = axes[3]\n",
    "        if 'runtime' in df.columns and not df['runtime'].isna().all():\n",
    "            ax.scatter(df['runtime'], df['score_final'], alpha=0.6, s=50, color='steelblue')\n",
    "            \n",
    "            # Add correlation\n",
    "            corr_rt = df[['runtime', 'score_final']].corr().iloc[0, 1]\n",
    "            ax.set_xlabel('Runtime (s)')\n",
    "            ax.set_ylabel('Final Score (Area per Tree)')\n",
    "            ax.set_title(f'Runtime vs Score (Correlation: {corr_rt:.3f})')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add trend line\n",
    "            if len(df) > 1:\n",
    "                z = np.polyfit(df['runtime'], df['score_final'], 1)\n",
    "                p = np.poly1d(z)\n",
    "                x_line = np.linspace(df['runtime'].min(), df['runtime'].max(), 100)\n",
    "                ax.plot(x_line, p(x_line), \"r--\", alpha=0.8, linewidth=2)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No runtime data', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title('Runtime vs Score')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print correlation summary\n",
    "        print(\"=\"*70)\n",
    "        print(\"SCORE CORRELATIONS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nScore vs Cost:    {df[['cost_final', 'score_final']].corr().iloc[0, 1]:+.4f}\")\n",
    "        if 'runtime' in df.columns and not df['runtime'].isna().all():\n",
    "            print(f\"Score vs Runtime: {df[['runtime', 'score_final']].corr().iloc[0, 1]:+.4f}\")\n",
    "    else:\n",
    "        print(\"No per-phenotype score data available\")\n",
    "else:\n",
    "    print(\"No score data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyperparameter_header",
   "metadata": {},
   "source": [
    "## Hyperparameter Impact Analysis\n",
    "\n",
    "Analyze how each hyperparameter affects performance and runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameter_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    # Identify modifier columns\n",
    "    modifier_cols = [col for col in df.columns if col not in \n",
    "                     ['seed', 'runtime', 'filename', 'cost_final', 'cost_final_std', \n",
    "                      'score_final', 'score_final_std'] and \n",
    "                     not col.startswith('cost_gen_') and not col.startswith('score_trees_')]\n",
    "    \n",
    "    print(f\"Analyzing {len(modifier_cols)} hyperparameters...\\n\")\n",
    "    \n",
    "    for modifier in modifier_cols:\n",
    "        # Check if numeric or categorical\n",
    "        is_numeric = pd.api.types.is_numeric_dtype(df[modifier])\n",
    "        is_boolean = df[modifier].dtype == bool or set(df[modifier].unique()).issubset({True, False, np.nan})\n",
    "        n_unique = df[modifier].nunique()\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"{modifier}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Type: {'Boolean' if is_boolean else ('Numeric' if is_numeric else 'Categorical')}\")\n",
    "        print(f\"Unique values: {n_unique}\")\n",
    "        if n_unique <= 10:\n",
    "            print(f\"Values: {sorted(df[modifier].unique())}\")\n",
    "        else:\n",
    "            print(f\"Range: [{df[modifier].min():.4g}, {df[modifier].max():.4g}]\")\n",
    "        \n",
    "        # Correlation and p-value with final cost\n",
    "        if is_numeric and not is_boolean and n_unique > 2:\n",
    "            corr = df[[modifier, 'cost_final']].corr().iloc[0, 1]\n",
    "            p_val, test_name = compute_pvalue(df, modifier, 'cost_final')\n",
    "            print(f\"Correlation with final cost: {corr:.3f} ({format_pvalue(p_val)})\")\n",
    "        elif n_unique >= 2:\n",
    "            p_val, test_name = compute_pvalue(df, modifier, 'cost_final')\n",
    "            print(f\"Association with final cost: {format_pvalue(p_val)} [{test_name}]\")\n",
    "        \n",
    "        # Correlation and p-value with final score\n",
    "        if 'score_final' in df.columns and not df['score_final'].isna().all():\n",
    "            if is_numeric and not is_boolean and n_unique > 2:\n",
    "                corr_score = df[[modifier, 'score_final']].corr().iloc[0, 1]\n",
    "                p_val_sc, test_name_sc = compute_pvalue(df, modifier, 'score_final')\n",
    "                print(f\"Correlation with final score: {corr_score:.3f} ({format_pvalue(p_val_sc)})\")\n",
    "            elif n_unique >= 2:\n",
    "                p_val_sc, test_name_sc = compute_pvalue(df, modifier, 'score_final')\n",
    "                print(f\"Association with final score: {format_pvalue(p_val_sc)} [{test_name_sc}]\")\n",
    "        \n",
    "        # Correlation and p-value with runtime\n",
    "        if 'runtime' in df.columns and not df['runtime'].isna().all():\n",
    "            if is_numeric and not is_boolean and n_unique > 2:\n",
    "                corr_runtime = df[[modifier, 'runtime']].corr().iloc[0, 1]\n",
    "                p_val_rt, test_name_rt = compute_pvalue(df, modifier, 'runtime')\n",
    "                print(f\"Correlation with runtime: {corr_runtime:.3f} ({format_pvalue(p_val_rt)})\")\n",
    "            elif n_unique >= 2:\n",
    "                p_val_rt, test_name_rt = compute_pvalue(df, modifier, 'runtime')\n",
    "                print(f\"Association with runtime: {format_pvalue(p_val_rt)} [{test_name_rt}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scatter_header",
   "metadata": {},
   "source": [
    "## Scatter Plots: Cost vs Hyperparameters\n",
    "\n",
    "Visualize relationship between each hyperparameter and cost at different generations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457i0tz514",
   "metadata": {},
   "source": [
    "## Hyperparameter Impact on Scores\n",
    "\n",
    "Analyze how each hyperparameter affects final scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csbdvz5eunr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify modifier columns\n",
    "modifier_cols = [col for col in df.columns if col not in \n",
    "                 ['seed', 'runtime', 'filename', 'cost_final', 'cost_final_std', \n",
    "                  'score_final', 'score_final_std'] and \n",
    "                 not col.startswith('cost_gen_') and not col.startswith('score_config_')]\n",
    "\n",
    "# Analyze scores vs hyperparameters using unified function\n",
    "analyze_outcome_vs_hyperparameters(df, 'score_final', 'Score', modifier_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scatter_plots_cost",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    modifier_cols = [col for col in df.columns if col not in \n",
    "                     ['seed', 'runtime', 'filename', 'cost_final', 'cost_final_std', \n",
    "                      'score_final', 'score_final_std'] and \n",
    "                     not col.startswith('cost_gen_') and not col.startswith('score_config_')]\n",
    "    \n",
    "    cost_cols = [f'cost_gen_{gen}' for gen in generations_to_analyze if f'cost_gen_{gen}' in df.columns]\n",
    "    \n",
    "    if len(cost_cols) == 0:\n",
    "        print(\"No cost columns found for specified generations!\")\n",
    "    \n",
    "    # Filter out modifiers with no variation\n",
    "    modifier_cols = [col for col in modifier_cols if df[col].nunique() > 1]\n",
    "    \n",
    "    for modifier in modifier_cols:\n",
    "        # Create subplots for each generation\n",
    "        n_plots = len(cost_cols)\n",
    "        fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 4))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Compute p-value for final cost to show in main title\n",
    "        p_val_final, test_name = compute_pvalue(df, modifier, 'cost_final')\n",
    "        fig.suptitle(f'Cost vs {modifier} â€” {format_pvalue(p_val_final)}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for ax, cost_col in zip(axes, cost_cols):\n",
    "            gen = int(cost_col.split('_')[-1])\n",
    "            \n",
    "            # Use helper function to create plot\n",
    "            p_val = plot_modifier_vs_outcome(ax, df, modifier, cost_col, show_pval=True)\n",
    "            \n",
    "            # Update title to include generation info\n",
    "            current_title = ax.get_title()\n",
    "            ax.set_title(f'Gen {gen}: {current_title}', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "runtime_header",
   "metadata": {},
   "source": [
    "## Runtime Analysis\n",
    "\n",
    "Analyze how each hyperparameter affects runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "runtime_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify modifier columns\n",
    "modifier_cols = [col for col in df.columns if col not in \n",
    "                 ['seed', 'runtime', 'filename', 'cost_final', 'cost_final_std', \n",
    "                  'score_final', 'score_final_std'] and \n",
    "                 not col.startswith('cost_gen_') and not col.startswith('score_config_')]\n",
    "\n",
    "# Analyze runtime vs hyperparameters using unified function\n",
    "analyze_outcome_vs_hyperparameters(df, 'runtime', 'Runtime', modifier_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correlation_header",
   "metadata": {},
   "source": [
    "## Correlation Matrix\n",
    "\n",
    "Show correlations between all numeric hyperparameters, runtime, and costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    # Select numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    # Remove seed and per-generation/per-config columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['seed'] and \n",
    "                    not col.startswith('cost_gen_') and not col.startswith('score_config_') or \n",
    "                    col in [f'cost_gen_{gen}' for gen in generations_to_analyze[:2]]]\n",
    "    \n",
    "    if len(numeric_cols) > 1:\n",
    "        # Compute correlation matrix\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        \n",
    "        # Plot heatmap\n",
    "        plt.figure(figsize=(max(10, len(numeric_cols)*0.8), max(8, len(numeric_cols)*0.6)))\n",
    "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "                    square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "        plt.title('Correlation Matrix: Hyperparameters, Runtime, Costs, and Scores', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print strongest correlations with final cost\n",
    "        if 'cost_final' in corr_matrix.columns:\n",
    "            cost_corrs = corr_matrix['cost_final'].drop('cost_final').sort_values(key=abs, ascending=False)\n",
    "            print(\"\\nStrongest correlations with final cost:\")\n",
    "            print(\"=\"*50)\n",
    "            for param, corr in cost_corrs.head(10).items():\n",
    "                print(f\"{param:30s}: {corr:+.3f}\")\n",
    "        \n",
    "        # Print strongest correlations with final score\n",
    "        if 'score_final' in corr_matrix.columns:\n",
    "            score_corrs = corr_matrix['score_final'].drop('score_final').sort_values(key=abs, ascending=False)\n",
    "            print(\"\\nStrongest correlations with final score:\")\n",
    "            print(\"=\"*50)\n",
    "            for param, corr in score_corrs.head(10).items():\n",
    "                print(f\"{param:30s}: {corr:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_settings_header",
   "metadata": {},
   "source": [
    "## Best Hyperparameter Settings\n",
    "\n",
    "Identify the best hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best_settings",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    modifier_cols = [col for col in df.columns if col not in \n",
    "                     ['seed', 'runtime', 'filename', 'cost_final', 'cost_final_std', \n",
    "                      'score_final', 'score_final_std', 'n_generations'] and \n",
    "                     not col.startswith('cost_gen_') and not col.startswith('score_config_')]\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"TOP 10 BEST HYPERPARAMETER SETTINGS (by final cost)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    best_df = df.nsmallest(10, 'cost_final')\n",
    "    \n",
    "    for rank, (idx, row) in enumerate(best_df.iterrows(), 1):\n",
    "        score_str = f\" | Score: {row.get('score_final', np.nan):.6f}\" if 'score_final' in row and not pd.isna(row.get('score_final')) else \"\"\n",
    "        print(f\"\\n#{rank} - Seed {row['seed']} - Cost: {row['cost_final']:.6f}{score_str}\")\n",
    "        print(\"-\" * 70)\n",
    "        for col in modifier_cols:\n",
    "            print(f\"  {col:30s}: {row[col]}\")\n",
    "        if 'runtime' in row and not pd.isna(row['runtime']):\n",
    "            print(f\"  {'Runtime':30s}: {row['runtime']:.1f}s\")\n",
    "    \n",
    "    # Best by score if available\n",
    "    if 'score_final' in df.columns and not df['score_final'].isna().all():\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TOP 10 BEST HYPERPARAMETER SETTINGS (by final score)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        best_score_df = df.nsmallest(10, 'score_final')\n",
    "        \n",
    "        for rank, (idx, row) in enumerate(best_score_df.iterrows(), 1):\n",
    "            print(f\"\\n#{rank} - Seed {row['seed']} - Score: {row['score_final']:.6f} | Cost: {row['cost_final']:.6f}\")\n",
    "            print(\"-\" * 70)\n",
    "            for col in modifier_cols:\n",
    "                print(f\"  {col:30s}: {row[col]}\")\n",
    "            if 'runtime' in row and not pd.isna(row['runtime']):\n",
    "                print(f\"  {'Runtime':30s}: {row['runtime']:.1f}s\")\n",
    "    \n",
    "    # Summary statistics for best vs worst\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON: Top 25% vs Bottom 25% (by cost)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    n_quartile = max(1, len(df) // 4)\n",
    "    top_25 = df.nsmallest(n_quartile, 'cost_final')\n",
    "    bottom_25 = df.nlargest(n_quartile, 'cost_final')\n",
    "    \n",
    "    for col in modifier_cols:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            top_mean = top_25[col].mean()\n",
    "            bottom_mean = bottom_25[col].mean()\n",
    "            diff = top_mean - bottom_mean\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Top 25% mean:    {top_mean:.4g}\")\n",
    "            print(f\"  Bottom 25% mean: {bottom_mean:.4g}\")\n",
    "            print(f\"  Difference:      {diff:+.4g}\")\n",
    "    \n",
    "    # Summary by score if available\n",
    "    if 'score_final' in df.columns and not df['score_final'].isna().all():\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPARISON: Top 25% vs Bottom 25% (by score)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        top_25_score = df.nsmallest(n_quartile, 'score_final')\n",
    "        bottom_25_score = df.nlargest(n_quartile, 'score_final')\n",
    "        \n",
    "        for col in modifier_cols:\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                top_mean = top_25_score[col].mean()\n",
    "                bottom_mean = bottom_25_score[col].mean()\n",
    "                diff = top_mean - bottom_mean\n",
    "                print(f\"\\n{col}:\")\n",
    "                print(f\"  Top 25% mean:    {top_mean:.4g}\")\n",
    "                print(f\"  Bottom 25% mean: {bottom_mean:.4g}\")\n",
    "                print(f\"  Difference:      {diff:+.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export_header",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Save analysis results to CSV for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    output_file = f'../../results/many_ga/analysis_{runner_name}.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to: {output_file}\")\n",
    "    print(f\"Shape: {df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
