{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dba4b88",
   "metadata": {},
   "source": [
    "# PyTorch IPC (Inter-Process Communication) Examples\n",
    "\n",
    "This notebook demonstrates how to share memory between processes using PyTorch.\n",
    "\n",
    "**Note:** True multiprocessing with `spawn` doesn't work directly in Jupyter notebooks because child processes can't pickle the function definitions. We'll show:\n",
    "1. A threading example that works in the notebook\n",
    "2. How to run the full multiprocessing examples from a Python script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64124bd",
   "metadata": {},
   "source": [
    "## Option 1: Threading (Works in Notebook)\n",
    "\n",
    "Threads share memory naturally in Python, making this approach simpler for notebook environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff2853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial tensor: tensor([0., 0., 0., 0., 0.])\n",
      "Thread 0 starting...\n",
      "Thread 0 set position 0 to 0.0\n",
      "Thread 1 starting...\n",
      "Thread 2 starting...\n",
      "Thread 2 set position 2 to 20.0\n",
      "Thread 1 set position 1 to 10.0\n",
      "Thread 3 starting...\n",
      "Thread 4 starting...\n",
      "Thread 4 set position 4 to 40.0\n",
      "Thread 3 set position 3 to 30.0\n",
      "\n",
      "Final tensor: tensor([ 0., 10., 20., 30., 40.])\n",
      "All threads completed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import threading\n",
    "\n",
    "def thread_worker(shared_tensor, worker_id):\n",
    "    \"\"\"Thread worker that modifies shared tensor\"\"\"\n",
    "    print(f\"Thread {worker_id} starting...\")\n",
    "    shared_tensor[worker_id] = worker_id * 10\n",
    "    print(f\"Thread {worker_id} set position {worker_id} to {shared_tensor[worker_id].item()}\")\n",
    "\n",
    "# Create shared tensor (threads share memory naturally)\n",
    "shared_tensor = torch.zeros(5)\n",
    "print(\"Initial tensor:\", shared_tensor)\n",
    "\n",
    "# Create and start threads\n",
    "threads = []\n",
    "for i in range(5):\n",
    "    t = threading.Thread(target=thread_worker, args=(shared_tensor, i))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# Wait for all threads\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"\\nFinal tensor:\", shared_tensor)\n",
    "print(\"All threads completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92100d6a",
   "metadata": {},
   "source": [
    "## Option 2: True Multiprocessing (Run from Script)\n",
    "\n",
    "For true process isolation and to fully demonstrate `torch.multiprocessing` with shared memory, run the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd0d649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch IPC Examples\n",
      "Using multiprocessing to demonstrate shared memory\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 1: Basic Shared Memory\n",
      "============================================================\n",
      "Initial tensor: tensor([0., 0., 0., 0., 0.])\n",
      "Worker 0 starting...\n",
      "Worker 4 starting...\n",
      "Worker 2 starting...\n",
      "Worker 3 starting...\n",
      "Worker 1 starting...\n",
      "Worker 0 set position 0 to 0.0\n",
      "Worker 4 set position 4 to 40.0\n",
      "Worker 2 set position 2 to 20.0\n",
      "Worker 3 set position 3 to 30.0\n",
      "Worker 1 set position 1 to 10.0\n",
      "Final tensor: tensor([ 0., 10., 20., 30., 40.])\n",
      "All workers completed!\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 2: Producer-Consumer Pattern\n",
      "============================================================\n",
      "Producer: Generating data...\n",
      "Consumer: Waiting for data...\n",
      "Consumer: Read position 0 = 0.0\n",
      "Consumer: Read position 1 = 1.0\n",
      "Consumer: Read position 2 = 4.0\n",
      "Consumer: Read position 3 = 9.0\n",
      "Consumer: Read position 4 = 16.0\n",
      "Consumer: Read position 5 = 25.0\n",
      "Consumer: Read position 6 = 36.0\n",
      "Consumer: Read position 7 = 49.0\n",
      "Consumer: Read position 8 = 64.0\n",
      "Consumer: Read position 9 = 81.0\n",
      "Producer: Done\n",
      "Consumer: Done\n",
      "\n",
      "Final shared data: tensor([ 0.,  1.,  4.,  9., 16., 25., 36., 49., 64., 81.])\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 3: Parallel Computation\n",
      "============================================================\n",
      "Initial tensor sum: 499500.0\n",
      "Worker processing indices 250:500 on cuda:0Worker processing indices 500:750 on cuda:0Worker processing indices 750:1000 on cuda:0\n",
      "\n",
      "\n",
      "Worker processing indices 0:250 on cuda:0\n",
      "Final tensor sum: 334332000.0\n",
      "First 10 values: tensor([  0.,   4.,  10.,  18.,  28.,  40.,  54.,  70.,  88., 108.])\n",
      "\n",
      "============================================================\n",
      "All examples completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /mnt/d/packing/code/analysis\n",
    "~/miniconda3/envs/rapids-25.10/bin/python ipc_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cfb5b5",
   "metadata": {},
   "source": [
    "The script demonstrates three patterns:\n",
    "\n",
    "1. **Basic Shared Memory**: 5 workers each write to different positions\n",
    "2. **Producer-Consumer**: One process produces data, another consumes it with queue synchronization\n",
    "3. **Parallel Computation**: 4 workers process different slices of a 1000-element tensor\n",
    "\n",
    "### Key Concepts:\n",
    "- Use `.share_memory_()` to make tensors shareable across processes\n",
    "- Use `torch.multiprocessing` instead of regular `multiprocessing`\n",
    "- Shared tensors must be in CPU memory (each process copies to/from GPU)\n",
    "- Use `mp.Queue()` for synchronization between processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ae613",
   "metadata": {},
   "source": [
    "### View the multiprocessing script source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37290e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "PyTorch IPC (Inter-Process Communication) Examples\n",
      "Demonstrates shared memory between processes using torch.multiprocessing\n",
      "\"\"\"\n",
      "\n",
      "import torch\n",
      "import torch.multiprocessing as mp\n",
      "import time\n",
      "\n",
      "# Example 1: Basic shared memory tensor\n",
      "def worker(shared_tensor, worker_id):\n",
      "    \"\"\"Worker process that modifies shared tensor\"\"\"\n",
      "    print(f\"Worker {worker_id} starting...\")\n",
      "    \n",
      "    # Each worker adds its ID to its assigned position\n",
      "    shared_tensor[worker_id] = worker_id * 10\n",
      "    \n",
      "    # Simulate some work\n",
      "    time.sleep(0.1)\n",
      "    \n",
      "    print(f\"Worker {worker_id} set position {worker_id} to {shared_tensor[worker_id].item()}\")\n",
      "\n",
      "\n",
      "def example1_basic_shared_memory():\n",
      "    \"\"\"Basic example of shared memory between processes\"\"\"\n",
      "    print(\"\\n\" + \"=\"*60)\n",
      "    print(\"EXAMPLE 1: Basic Shared Memory\")\n",
      "    print(\"=\"*60)\n",
      "    \n",
      "    # Create a tensor and move it to shared memory\n",
      "    shared_tensor = torch.zeros(5)\n",
      "    shared_tensor.share_memory_()  # Make tensor shareable across processes\n",
      "    \n",
      "    print(\"Initial tensor:\", shared_tensor)\n",
      "    \n",
      "    # Spawn multiple processes\n",
      "    processes = []\n",
      "    for i in range(5):\n",
      "        p = mp.Process(target=worker, args=(shared_tensor, i))\n",
      "        p.start()\n",
      "        processes.append(p)\n",
      "    \n",
      "    # Wait for all processes to complete\n",
      "    for p in processes:\n",
      "        p.join()\n",
      "    \n",
      "    print(\"Final tensor:\", shared_tensor)\n",
      "    print(\"All workers completed!\")\n",
      "\n",
      "\n",
      "# Example 2: Producer-Consumer pattern with Queue\n",
      "def producer(queue, shared_data):\n",
      "    \"\"\"Producer writes data to shared tensor and signals via queue\"\"\"\n",
      "    print(\"Producer: Generating data...\")\n",
      "    \n",
      "    # Fill shared tensor with values\n",
      "    for i in range(len(shared_data)):\n",
      "        shared_data[i] = i ** 2\n",
      "        queue.put(i)  # Signal that position i is ready\n",
      "        time.sleep(0.05)\n",
      "    \n",
      "    queue.put(None)  # Signal completion\n",
      "    print(\"Producer: Done\")\n",
      "\n",
      "\n",
      "def consumer(queue, shared_data):\n",
      "    \"\"\"Consumer reads from shared tensor as data becomes available\"\"\"\n",
      "    print(\"Consumer: Waiting for data...\")\n",
      "    \n",
      "    while True:\n",
      "        idx = queue.get()\n",
      "        if idx is None:  # Check for completion signal\n",
      "            break\n",
      "        \n",
      "        value = shared_data[idx].item()\n",
      "        print(f\"Consumer: Read position {idx} = {value}\")\n",
      "    \n",
      "    print(\"Consumer: Done\")\n",
      "\n",
      "\n",
      "def example2_producer_consumer():\n",
      "    \"\"\"Producer-Consumer pattern with synchronization\"\"\"\n",
      "    print(\"\\n\" + \"=\"*60)\n",
      "    print(\"EXAMPLE 2: Producer-Consumer Pattern\")\n",
      "    print(\"=\"*60)\n",
      "    \n",
      "    # Create shared tensor\n",
      "    shared_data = torch.zeros(10)\n",
      "    shared_data.share_memory_()\n",
      "    \n",
      "    # Create queue for synchronization\n",
      "    queue = mp.Queue()\n",
      "    \n",
      "    # Start producer and consumer\n",
      "    prod = mp.Process(target=producer, args=(queue, shared_data))\n",
      "    cons = mp.Process(target=consumer, args=(queue, shared_data))\n",
      "    \n",
      "    prod.start()\n",
      "    cons.start()\n",
      "    \n",
      "    prod.join()\n",
      "    cons.join()\n",
      "    \n",
      "    print(\"\\nFinal shared data:\", shared_data)\n",
      "\n",
      "\n",
      "# Example 3: Parallel computation on shared GPU tensor\n",
      "def parallel_compute(shared_tensor, start_idx, end_idx, gpu_id):\n",
      "    \"\"\"Each worker processes a slice of the tensor on GPU\"\"\"\n",
      "    if torch.cuda.is_available():\n",
      "        device = torch.device(f'cuda:{gpu_id % torch.cuda.device_count()}')\n",
      "        print(f\"Worker processing indices {start_idx}:{end_idx} on {device}\")\n",
      "        \n",
      "        # Copy slice to GPU, compute, copy back\n",
      "        slice_data = shared_tensor[start_idx:end_idx].to(device)\n",
      "        slice_data = slice_data ** 2 + slice_data * 3  # Some computation\n",
      "        shared_tensor[start_idx:end_idx] = slice_data.cpu()\n",
      "    else:\n",
      "        # CPU fallback\n",
      "        print(f\"Worker processing indices {start_idx}:{end_idx} on CPU\")\n",
      "        shared_tensor[start_idx:end_idx] = shared_tensor[start_idx:end_idx] ** 2 + shared_tensor[start_idx:end_idx] * 3\n",
      "\n",
      "\n",
      "def example3_parallel_computation():\n",
      "    \"\"\"Parallel computation across multiple processes\"\"\"\n",
      "    print(\"\\n\" + \"=\"*60)\n",
      "    print(\"EXAMPLE 3: Parallel Computation\")\n",
      "    print(\"=\"*60)\n",
      "    \n",
      "    # Create large shared tensor\n",
      "    size = 1000\n",
      "    shared_tensor = torch.arange(size, dtype=torch.float32)\n",
      "    shared_tensor.share_memory_()\n",
      "    \n",
      "    print(f\"Initial tensor sum: {shared_tensor.sum().item()}\")\n",
      "    \n",
      "    # Split work among processes\n",
      "    num_workers = 4\n",
      "    chunk_size = size // num_workers\n",
      "    processes = []\n",
      "    \n",
      "    for i in range(num_workers):\n",
      "        start = i * chunk_size\n",
      "        end = (i + 1) * chunk_size if i < num_workers - 1 else size\n",
      "        p = mp.Process(target=parallel_compute, args=(shared_tensor, start, end, i))\n",
      "        p.start()\n",
      "        processes.append(p)\n",
      "    \n",
      "    for p in processes:\n",
      "        p.join()\n",
      "    \n",
      "    print(f\"Final tensor sum: {shared_tensor.sum().item()}\")\n",
      "    print(f\"First 10 values: {shared_tensor[:10]}\")\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    # Set multiprocessing start method\n",
      "    mp.set_start_method('spawn', force=True)\n",
      "    \n",
      "    print(\"\\nPyTorch IPC Examples\")\n",
      "    print(\"Using multiprocessing to demonstrate shared memory\")\n",
      "    \n",
      "    # Run all examples\n",
      "    example1_basic_shared_memory()\n",
      "    example2_producer_consumer()\n",
      "    example3_parallel_computation()\n",
      "    \n",
      "    print(\"\\n\" + \"=\"*60)\n",
      "    print(\"All examples completed!\")\n",
      "    print(\"=\"*60)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('ipc_demo.py', 'r') as f:\n",
    "    print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
