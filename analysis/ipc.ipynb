{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dba4b88",
   "metadata": {},
   "source": [
    "# PyTorch IPC (Inter-Process Communication) Examples\n",
    "\n",
    "This notebook demonstrates how to share memory between processes using PyTorch.\n",
    "\n",
    "**Note:** True multiprocessing with `spawn` doesn't work directly in Jupyter notebooks because child processes can't pickle the function definitions. We'll show:\n",
    "1. A threading example that works in the notebook\n",
    "2. How to run the full multiprocessing examples from a Python script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92100d6a",
   "metadata": {},
   "source": [
    "## Option 2: True Multiprocessing (Run from Script)\n",
    "\n",
    "For true process isolation and to fully demonstrate `torch.multiprocessing` with shared memory, run the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bd0d649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch GPU Multiprocessing Demo\n",
      "============================================================\n",
      "Initial tensor sum: 499500\n",
      "Initial first 10: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "\n",
      "Launching 4 GPU workers...\n",
      "Worker 2: Processing indices [500:750] on cuda:0\n",
      "Worker 1: Processing indices [250:500] on cuda:0\n",
      "Worker 3: Processing indices [750:1000] on cuda:0\n",
      "Worker 0: Processing indices [0:250] on cuda:0\n",
      "Worker 2: Completed\n",
      "Worker 1: Completed\n",
      "Worker 3: Completed\n",
      "Worker 0: Completed\n",
      "\n",
      "Final tensor sum: 334332000\n",
      "Final first 10: [0.0, 4.0, 10.0, 18.0, 28.0, 40.0, 54.0, 70.0, 88.0, 108.0]\n",
      "============================================================\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /mnt/d/packing/code/analysis\n",
    "~/miniconda3/envs/rapids-25.10/bin/python ipc_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cfb5b5",
   "metadata": {},
   "source": [
    "The script demonstrates three patterns:\n",
    "\n",
    "1. **Basic Shared Memory**: 5 workers each write to different positions\n",
    "2. **Producer-Consumer**: One process produces data, another consumes it with queue synchronization\n",
    "3. **Parallel Computation**: 4 workers process different slices of a 1000-element tensor\n",
    "\n",
    "### Key Concepts:\n",
    "- Use `.share_memory_()` to make tensors shareable across processes\n",
    "- Use `torch.multiprocessing` instead of regular `multiprocessing`\n",
    "- Shared tensors must be in CPU memory (each process copies to/from GPU)\n",
    "- Use `mp.Queue()` for synchronization between processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ae613",
   "metadata": {},
   "source": [
    "### View the multiprocessing script source:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
