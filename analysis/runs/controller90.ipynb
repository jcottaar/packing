{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667de78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vast\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '../../core'))\n",
    "import kaggle_support as kgs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import multiprocess as mp\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55799c50",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e6e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, 500), (24, 500), (28, 500), (32, 500), (36, 500), (40, 500), (44, 500), (48, 500), (52, 500), (56, 500), (60, 500), (64, 500), (68, 500), (72, 500), (76, 500), (80, 500), (84, 500), (88, 500), (92, 500), (96, 500), (100, 500)]\n",
      "Using checkpoint directory: controller_runs/\n"
     ]
    }
   ],
   "source": [
    "DO_CHECK = False\n",
    "\n",
    "# User inputs\n",
    "N_TREES_LIST = [x.item() for x in np.arange(4, 101, 4)]  # List of N_trees to run]\n",
    "N_PROCESSES = 2 if kgs.env=='local' else 1  # Maximum concurrent processes\n",
    "DIAGNOSTIC_INTERVAL = 120  # Seconds between diagnostic updates\n",
    "SEED_LIST = [500]  # Base seed for reproducibility\n",
    "\n",
    "# Derived inputs (edit manually to customize seeds per task)\n",
    "N_TREES_SEED_LIST = [(N, SEED) for N in N_TREES_LIST for SEED in SEED_LIST]\n",
    "print(N_TREES_SEED_LIST)\n",
    "\n",
    "# Set up checkpoint/log directory\n",
    "CHECKPOINT_DIR = 'controller_runs/' if not DO_CHECK else 'check_runs/'\n",
    "os.makedirs(kgs.temp_dir + CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Using checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# Create base runner configuration\n",
    "import pack_ga3\n",
    "import pack_runner\n",
    "\n",
    "base_runner = pack_ga3.baseline_symmetry_90()\n",
    "\n",
    "base_runner.ga.do_legalize = True\n",
    "base_runner.use_atomic_save = True  # Enable atomic saves for multiprocess safety\n",
    "\n",
    "base_runner.diagnostic_plot = False  # Disabled in subprocessbase_runner.save_every = 1\n",
    "if DO_CHECK:\n",
    "    base_runner.n_generations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030624c",
   "metadata": {},
   "source": [
    "## Worker Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e388a796-d521-40f9-b254-3c73758a6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_function(N_trees, seed, base_runner_pickled, checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Worker process that runs GA for a specific N_trees.\n",
    "    Runs in subprocess - output redirected to log file.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import os\n",
    "    sys.path.insert(0, os.path.join(os.getcwd(), '../../core'))\n",
    "    import kaggle_support as kgs\n",
    "    import copy\n",
    "\n",
    "    # Redirect stdout/stderr to log file\n",
    "    # log_path = kgs.temp_dir + checkpoint_dir + f'ga_N{N_trees}_seed{seed}.log'\n",
    "    # log_file = open(log_path, 'a')\n",
    "    # sys.stdout = log_file\n",
    "    # sys.stderr = log_file\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting worker: N_trees={N_trees}, seed={seed}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Unpickle base runner and configure\n",
    "        import dill\n",
    "        runner = dill.loads(base_runner_pickled)\n",
    "        runner.seed = seed\n",
    "        runner.filename = checkpoint_dir + f'ga_N{N_trees}_seed{seed}'\n",
    "        \n",
    "        # Configure for this N_trees\n",
    "        runner.ga.ga_base.N_trees_to_do = N_trees\n",
    "        \n",
    "        # Check for existing checkpoint\n",
    "        checkpoint_path =  kgs.temp_dir + runner.filename + '.pickle'\n",
    "        if os.path.isfile(checkpoint_path):\n",
    "            print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "            runner = kgs.dill_load(checkpoint_path)\n",
    "        \n",
    "        # Run with atomic saves (enabled via use_atomic_save flag)\n",
    "        runner.run()\n",
    "        \n",
    "        print(f\"\\nCompleted: N_trees={N_trees}, seed={seed}, final generation={runner._current_generation}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"\\nERROR in worker N_trees={N_trees}, seed={seed}:\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91ca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting worker: N_trees=4, seed=500\n",
      "============================================================\n",
      "\n",
      "init CUDA\n",
      "Detected GPU compute capability: 12.0 (arch=sm_120)\n",
      "GPU max threads per block: 1024\n",
      "=== Compiling kernel variant: crystal ===\n",
      "Defines: ENABLE_CRYSTAL_AXES, ENABLE_OVERLAP_AREA, ENABLE_SEPARATION\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_120 -DENABLE_CRYSTAL_AXES -DENABLE_OVERLAP_AREA -DENABLE_SEPARATION -cubin /packing/temp/pack_cuda_saved.cu -o /packing/temp/pack_cuda_crystal.cubin\n",
      "ptxas info    : 0 bytes gmem\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_120'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers\n",
      "ptxas info    : Compile time = 9.613 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_120'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    224 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 54 registers, used 1 barriers, 224 bytes cumulative stack size\n",
      "ptxas info    : Compile time = 553.917 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_120'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    1232 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 89 registers, used 1 barriers, 1232 bytes cumulative stack size\n",
      "ptxas info    : Compile time = 316.068 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [crystal] ---\n",
      "  Max threads per block (kernel): 640\n",
      "  Num registers: 89\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 1232\n",
      "=== Compiling kernel variant: no_sep ===\n",
      "Defines: ENABLE_OVERLAP_AREA\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_120 -DENABLE_OVERLAP_AREA -cubin /packing/temp/pack_cuda_saved.cu -o /packing/temp/pack_cuda_no_sep.cubin\n",
      "ptxas info    : 0 bytes gmem\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_120'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers\n",
      "ptxas info    : Compile time = 9.171 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_120'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    224 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 54 registers, used 1 barriers, 224 bytes cumulative stack size\n",
      "ptxas info    : Compile time = 533.081 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_120'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    1232 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 70 registers, used 1 barriers, 1232 bytes cumulative stack size\n",
      "ptxas info    : Compile time = 75.800 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [no_sep] ---\n",
      "  Max threads per block (kernel): 896\n",
      "  Num registers: 70\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 1232\n",
      "=== Compiling kernel variant: sep ===\n",
      "Defines: ENABLE_SEPARATION\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_120 -DENABLE_SEPARATION -cubin /packing/temp/pack_cuda_saved.cu -o /packing/temp/pack_cuda_sep.cubin\n",
      "ptxas info    : 0 bytes gmem\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_120'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers\n",
      "ptxas info    : Compile time = 9.136 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_120'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    224 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 54 registers, used 1 barriers, 224 bytes cumulative stack size\n",
      "ptxas info    : Compile time = 532.565 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_120'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    128 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 64 registers, used 1 barriers, 128 bytes cumulative stack size\n",
      "ptxas info    : Compile time = 69.341 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [sep] ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 64\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 128\n",
      "\n",
      "--- Kernel: multi_boundary_list_total ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 54\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 224\n",
      "\n",
      "--- Kernel: multi_boundary_distance_list_total ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 36\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 0\n",
      "Loading cached lookup table from /packing/temp//lut_cache/exact_sep_Nx900_Ny900_Nt900_trimTrue_tree42d0f17d.lut_cache...\n",
      "Successfully loaded cached lookup table\n",
      "Compiling CUDA LUT kernel one-time only)\n",
      "Detected GPU compute capability: 120 (arch=sm_120)\n",
      "Compiling: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -lineinfo -arch=sm_120 -cubin /packing/temp/pack_cuda_lut_saved.cu -o /packing/temp/pack_cuda_lut.cubin\n",
      "/packing/temp/pack_cuda_lut_saved.cu:5: warning: \"M_PI\" redefined\n",
      "    5 | #define M_PI 3.14159265358979323846f\n",
      "      | \n",
      "In file included from /usr/include/c++/13/cmath:47,\n",
      "                 from /usr/include/c++/13/math.h:36,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/crt/math_functions.h:4617,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/crt/common_functions.h:303,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:117,\n",
      "                 from <command-line>:\n",
      "/usr/include/math.h:1152: note: this is the location of the previous definition\n",
      " 1152 | # define M_PI           3.14159265358979323846  /* pi */\n",
      "      | \n",
      "ptxas info    : 16 bytes gmem\n",
      "ptxas info    : Compiling entry function 'multi_overlap_lut_total' for 'sm_120'\n",
      "ptxas info    : Function properties for multi_overlap_lut_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 48 registers, used 1 barriers\n",
      "ptxas info    : Compile time = 13.727 ms\n",
      "\n",
      "Kernel multi_overlap_lut_total:\n",
      "  Registers: 48\n",
      "  Shared mem: 0 bytes\n",
      "  Max threads/block: 1024\n"
     ]
    }
   ],
   "source": [
    "worker_function(4,500,kgs.dill.dumps(base_runner),CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d0344",
   "metadata": {},
   "source": [
    "## Process Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessManager:\n",
    "    \"\"\"Manages pool of worker processes.\"\"\"\n",
    "    \n",
    "    def __init__(self, N_trees_seed_list, n_processes, base_runner, checkpoint_dir):\n",
    "        self.N_trees_seed_list = list(N_trees_seed_list)\n",
    "        self.n_processes = n_processes\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.base_runner_pickled = kgs.dill.dumps(base_runner)\n",
    "        \n",
    "        self.pending_tasks = list(self.N_trees_seed_list)\n",
    "        self.running = {}  # {Process: (N_trees, seed)}\n",
    "        self.completed = set()  # Set of (N_trees, seed) tuples\n",
    "        self.errored = {}  # {(N_trees, seed): exit_code}\n",
    "        self.last_start_time = 0  # Track last process start time\n",
    "    \n",
    "    @staticmethod\n",
    "    def _format_task(N_trees, seed):\n",
    "        return f\"N_trees={N_trees}, seed={seed}\"\n",
    "        \n",
    "    def start_process(self, N_trees, seed):\n",
    "        \"\"\"Start a new worker process, or mark done if done file exists.\"\"\"\n",
    "        # Check if done file already exists\n",
    "        filename = f'ga_N{N_trees}_seed{seed}'\n",
    "        done_path = kgs.temp_dir + self.checkpoint_dir + f'done/{filename}_done.pickle'\n",
    "        \n",
    "        if os.path.isfile(done_path):\n",
    "            # Done file exists, just mark it as completed without starting process\n",
    "            self.completed.add((N_trees, seed))\n",
    "            print(f\"Skipping {self._format_task(N_trees, seed)} - already done (found {done_path})\")\n",
    "            return\n",
    "        \n",
    "        # No done file, start the process normally\n",
    "        p = mp.Process(target=worker_function, \n",
    "                      args=(N_trees, seed, self.base_runner_pickled, self.checkpoint_dir))\n",
    "        p.start()\n",
    "        self.running[p] = (N_trees, seed)\n",
    "        self.last_start_time = time.time()\n",
    "        print(f\"Started process for {self._format_task(N_trees, seed)} (PID: {p.pid})\")\n",
    "        \n",
    "    def update(self):\n",
    "        \"\"\"Check for completed processes and start new ones.\"\"\"\n",
    "        # Check running processes\n",
    "        for p in list(self.running.keys()):\n",
    "            if not p.is_alive():\n",
    "                N_trees, seed = self.running[p]\n",
    "                p.join()\n",
    "                del self.running[p]\n",
    "                if p.exitcode == 0:\n",
    "                    self.completed.add((N_trees, seed))\n",
    "                    print(f\"Process completed: {self._format_task(N_trees, seed)}\")\n",
    "                else:\n",
    "                    self.errored[(N_trees, seed)] = p.exitcode\n",
    "                    print(f\"Process FAILED: {self._format_task(N_trees, seed)} (exit code: {p.exitcode})\")\n",
    "        \n",
    "        # Start new processes if slots available\n",
    "        while len(self.running) < self.n_processes and self.pending_tasks:\n",
    "            # Stagger startup: wait 5 seconds after last start\n",
    "            time_since_last_start = time.time() - self.last_start_time\n",
    "            if time_since_last_start < 5.0:\n",
    "                time.sleep(5.0 - time_since_last_start)\n",
    "            \n",
    "            N_trees, seed = self.pending_tasks.pop(0)\n",
    "            self.start_process(N_trees, seed)\n",
    "    \n",
    "    def get_status(self, task):\n",
    "        \"\"\"Get status of a specific (N_trees, seed) value.\"\"\"\n",
    "        N_trees, seed = task\n",
    "        if task in self.errored:\n",
    "            return f\"ERROR (exit {self.errored[task]})\"\n",
    "        if task in self.completed:\n",
    "            return \"DONE\"\n",
    "        for p, (nt, sd) in self.running.items():\n",
    "            if (nt, sd) == task:\n",
    "                return \"RUNNING\"\n",
    "        for pending_task in self.pending_tasks:\n",
    "            if pending_task == task:\n",
    "                return \"PENDING\"\n",
    "        return \"UNKNOWN\"\n",
    "    \n",
    "    def is_complete(self):\n",
    "        \"\"\"Check if all tasks are done.\"\"\"\n",
    "        return len(self.pending_tasks) == 0 and len(self.running) == 0\n",
    "    \n",
    "    def status(self):\n",
    "        \"\"\"Return status string.\"\"\"\n",
    "        return (f\"Running: {len(self.running)}, \"\n",
    "                f\"Pending: {len(self.pending_tasks)}, \"\n",
    "                f\"Completed: {len(self.completed)}, \"\n",
    "                f\"Errored: {len(self.errored)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b297c",
   "metadata": {},
   "source": [
    "## Diagnostic Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a25c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_diagnostics(N_trees_seed_list, checkpoint_dir, manager):\n",
    "    \"\"\"\n",
    "    Load all runner pickles and display diagnostic plots.\n",
    "    Handles missing/corrupted files gracefully.\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, load all pickle files before clearing display\n",
    "    runners = {}\n",
    "    status_info = {}  # Store status and error messages\n",
    "    \n",
    "    for N_trees, seed in N_trees_seed_list:\n",
    "        task = (N_trees, seed)\n",
    "        status = manager.get_status(task)\n",
    "        filename = f'ga_N{N_trees}_seed{seed}'\n",
    "        task_label = f\"N={N_trees}, seed={seed}\"\n",
    "        \n",
    "        # If process is DONE, load from done directory\n",
    "        if status == \"DONE\":\n",
    "            done_path = kgs.temp_dir + checkpoint_dir + f'done/{filename}_done.pickle'\n",
    "            \n",
    "            if os.path.isfile(done_path):\n",
    "                try:\n",
    "                    runner = kgs.dill_load(done_path)\n",
    "                    \n",
    "                    # Store info for later display (no diagnostic plots for done processes)\n",
    "                    best_costs = [[round(float(x), 6) for x in s[-1].flatten()] \n",
    "                                  for s in runner.ga.best_costs_per_generation]\n",
    "                    status_info[task] = {\n",
    "                        'status': status,\n",
    "                        'type': 'done',\n",
    "                        'message': f\"{task_label} [{status:8s}]: Gen {runner._current_generation}/{runner.n_generations} | Best: {best_costs}\"\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    status_info[task] = {\n",
    "                        'status': 'MISSING',\n",
    "                        'type': 'error',\n",
    "                        'message': f\"{task_label} [MISSING ]: Done file exists but failed to load - {str(e)[:50]}\"\n",
    "                    }\n",
    "            else:\n",
    "                status_info[task] = {\n",
    "                    'status': 'MISSING',\n",
    "                    'type': 'missing',\n",
    "                    'message': f\"{task_label} [MISSING ]\"\n",
    "                }\n",
    "        else:\n",
    "            # For ongoing processes, load checkpoint and prepare for diagnostic plots\n",
    "            pickle_path = kgs.temp_dir + checkpoint_dir + filename + '.pickle'\n",
    "            \n",
    "            if os.path.isfile(pickle_path):\n",
    "                try:\n",
    "                    runner = kgs.dill_load(pickle_path)\n",
    "                    if status == \"RUNNING\":\n",
    "                        runners[task] = runner  # Only add to runners for diagnostic plots\n",
    "                    \n",
    "                    # Store info for later display\n",
    "                    best_costs = [[round(float(x), 6) for x in s[-1].flatten()] \n",
    "                                  for s in runner.ga.best_costs_per_generation]\n",
    "                    status_info[task] = {\n",
    "                        'status': status,\n",
    "                        'type': 'loaded',\n",
    "                        'message': f\"{task_label} [{status:8s}]: Gen {runner._current_generation}/{runner.n_generations} | Best: {best_costs}\"\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    status_info[task] = {\n",
    "                        'status': status,\n",
    "                        'type': 'error',\n",
    "                        'message': f\"{task_label} [{status:8s}]: Error loading pickle - {str(e)[:50]}\"\n",
    "                    }\n",
    "            else:\n",
    "                status_info[task] = {\n",
    "                    'status': status,\n",
    "                    'type': 'missing',\n",
    "                    'message': f\"{task_label} [{status:8s}]: No checkpoint found\"\n",
    "                }\n",
    "    \n",
    "    # Now clear output and display everything\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Diagnostics Update - {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Print all status messages\n",
    "    for task in N_trees_seed_list:\n",
    "        print(status_info[task]['message'])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "    \n",
    "    # Display diagnostic plots ONLY for ongoing processes (runners dict only contains those)\n",
    "    for (N_trees, seed), runner in runners.items():\n",
    "        try:\n",
    "            runner.ga.diagnostic_plots(runner._current_generation - 1, None)            \n",
    "            #plt.suptitle(f\"N_trees={N_trees} (Gen {runner._current_generation})\")\n",
    "            #plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying diagnostics for N={N_trees}, seed={seed}: {e}\")\n",
    "\n",
    "    plt.close('all')  # Close figures to prevent accumulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ecf34",
   "metadata": {},
   "source": [
    "## Main Controller Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584049d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize process manager\n",
    "manager = ProcessManager(N_TREES_SEED_LIST, N_PROCESSES, base_runner, CHECKPOINT_DIR)\n",
    "\n",
    "task_labels = [f\"(N={N}, seed={seed})\" for N, seed in N_TREES_SEED_LIST]\n",
    "print(f\"Starting controller for tasks: {task_labels}\")\n",
    "print(f\"Max concurrent processes: {N_PROCESSES}\")\n",
    "print(f\"Diagnostic interval: {DIAGNOSTIC_INTERVAL}s\\n\")\n",
    "\n",
    "last_diagnostic_time = 0\n",
    "\n",
    "try:\n",
    "    while not manager.is_complete():\n",
    "        # Update process pool\n",
    "        manager.update()\n",
    "        \n",
    "        # Display diagnostics periodically\n",
    "        current_time = time.time()\n",
    "        if current_time - last_diagnostic_time >= DIAGNOSTIC_INTERVAL:\n",
    "            display_diagnostics(N_TREES_SEED_LIST, CHECKPOINT_DIR, manager)\n",
    "            print(f\"\\nStatus: {manager.status()}\\n\")\n",
    "            last_diagnostic_time = current_time\n",
    "        \n",
    "        # Sleep briefly to avoid busy-waiting\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Final update\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"All processes completed!\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    display_diagnostics(N_TREES_SEED_LIST, CHECKPOINT_DIR, manager)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nInterrupted by user!\")\n",
    "    print(\"Terminating running processes...\")\n",
    "    for p in manager.running.keys():\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "    print(\"Cleanup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dcca43",
   "metadata": {},
   "source": [
    "## Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4901a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all completed runners for analysis\n",
    "completed_runners = {}\n",
    "\n",
    "for N_trees, seed in N_TREES_SEED_LIST:\n",
    "    filename = f'ga_N{N_trees}_seed{seed}'\n",
    "    pickle_path = kgs.temp_dir + CHECKPOINT_DIR + filename + '.pickle'\n",
    "    \n",
    "    if os.path.isfile(pickle_path):\n",
    "        try:\n",
    "            runner = kgs.dill_load(pickle_path)\n",
    "            completed_runners[(N_trees, seed)] = runner\n",
    "            print(f\"Loaded N={N_trees}, seed={seed}: {runner._current_generation} generations\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load N={N_trees}, seed={seed}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal loaded: {len(completed_runners)} runners\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ce5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best solutions\n",
    "import pack_vis_sol\n",
    "\n",
    "for (N_trees, seed), runner in completed_runners.items():\n",
    "    best_costs = [[float(x) for x in s[-1].flatten()] \n",
    "                  for s in runner.ga.best_costs_per_generation]\n",
    "    print(f\"\\nN={N_trees}, seed={seed}: Best costs = {best_costs}\")\n",
    "    \n",
    "    # Visualize best solution\n",
    "    # (Adjust based on your GA structure - GAMulti vs single GA)\n",
    "    try:\n",
    "        plt.figure()\n",
    "        pack_vis_sol.pack_vis_sol(runner.ga.champions[-1].phenotype)\n",
    "        plt.title(f\"N={N_trees}, seed={seed}, Cost={runner.ga.champions[-1].fitness[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not visualize N={N_trees}, seed={seed}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82fabd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
