{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e2f534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n",
      "WARNING: CUDA MPS not active\n"
     ]
    },
    {
     "ename": "CUDARuntimeError",
     "evalue": "cudaErrorInvalidResourceHandle: invalid resource handle",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCUDARuntimeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcupy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcp\u001b[39;00m\n\u001b[32m      6\u001b[39m handle = kgs.dill_load(kgs.temp_dir + \u001b[33m'\u001b[39m\u001b[33m/test_cp_handle.pickle\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mruntime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mipcOpenMemHandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends/cuda/api/runtime.pyx:510\u001b[39m, in \u001b[36mcupy_backends.cuda.api.runtime.ipcOpenMemHandle\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends/cuda/api/runtime.pyx:516\u001b[39m, in \u001b[36mcupy_backends.cuda.api.runtime.ipcOpenMemHandle\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends/cuda/api/runtime.pyx:146\u001b[39m, in \u001b[36mcupy_backends.cuda.api.runtime.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCUDARuntimeError\u001b[39m: cudaErrorInvalidResourceHandle: invalid resource handle"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '../core'))\n",
    "import kaggle_support as kgs\n",
    "import cupy as cp\n",
    "handle = kgs.dill_load(kgs.temp_dir + '/test_cp_handle.pickle')\n",
    "cp.cuda.runtime.ipcOpenMemHandle(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d81669",
   "metadata": {},
   "outputs": [
    {
     "ename": "CUDARuntimeError",
     "evalue": "cudaErrorInvalidResourceHandle: invalid resource handle",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCUDARuntimeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mruntime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mipcOpenMemHandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends/cuda/api/runtime.pyx:510\u001b[39m, in \u001b[36mcupy_backends.cuda.api.runtime.ipcOpenMemHandle\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends/cuda/api/runtime.pyx:516\u001b[39m, in \u001b[36mcupy_backends.cuda.api.runtime.ipcOpenMemHandle\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends/cuda/api/runtime.pyx:146\u001b[39m, in \u001b[36mcupy_backends.cuda.api.runtime.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCUDARuntimeError\u001b[39m: cudaErrorInvalidResourceHandle: invalid resource handle"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why IPC Handles Don't Work This Way\n",
    "\n",
    "**The problem:** IPC handles are only valid while:\n",
    "1. The original process that created them is **still running**\n",
    "2. The GPU memory allocation still exists\n",
    "3. You open the handle in a **different process** (not the same one)\n",
    "\n",
    "**What's happening:**\n",
    "- You created the handle in one Python session\n",
    "- Saved it to a file\n",
    "- That Python session ended → GPU memory was freed\n",
    "- Now the handle points to freed memory → invalid handle error\n",
    "\n",
    "**For actual IPC, you need:**\n",
    "1. Process A: Creates array, gets IPC handle, **stays running**\n",
    "2. Process A: Shares handle (via file, socket, shared memory, etc.)\n",
    "3. Process B: Opens the handle **while Process A is still running**\n",
    "4. Both processes can now access the same GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9c23c",
   "metadata": {},
   "source": [
    "## Alternative: Multi-processing Demo\n",
    "\n",
    "If you want to test IPC, here's a proper setup using Python's multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96cb3eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Original array: [0. 1. 2. 3. 4.]\n",
      "Parent: Got IPC handle, type: <class 'bytes'>, len: 64\n",
      "Child: Received handle\n",
      "Child: Error: cudaErrorInvalidResourceHandle: invalid resource handle\n",
      "Parent: Array after child ran: [0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "import multiprocess as mp\n",
    "import cupy as cp\n",
    "\n",
    "def child_process(handle_bytes, shape, dtype, nbytes):\n",
    "    \"\"\"Child process that receives and opens an IPC handle\"\"\"\n",
    "    import cupy as cp\n",
    "    \n",
    "    # CRITICAL: Initialize CUDA in this process first\n",
    "    cp.cuda.Device(0).use()\n",
    "    \n",
    "    print(f\"Child: Received handle\")\n",
    "    \n",
    "    try:\n",
    "        # Open it in this process - handle_bytes is the raw bytes\n",
    "        device_ptr = cp.cuda.runtime.ipcOpenMemHandle(handle_bytes)\n",
    "        \n",
    "        # Reconstruct the array\n",
    "        mem = cp.cuda.UnownedMemory(device_ptr, nbytes, owner=None)\n",
    "        memptr = cp.cuda.MemoryPointer(mem, 0)\n",
    "        arr_child = cp.ndarray(shape=shape, dtype=dtype, memptr=memptr)\n",
    "        \n",
    "        print(f\"Child: Array contents: {arr_child[:5]}\")\n",
    "        \n",
    "        # Modify it to prove it's shared\n",
    "        arr_child[:] = 42.0\n",
    "        print(f\"Child: Modified array to all 42s\")\n",
    "        \n",
    "        # Clean up\n",
    "        cp.cuda.runtime.ipcCloseMemHandle(device_ptr)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Child: Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Parent process - allocate with IPC flag\n",
    "# Use cudaMalloc which supports IPC by default\n",
    "arr = cp.arange(100, dtype=cp.float32)\n",
    "print(f\"Parent: Original array: {arr[:5]}\")\n",
    "\n",
    "try:\n",
    "    # Get IPC handle - this returns a cudaIpcMemHandle_t (64 bytes)\n",
    "    handle = cp.cuda.runtime.ipcGetMemHandle(arr.data.ptr)\n",
    "    print(f\"Parent: Got IPC handle, type: {type(handle)}, len: {len(handle) if hasattr(handle, '__len__') else 'N/A'}\")\n",
    "    \n",
    "    # Start child process - pass handle as bytes\n",
    "    result_queue = mp.Queue()\n",
    "    p = mp.Process(target=child_process, args=(handle, arr.shape, arr.dtype, arr.nbytes))\n",
    "    p.start()\n",
    "    p.join()\n",
    "    \n",
    "    if p.exitcode == 0:\n",
    "        print(f\"Parent: Array after child ran: {arr[:5]}\")\n",
    "    else:\n",
    "        print(f\"Parent: Child process failed with exit code {p.exitcode}\")\n",
    "except Exception as e:\n",
    "    print(f\"Parent: Error - {e}\")\n",
    "    print(\"Note: IPC may not be supported on all GPU configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a85426",
   "metadata": {},
   "source": [
    "## Important Notes on IPC\n",
    "\n",
    "**Why IPC often fails:**\n",
    "- Not all CUDA memory allocations support IPC\n",
    "- IPC requires specific GPU hardware support (works on most modern NVIDIA GPUs)\n",
    "- WSL2 and some virtualized environments don't support CUDA IPC\n",
    "- IPC only works between processes on the **same physical machine**\n",
    "\n",
    "**When does your GPU support IPC?**\n",
    "Check with: `nvidia-smi -q | grep \"IPC\"` or run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b26327f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPC information not found in nvidia-smi output\n",
      "This doesn't necessarily mean IPC is unsupported - try the test above\n"
     ]
    }
   ],
   "source": [
    "# Check if IPC is supported on your GPU\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi', '-q'], capture_output=True, text=True)\n",
    "if 'IPC' in result.stdout:\n",
    "    for line in result.stdout.split('\\n'):\n",
    "        if 'IPC' in line or 'Compute Mode' in line:\n",
    "            print(line)\n",
    "else:\n",
    "    print(\"IPC information not found in nvidia-smi output\")\n",
    "    print(\"This doesn't necessarily mean IPC is unsupported - try the test above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f1383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in WSL2 (where IPC often doesn't work)\n",
    "import os\n",
    "is_wsl = os.path.exists('/proc/version') and 'microsoft' in open('/proc/version').read().lower()\n",
    "print(f\"Running in WSL: {is_wsl}\")\n",
    "\n",
    "# Check CUDA IPC properties\n",
    "props = cp.cuda.runtime.getDeviceProperties(0)\n",
    "print(f\"Device name: {props['name'].decode()}\")\n",
    "print(f\"Compute capability: {props['major']}.{props['minor']}\")\n",
    "\n",
    "# Try a simple IPC test\n",
    "try:\n",
    "    test_arr = cp.zeros(10, dtype=cp.float32)\n",
    "    test_handle = cp.cuda.runtime.ipcGetMemHandle(test_arr.data.ptr)\n",
    "    print(\"✓ IPC handle creation works\")\n",
    "    \n",
    "    # The real test is opening it in another process, which we saw fails\n",
    "    print(\"✗ IPC handle opening in child process fails (as seen above)\")\n",
    "    print(\"\\nConclusion: CUDA IPC is NOT working in your environment\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ IPC handle creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f00cf2",
   "metadata": {},
   "source": [
    "## Practical Alternative: Multi-Process GPU Computing WITHOUT IPC\n",
    "\n",
    "Since IPC doesn't work in your environment, here's how to share GPU work between processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "import numpy as np\n",
    "\n",
    "def worker_process(input_data, output_queue, worker_id):\n",
    "    \"\"\"Worker that does GPU computation\"\"\"\n",
    "    import cupy as cp\n",
    "    \n",
    "    # Each process gets its own GPU arrays\n",
    "    arr = cp.array(input_data)\n",
    "    \n",
    "    # Do some GPU computation\n",
    "    result = arr * worker_id + 100\n",
    "    \n",
    "    # Return results via CPU memory\n",
    "    output_queue.put(result.get())\n",
    "    print(f\"Worker {worker_id}: Processed {len(arr)} elements\")\n",
    "\n",
    "# Parent: Create data\n",
    "data = np.arange(100, dtype=np.float32)\n",
    "print(f\"Parent: Input data: {data[:5]}\")\n",
    "\n",
    "# Spawn multiple workers\n",
    "output_queue = mp.Queue()\n",
    "processes = []\n",
    "\n",
    "for i in range(3):\n",
    "    p = mp.Process(target=worker_process, args=(data, output_queue, i))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "for _ in range(3):\n",
    "    results.append(output_queue.get())\n",
    "\n",
    "# Wait for all processes\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "print(f\"\\nCollected {len(results)} results:\")\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"  Worker {i}: {r[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f95dbd",
   "metadata": {},
   "source": [
    "## Summary: CUDA IPC Limitations\n",
    "\n",
    "**Your situation:** CUDA IPC is **not working** in your environment (likely WSL2 or virtualization)\n",
    "\n",
    "**What works instead:**\n",
    "1. ✓ **Copy data between processes** via NumPy arrays (as shown above)\n",
    "2. ✓ **Save/load arrays** to disk (fast with NVMe SSDs)\n",
    "3. ✓ **Use CUDA streams** for async operations within one process\n",
    "4. ✓ **Multi-GPU with one process** using `cp.cuda.Device(gpu_id)`\n",
    "\n",
    "**What doesn't work:**\n",
    "- ✗ Zero-copy sharing of GPU memory between processes via IPC\n",
    "- ✗ Saving IPC handles to disk for later use\n",
    "\n",
    "**When would IPC work?**\n",
    "- Native Linux (not WSL2)\n",
    "- Bare metal CUDA installation\n",
    "- Modern NVIDIA GPU (compute capability 2.0+)\n",
    "- No virtualization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a417f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GPU array metadata and data\n",
    "arr = cp.arange(100, dtype=cp.float32) * 2\n",
    "\n",
    "# Method 1: Save to numpy file\n",
    "cp.save(kgs.temp_dir + '/my_array.npy', arr)\n",
    "\n",
    "# Later, load it back\n",
    "arr_loaded = cp.load(kgs.temp_dir + '/my_array.npy')\n",
    "print(\"Loaded array:\", arr_loaded[:5])\n",
    "\n",
    "# Method 2: Use pickle for metadata + data pointer info (within same process)\n",
    "data_dict = {\n",
    "    'shape': arr.shape,\n",
    "    'dtype': arr.dtype,\n",
    "    'ptr': arr.data.ptr,\n",
    "    'data': arr.get()  # Copy to CPU for saving\n",
    "}\n",
    "kgs.dill_save(kgs.temp_dir + '/array_data.pickle', data_dict)\n",
    "\n",
    "# Load it back\n",
    "loaded_dict = kgs.dill_load(kgs.temp_dir + '/array_data.pickle')\n",
    "arr_restored = cp.array(loaded_dict['data'])\n",
    "print(\"Restored array:\", arr_restored[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
