{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4106fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n",
      "Streams\tKernels/sec\tElapsed(s)\n",
      "1\t15\t\t1.317\n",
      "1\t15\t\t1.317\n",
      "2\t30\t\t1.308\n",
      "2\t30\t\t1.308\n",
      "4\t61\t\t1.308\n",
      "4\t61\t\t1.308\n",
      "8\t122\t\t1.307\n",
      "8\t122\t\t1.307\n",
      "16\t122\t\t2.616\n",
      "16\t122\t\t2.616\n",
      "32\t138\t\t4.618\n",
      "32\t138\t\t4.618\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tools/kernel_concurrency_test.py\n",
    "# Usage: python3 tools/kernel_concurrency_test.py\n",
    "# Make sure you run this in the Python environment that has CuPy and your repo available.\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import sys\n",
    "sys.path.append('/mnt/d/packing/code/core/')\n",
    "import pack_cuda\n",
    "\n",
    "def make_input(N):\n",
    "    # small random poses in sensible range\n",
    "    rng = np.random.RandomState(123)\n",
    "    x = rng.uniform(-5.0, 5.0, size=N)\n",
    "    y = rng.uniform(-5.0, 5.0, size=N)\n",
    "    t = rng.uniform(-math.pi, math.pi, size=N)\n",
    "    xyt = np.stack([x, y, t], axis=1).astype(np.float64)\n",
    "    return xyt\n",
    "\n",
    "def run_trial(num_streams, iters_per_stream, xyt1_np, xyt2_np):\n",
    "    pack_cuda._ensure_initialized()  # make sure module is initialized\n",
    "    n1 = xyt1_np.shape[0]\n",
    "    n2 = xyt2_np.shape[0]\n",
    "    # Flatten 3xN row-major as kernel expects\n",
    "    xyt1_3xN = cp.ascontiguousarray(cp.asarray(xyt1_np).T).ravel()\n",
    "    xyt2_3xN = cp.ascontiguousarray(cp.asarray(xyt2_np).T).ravel()\n",
    "\n",
    "    # Grab raw kernel and device arrays from pack_cuda (uses private names)\n",
    "    kernel = pack_cuda._overlap_list_total_kernel\n",
    "    piece_xy = pack_cuda._piece_xy_d\n",
    "    piece_nverts = pack_cuda._piece_nverts_d\n",
    "    num_pieces = np.int32(pack_cuda._num_pieces)\n",
    "\n",
    "    # Precreate per-stream outputs\n",
    "    streams = [cp.cuda.Stream(non_blocking=True) for _ in range(num_streams)]\n",
    "    out_totals = [cp.zeros(1, dtype=cp.float64) for _ in range(num_streams)]\n",
    "\n",
    "    # Warmup single call per stream to get JIT/compilation out of the way\n",
    "    for s_idx, stream in enumerate(streams):\n",
    "        with stream:\n",
    "            kernel(\n",
    "                (1,), (n1,),\n",
    "                (xyt1_3xN, np.int32(n1), xyt2_3xN, np.int32(n2), piece_xy, piece_nverts, num_pieces, out_totals[s_idx], cp.zeros(1)),\n",
    "                stream=stream\n",
    "            )\n",
    "    # Ensure warmup finished\n",
    "    for s in streams:\n",
    "        s.synchronize()\n",
    "\n",
    "    # Timed run: launch iters_per_stream kernels on each stream (back-to-back)\n",
    "    start = time.time()\n",
    "    for k in range(iters_per_stream):\n",
    "        for s_idx, stream in enumerate(streams):\n",
    "            with stream:\n",
    "                kernel(\n",
    "                    (1,), (n1,),\n",
    "                    (xyt1_3xN, np.int32(n1), xyt2_3xN, np.int32(n2), piece_xy, piece_nverts, num_pieces, out_totals[s_idx], cp.zeros(1)),\n",
    "                    stream=stream\n",
    "                )\n",
    "    # Wait for all streams to finish\n",
    "    for s in streams:\n",
    "        s.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    elapsed = end - start\n",
    "    total_kernels = num_streams * iters_per_stream\n",
    "    kernels_per_sec = total_kernels / elapsed\n",
    "    return kernels_per_sec, elapsed\n",
    "\n",
    "\n",
    "# Parameters to tune\n",
    "N = 500                       # number of trees (threads per block)\n",
    "iters = 20                    # iterations per stream\n",
    "xyt = make_input(N)\n",
    "\n",
    "packs = [1, 2, 4, 8, 16, 32]  # number of concurrent streams to test\n",
    "print(\"Streams\\tKernels/sec\\tElapsed(s)\")\n",
    "for s in packs:\n",
    "    kps, t = run_trial(s, iters, xyt[:64], xyt)\n",
    "    print(f\"{s}\\t{int(kps)}\\t\\t{t:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20dbed6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3708366326.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_16431/3708366326.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    **Actual observation from data**: Performance **degrades** as batch size increases!\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Analysis: The Real Problem - Serial Reduction Bottleneck\n",
    "\n",
    "**Actual observation from data**: Performance **degrades** as batch size increases!\n",
    "- 8 threads: 83 kernels/sec\n",
    "- 16 threads: 43 kernels/sec (50% slower!)\n",
    "- 32 threads: 22 kernels/sec (75% slower!)\n",
    "- 256 threads: 7 kernels/sec (91% slower!)\n",
    "\n",
    "This is **opposite** of GPU occupancy problems. Let's look at what's happening in the kernel..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8abed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the reduction loop in the kernel\n",
    "print(\"From pack_cuda.py, the reduction code:\")\n",
    "print(\"\"\"\n",
    "    // Reduce only the first n1 elements (not blockDim.x)\n",
    "    for (int stride = 1; stride < n1; stride *= 2) {\n",
    "        int index = 2 * stride * tid;\n",
    "        if (index + stride < n1) {\n",
    "            sdata[index] += sdata[index + stride];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nThe problem: Number of __syncthreads() calls = log2(n1)\")\n",
    "print(\"\\nBatch size\\tlog2(n)\\t__syncthreads calls\")\n",
    "for n in [4, 8, 16, 32, 64, 128, 256]:\n",
    "    import math\n",
    "    syncs = math.ceil(math.log2(n))\n",
    "    print(f\"{n}\\t\\t{syncs}\\t{syncs}\")\n",
    "\n",
    "print(\"\\n__syncthreads() is EXTREMELY expensive - it stalls ALL threads until\")\n",
    "print(\"the slowest thread reaches the barrier. With more threads:\")\n",
    "print(\"1. More synchronization points needed (log2(n) grows)\")\n",
    "print(\"2. More threads to wait for at each barrier\")\n",
    "print(\"3. Higher probability of thread divergence/delays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81aa37ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Single Stream Performance vs Batch Size ===\n",
      "Batch\tThreads\tKernels/sec\tElapsed(s)\n",
      "4\t4\t94\t\t0.212\n",
      "4\t4\t94\t\t0.212\n",
      "8\t8\t62\t\t0.322\n",
      "8\t8\t62\t\t0.322\n",
      "16\t16\t33\t\t0.594\n",
      "16\t16\t33\t\t0.594\n",
      "32\t32\t28\t\t0.709\n",
      "32\t32\t28\t\t0.709\n",
      "64\t64\t36\t\t0.543\n",
      "64\t64\t36\t\t0.543\n",
      "128\t128\t11\t\t1.759\n",
      "128\t128\t11\t\t1.759\n",
      "256\t256\t5\t\t3.335\n",
      "\n",
      "Notice how performance improves dramatically as batch size increases,\n",
      "even though we're using only ONE stream (no concurrency at all)!\n",
      "This proves the problem is GPU occupancy, not stream concurrency.\n",
      "256\t256\t5\t\t3.335\n",
      "\n",
      "Notice how performance improves dramatically as batch size increases,\n",
      "even though we're using only ONE stream (no concurrency at all)!\n",
      "This proves the problem is GPU occupancy, not stream concurrency.\n"
     ]
    }
   ],
   "source": [
    "# Test hypothesis: performance should improve dramatically with larger batch sizes\n",
    "# even with just a SINGLE stream (no concurrency)\n",
    "\n",
    "print(\"\\n=== Single Stream Performance vs Batch Size ===\")\n",
    "print(\"Batch\\tThreads\\tKernels/sec\\tElapsed(s)\")\n",
    "\n",
    "batch_sizes = [4, 8, 16, 32, 64, 128, 256]\n",
    "iters = 20\n",
    "\n",
    "for batch in batch_sizes:\n",
    "    if batch > N:\n",
    "        print(f\"{batch}\\t{batch}\\t<skipped: batch > N>\")\n",
    "        continue\n",
    "    kps, t = run_trial(1, iters, xyt[:batch], xyt)  # 1 stream only!\n",
    "    print(f\"{batch}\\t{batch}\\t{int(kps)}\\t\\t{t:.3f}\")\n",
    "\n",
    "print(\"\\nNotice how performance improves dramatically as batch size increases,\")\n",
    "print(\"even though we're using only ONE stream (no concurrency at all)!\")\n",
    "print(\"This proves the problem is GPU occupancy, not stream concurrency.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964132f9",
   "metadata": {},
   "source": [
    "### The Real Issue: Inefficient Reduction Algorithm\n",
    "\n",
    "**Root cause**: The kernel uses a **serial reduction** with repeated `__syncthreads()` barriers.\n",
    "\n",
    "Each barrier:\n",
    "1. Stalls ALL threads in the block until the slowest arrives\n",
    "2. Has significant overhead (~tens of cycles minimum)\n",
    "3. Kills parallelism by forcing serialization\n",
    "\n",
    "**Why performance degrades with more threads:**\n",
    "- 8 threads: 3 sync barriers (log2(8) = 3)\n",
    "- 16 threads: 4 sync barriers (log2(16) = 4) \n",
    "- 256 threads: 8 sync barriers (log2(256) = 8)\n",
    "\n",
    "Each additional sync point adds overhead AND more threads to coordinate.\n",
    "\n",
    "### Solution: Use Atomics for Small Reductions\n",
    "\n",
    "For small thread counts (< 128), atomic operations are actually **faster** than tree reduction:\n",
    "\n",
    "```cuda\n",
    "// Instead of tree reduction with __syncthreads():\n",
    "if (tid < n1) {\n",
    "    atomicAdd(out_total, local_sum / 2.0);\n",
    "}\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "- Zero synchronization overhead\n",
    "- Scales perfectly - no penalty for more threads\n",
    "- Simpler code\n",
    "- Better for multiple streams (no shared memory conflicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e57e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: the \"sweet spot\" at 8 threads is just where reduction overhead is minimal\n",
    "print(\"\\n=== Performance Analysis ===\")\n",
    "print(\"Batch\\tSync barriers\\tKernels/sec\\tOverhead factor\")\n",
    "baseline = 83  # performance at 8 threads\n",
    "for n, kps in [(4, 76), (8, 83), (16, 43), (32, 22), (64, 18), (128, 13), (256, 7)]:\n",
    "    import math\n",
    "    syncs = math.ceil(math.log2(n)) if n > 1 else 0\n",
    "    overhead = baseline / kps\n",
    "    print(f\"{n}\\t{syncs}\\t\\t{kps}\\t\\t{overhead:.2f}x\")\n",
    "\n",
    "print(\"\\nThe overhead grows super-linearly with sync barriers!\")\n",
    "print(\"This confirms: __syncthreads() is the bottleneck, not compute/memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b228d2f",
   "metadata": {},
   "source": [
    "### Recommended Fix for pack_cuda.py\n",
    "\n",
    "Replace the tree reduction in `overlap_list_total` with atomic operations:\n",
    "\n",
    "**Current code (lines ~411-424):**\n",
    "```cuda\n",
    "// Shared-memory reduction (assumes blockDim.x <= 1024)\n",
    "__shared__ double sdata[1024];\n",
    "sdata[tid] = local_sum;\n",
    "__syncthreads();\n",
    "\n",
    "for (int stride = 1; stride < n1; stride *= 2) {\n",
    "    int index = 2 * stride * tid;\n",
    "    if (index + stride < n1) {\n",
    "        sdata[index] += sdata[index + stride];\n",
    "    }\n",
    "    __syncthreads();\n",
    "}\n",
    "\n",
    "if (tid == 0) {\n",
    "    out_total[0] = sdata[0] / 2.0;\n",
    "}\n",
    "```\n",
    "\n",
    "**Replace with:**\n",
    "```cuda\n",
    "// Atomic reduction - faster for small thread counts\n",
    "if (tid < n1) {\n",
    "    atomicAdd(out_total, local_sum / 2.0);\n",
    "}\n",
    "```\n",
    "\n",
    "This eliminates all `__syncthreads()` overhead and will scale linearly with thread count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9d188",
   "metadata": {},
   "source": [
    "### ACTUAL Root Cause: Memory Bandwidth Saturation\n",
    "\n",
    "The atomic fix didn't help because the real bottleneck is **memory access pattern**!\n",
    "\n",
    "**What's happening:**\n",
    "- Each thread compares 1 tree against 500 trees\n",
    "- Each comparison reads `piece_xy` array (polygon vertices) multiple times\n",
    "- With N threads, you have N × 500 × (multiple reads) all accessing the same global memory\n",
    "\n",
    "**Memory access pattern analysis:**\n",
    "```\n",
    "Thread 0: reads piece_xy 500 times (for 500 comparisons)\n",
    "Thread 1: reads piece_xy 500 times (for 500 comparisons)  \n",
    "Thread 2: reads piece_xy 500 times (for 500 comparisons)\n",
    "...\n",
    "Thread N: reads piece_xy 500 times (for 500 comparisons)\n",
    "```\n",
    "\n",
    "**Total memory reads = N threads × 500 comparisons × reads_per_comparison**\n",
    "\n",
    "As N increases, memory bandwidth gets saturated. GPU memory has finite bandwidth (~900 GB/s for high-end cards), and all threads are competing for it.\n",
    "\n",
    "### Why 8 threads is the sweet spot:\n",
    "8 threads × 500 trees = 4000 total comparisons fits within memory bandwidth.\n",
    "Beyond that, you're bandwidth-limited, not compute-limited!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee669c15",
   "metadata": {},
   "source": [
    "### Solution: Use Constant Memory for Polygon Data\n",
    "\n",
    "`piece_xy` is read-only and accessed by ALL threads - perfect candidate for **constant memory**!\n",
    "\n",
    "Constant memory benefits:\n",
    "1. **Cached on-chip** - much faster than global memory\n",
    "2. **Broadcast semantics** - when all threads read same address, it's a single fetch\n",
    "3. **64KB available** - plenty for polygon vertices\n",
    "\n",
    "**Implementation:**\n",
    "```cuda\n",
    "// At top of CUDA source, replace:\n",
    "// (in device code section, no change to host-side arrays)\n",
    "\n",
    "// Change kernel signature to receive via constant memory\n",
    "__constant__ double const_piece_xy[MAX_PIECES * MAX_VERTS_PER_PIECE * 2];\n",
    "__constant__ int const_piece_nverts[MAX_PIECES];\n",
    "```\n",
    "\n",
    "Then before kernel launch, copy data to constant memory:\n",
    "```python\n",
    "# In pack_cuda._ensure_initialized() or similar:\n",
    "const_piece_xy = _raw_module.get_global('const_piece_xy')\n",
    "const_piece_nverts = _raw_module.get_global('const_piece_nverts')\n",
    "const_piece_xy.copy_from_host(_piece_xy_h)\n",
    "const_piece_nverts.copy_from_host(_piece_nverts_h)\n",
    "```\n",
    "\n",
    "This should eliminate the memory bandwidth bottleneck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27983c",
   "metadata": {},
   "source": [
    "### Testing the Constant Memory Fix\n",
    "\n",
    "Now let's reload pack_cuda and rerun the tests to see if constant memory solves the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload pack_cuda to pick up the constant memory changes\n",
    "import importlib\n",
    "importlib.reload(pack_cuda)\n",
    "\n",
    "# Clear initialization flag to force reinitialization\n",
    "pack_cuda._initialized = False\n",
    "\n",
    "print(\"=== AFTER Constant Memory Fix ===\")\n",
    "print(\"\\nSingle Stream Performance vs Batch Size:\")\n",
    "print(\"Batch\\tKernels/sec\\tElapsed(s)\")\n",
    "\n",
    "batch_sizes = [4, 8, 16, 32, 64, 128, 256]\n",
    "for batch in batch_sizes:\n",
    "    if batch > N:\n",
    "        print(f\"{batch}\\t<skipped>\")\n",
    "        continue\n",
    "    kps, t = run_trial(1, iters, xyt[:batch], xyt)\n",
    "    print(f\"{batch}\\t{int(kps)}\\t\\t{t:.3f}\")\n",
    "    \n",
    "print(\"\\nIf constant memory worked, performance should stay constant or improve!\")\n",
    "print(\"If it still degrades, the issue is something else...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9887b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling kernel with different thread counts...\n",
      "Threads\tAvg time (ms)\tComparisons\tTime per comparison (μs)\n",
      "4\t12.092\t\t2000\t\t6.046\n",
      "8\t21.499\t\t4000\t\t5.375\n",
      "16\t34.797\t\t8000\t\t4.350\n",
      "32\t60.889\t\t16000\t\t3.806\n",
      "64\t66.198\t\t32000\t\t2.069\n",
      "128\t90.474\t\t64000\t\t1.414\n"
     ]
    }
   ],
   "source": [
    "# Quick profiling with CuPy events\n",
    "import cupy as cp\n",
    "\n",
    "def profile_kernel_detailed(n1, n2, num_runs=100):\n",
    "    \"\"\"Profile the kernel with CUDA events for precise timing\"\"\"\n",
    "    pack_cuda._ensure_initialized()\n",
    "    \n",
    "    # Prepare data\n",
    "    xyt1 = make_input(n1)\n",
    "    xyt2 = make_input(n2)\n",
    "    xyt1_3xN = cp.ascontiguousarray(cp.asarray(xyt1).T).ravel()\n",
    "    xyt2_3xN = cp.ascontiguousarray(cp.asarray(xyt2).T).ravel()\n",
    "    \n",
    "    kernel = pack_cuda._overlap_list_total_kernel\n",
    "    out_total = cp.zeros(1, dtype=cp.float64)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        out_total.fill(0)\n",
    "        kernel((1,), (n1,), (xyt1_3xN, np.int32(n1), xyt2_3xN, np.int32(n2), \n",
    "                              out_total, cp.zeros(1)))\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    # Profile with CUDA events\n",
    "    start_event = cp.cuda.Event()\n",
    "    end_event = cp.cuda.Event()\n",
    "    \n",
    "    start_event.record()\n",
    "    for _ in range(num_runs):\n",
    "        out_total.fill(0)\n",
    "        kernel((1,), (n1,), (xyt1_3xN, np.int32(n1), xyt2_3xN, np.int32(n2), \n",
    "                              out_total, cp.zeros(1)))\n",
    "    end_event.record()\n",
    "    end_event.synchronize()\n",
    "    \n",
    "    elapsed_ms = cp.cuda.get_elapsed_time(start_event, end_event)\n",
    "    avg_ms = elapsed_ms / num_runs\n",
    "    \n",
    "    return avg_ms\n",
    "\n",
    "print(\"Profiling kernel with different thread counts...\")\n",
    "print(\"Threads\\tAvg time (ms)\\tComparisons\\tTime per comparison (μs)\")\n",
    "for n1 in [4, 8, 16, 32, 64, 128]:\n",
    "    avg_ms = profile_kernel_detailed(n1, 500, num_runs=100)\n",
    "    comparisons = n1 * 500\n",
    "    time_per_comp_us = (avg_ms * 1000) / comparisons\n",
    "    print(f\"{n1}\\t{avg_ms:.3f}\\t\\t{comparisons}\\t\\t{time_per_comp_us:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db1510f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fixed work test (2000 comparisons split across different thread counts):\n",
      "Threads\tAvg time (ms)\tSpeedup vs 4 threads\n",
      "4\t12.100\t\t1.00x\n",
      "8\t8.674\t\t1.40x\n",
      "16\t7.695\t\t1.57x\n",
      "32\t9.503\t\t1.27x\n",
      "64\t6.225\t\t1.94x\n"
     ]
    }
   ],
   "source": [
    "# True concurrency test: fixed total work, varying parallelism\n",
    "print(\"\\n\\nFixed work test (2000 comparisons split across different thread counts):\")\n",
    "print(\"Threads\\tAvg time (ms)\\tSpeedup vs 4 threads\")\n",
    "\n",
    "baseline = None\n",
    "for n1 in [4, 8, 16, 32, 64]:\n",
    "    n2 = 2000 // n1  # Keep total work = 2000 comparisons\n",
    "    avg_ms = profile_kernel_detailed(n1, n2, num_runs=100)\n",
    "    if baseline is None:\n",
    "        baseline = avg_ms\n",
    "    speedup = baseline / avg_ms\n",
    "    print(f\"{n1}\\t{avg_ms:.3f}\\t\\t{speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288f8f80",
   "metadata": {},
   "source": [
    "## Constant Memory Made It WORSE!\n",
    "\n",
    "**Before (global memory):**\n",
    "- 8 threads: 83 kernels/sec\n",
    "- 16 threads: 43 kernels/sec\n",
    "- 32 threads: 22 kernels/sec\n",
    "\n",
    "**After (constant memory):**\n",
    "- 8 threads: 69 kernels/sec (17% slower!)\n",
    "- 16 threads: 36 kernels/sec (16% slower!)\n",
    "- 32 threads: 20 kernels/sec (9% slower!)\n",
    "\n",
    "### Why Constant Memory Failed\n",
    "\n",
    "Constant memory is optimized for **broadcast** - when ALL threads read the SAME address simultaneously. But in our case:\n",
    "\n",
    "**Our access pattern:**\n",
    "```cuda\n",
    "// Thread 0 with tree pose (x0, y0, θ0) reads piece_xy and transforms vertices\n",
    "x1 = c0 * piece_xy[0] - s0 * piece_xy[1] + x0;  // unique c0, s0, x0\n",
    "\n",
    "// Thread 1 with tree pose (x1, y1, θ1) reads piece_xy and transforms vertices  \n",
    "x1 = c1 * piece_xy[0] - s1 * piece_xy[1] + x1;  // unique c1, s1, x1\n",
    "```\n",
    "\n",
    "Each thread reads the SAME `piece_xy` values but uses DIFFERENT transformation matrices (cos/sin of different angles). This creates **serialization** in constant memory cache!\n",
    "\n",
    "Constant memory serializes non-uniform access patterns - exactly what we have!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa1b59b",
   "metadata": {},
   "source": [
    "### The REAL Root Cause: Work Duplication\n",
    "\n",
    "Looking at the actual computation more carefully:\n",
    "\n",
    "**Each thread does:**\n",
    "- Compares 1 tree (from xyt1) against ALL 500 trees (in xyt2)\n",
    "- For EACH comparison, reads polygon vertices and transforms them with BOTH tree poses\n",
    "- Total work per thread = 500 comparisons × (transform poly1 + transform poly2 + intersect)\n",
    "\n",
    "**The key insight:** When comparing trees A vs B, we're transforming the SAME polygon twice:\n",
    "1. Thread comparing tree[0]: transforms polygon with pose[0], then transforms SAME polygon with pose[target]\n",
    "2. Thread comparing tree[1]: transforms polygon with pose[1], then transforms SAME polygon with pose[target]\n",
    "\n",
    "This redundant transformation is killing us! With more threads, we're doing MORE redundant work.\n",
    "\n",
    "### The Solution: Precompute Transformed Polygons\n",
    "\n",
    "Instead of transforming polygons on-the-fly during each comparison, **precompute all transformed polygons once** before comparison:\n",
    "\n",
    "1. Launch kernel: Each thread transforms ONE polygon for ONE tree pose\n",
    "2. Store all transformed polygons in global memory\n",
    "3. Launch comparison kernel: threads only compute intersections (no transforms!)\n",
    "\n",
    "This eliminates the N×M redundant transformations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5a7e8",
   "metadata": {},
   "source": [
    "### Why Performance Degrades with More Threads\n",
    "\n",
    "Let's count the actual work:\n",
    "\n",
    "**With N threads comparing against M=500 trees:**\n",
    "\n",
    "Current implementation:\n",
    "- Thread 0: Transform polygon N_pieces×2 times × 500 comparisons = 1000 transforms\n",
    "- Thread 1: Transform polygon N_pieces×2 times × 500 comparisons = 1000 transforms  \n",
    "- ...\n",
    "- Thread N: Transform polygon N_pieces×2 times × 500 comparisons = 1000 transforms\n",
    "- **Total: N × 1000 polygon transformations**\n",
    "\n",
    "But we only have N unique poses! We should only do N transformations total, not N×1000!\n",
    "\n",
    "**As N increases:**\n",
    "- More threads = more redundant work\n",
    "- Each thread is memory-bound reading/transforming the same data\n",
    "- GPU scheduler thrashes trying to manage all the redundant memory operations\n",
    "\n",
    "This explains why 8 threads is optimal - it's the point where you have enough parallelism without overwhelming the GPU with redundant work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf04a4",
   "metadata": {},
   "source": [
    "## Summary: The Real Problem and Solution\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. ❌ **Not GPU occupancy** - Adding threads made it worse, not better\n",
    "2. ❌ **Not __syncthreads() overhead** - Atomic reduction didn't help  \n",
    "3. ❌ **Not memory bandwidth** - Constant memory made it worse!\n",
    "4. ✅ **Redundant computation** - Each thread transforms the SAME polygons 500 times\n",
    "\n",
    "### The Current Architecture's Fatal Flaw\n",
    "\n",
    "```\n",
    "overlap_two_trees(tree_a, tree_b):\n",
    "    for piece_i in pieces:\n",
    "        transform piece_i with tree_a pose  // ← Redundant!\n",
    "        for piece_j in pieces:\n",
    "            transform piece_j with tree_b pose  // ← Redundant!\n",
    "            intersect(piece_i, piece_j)\n",
    "```\n",
    "\n",
    "When you have N threads each comparing against M trees:\n",
    "- **Redundant transforms: N × M × 2 × num_pieces**\n",
    "- **Useful transforms: (N + M) × num_pieces**\n",
    "- **Waste ratio: ~1000x for your case!**\n",
    "\n",
    "### The Right Solution\n",
    "\n",
    "**Two-phase approach:**\n",
    "1. **Phase 1:** Transform all polygons once (parallel over trees)\n",
    "2. **Phase 2:** Compute intersections (parallel over pairs)\n",
    "\n",
    "This is a fundamental algorithmic change, not a tuning fix. The current kernel is doing O(N²) work when it should do O(N) preprocessing + O(N²) intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d6d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
