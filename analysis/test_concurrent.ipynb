{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb4ff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FRESH START ===\n"
     ]
    }
   ],
   "source": [
    "# Clear all outputs and restart\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)\n",
    "print(\"=== FRESH START ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4106fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocks\tCalls/sec\tElapsed(s)\n",
      "1\t312\t\t0.032\n",
      "2\t351\t\t0.028\n",
      "4\t251\t\t0.040\n",
      "8\t294\t\t0.034\n",
      "16\t277\t\t0.036\n",
      "32\t235\t\t0.043\n",
      "64\t383\t\t0.026\n",
      "128\t202\t\t0.049\n",
      "32\t235\t\t0.043\n",
      "64\t383\t\t0.026\n",
      "128\t202\t\t0.049\n",
      "256\t70\t\t0.142\n",
      "256\t70\t\t0.142\n",
      "512\t28\t\t0.352\n",
      "512\t28\t\t0.352\n"
     ]
    }
   ],
   "source": [
    "# Multi-block kernel concurrency test\n",
    "# Tests the new multi_ensemble_kernel which launches one block per ensemble\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import sys\n",
    "\n",
    "# Clear and set path\n",
    "if '/mnt/d/packing/code/core/' in sys.path:\n",
    "    sys.path.remove('/mnt/d/packing/code/core/')\n",
    "sys.path.insert(0, '/mnt/d/packing/code/core/')\n",
    "\n",
    "import pack_cuda\n",
    "pack_cuda.USE_FLOAT32 = True\n",
    "\n",
    "def make_input(N):\n",
    "    # small random poses in sensible range\n",
    "    rng = np.random.RandomState(123)\n",
    "    x = rng.uniform(-5.0, 5.0, size=N)\n",
    "    y = rng.uniform(-5.0, 5.0, size=N)\n",
    "    t = rng.uniform(-math.pi, math.pi, size=N)\n",
    "    xyt = np.stack([x, y, t], axis=1).astype(np.float64)\n",
    "    return xyt\n",
    "\n",
    "def run_trial(num_blocks, iters_per_call, xyt1_np, xyt2_np):\n",
    "    pack_cuda._ensure_initialized()\n",
    "    \n",
    "    # Convert to GPU once to avoid repeated pinned memory allocations\n",
    "    xyt1_gpu = cp.asarray(xyt1_np, dtype=cp.float32 if pack_cuda.USE_FLOAT32 else cp.float64)\n",
    "    xyt2_gpu = cp.asarray(xyt2_np, dtype=cp.float32 if pack_cuda.USE_FLOAT32 else cp.float64)\n",
    "    \n",
    "    # Create list of ensembles (all identical for this test)\n",
    "    xyt1_list = [xyt1_gpu for _ in range(num_blocks)]\n",
    "    xyt2_list = [xyt2_gpu for _ in range(num_blocks)]\n",
    "    \n",
    "    # Warmup with full synchronization\n",
    "    totals, grads = pack_cuda.overlap_multi_ensemble(xyt1_list, xyt2_list, compute_grad=True)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    \n",
    "    # Timed run with proper synchronization\n",
    "    cp.cuda.Device().synchronize()\n",
    "    start = time.perf_counter()\n",
    "    for k in range(iters_per_call):\n",
    "        totals, grads = pack_cuda.overlap_multi_ensemble(xyt1_list, xyt2_list, compute_grad=True)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    elapsed = end - start\n",
    "    total_calls = iters_per_call\n",
    "    calls_per_sec = total_calls / elapsed\n",
    "    return calls_per_sec, elapsed\n",
    "\n",
    "\n",
    "# Initialize everything first\n",
    "pack_cuda._ensure_initialized()\n",
    "\n",
    "# Parameters to tune\n",
    "N = 20\n",
    "iters = 10\n",
    "xyt = make_input(N)\n",
    "\n",
    "cp.cuda.Device().synchronize()\n",
    "\n",
    "packs = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "print(\"Blocks\\tCalls/sec\\tElapsed(s)\")\n",
    "for b in packs:\n",
    "    cps, t = run_trial(b, iters, xyt, xyt)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    print(f\"{b}\\t{int(cps)}\\t\\t{t:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485d30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87ca493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 Ti\n",
      "Number of SMs: 60\n",
      "Max threads per SM: 1536\n",
      "Max threads per block: 1024\n",
      "Max blocks per SM: 1\n"
     ]
    }
   ],
   "source": [
    "# Check GPU properties\n",
    "props = cp.cuda.runtime.getDeviceProperties(0)\n",
    "print(f\"GPU: {props['name'].decode()}\")\n",
    "print(f\"Number of SMs: {props['multiProcessorCount']}\")\n",
    "print(f\"Max threads per SM: {props['maxThreadsPerMultiProcessor']}\")\n",
    "print(f\"Max threads per block: {props['maxThreadsPerBlock']}\")\n",
    "print(f\"Max blocks per SM: {props['maxThreadsPerMultiProcessor'] // props['maxThreadsPerBlock']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b7f0df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 12080\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA version: {cp.cuda.runtime.runtimeGetVersion()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae781f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac71fe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing with N=50 (50 threads/block) ===\n",
      "Blocks\tCalls/sec\tElapsed(s)\n",
      "1\t128\t\t0.311\n",
      "2\t788\t\t0.051\n",
      "4\t770\t\t0.052\n",
      "8\t759\t\t0.053\n",
      "1\t128\t\t0.311\n",
      "2\t788\t\t0.051\n",
      "4\t770\t\t0.052\n",
      "8\t759\t\t0.053\n",
      "16\t730\t\t0.055\n",
      "32\t605\t\t0.066\n",
      "64\t375\t\t0.106\n",
      "16\t730\t\t0.055\n",
      "32\t605\t\t0.066\n",
      "64\t375\t\t0.106\n",
      "128\t181\t\t0.221\n",
      "128\t181\t\t0.221\n"
     ]
    }
   ],
   "source": [
    "# Test with smaller N to see if occupancy improves\n",
    "N_small = 50\n",
    "iters_small = 40\n",
    "xyt_small = make_input(N_small)\n",
    "\n",
    "packs_test = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "print(\"\\n=== Testing with N=50 (50 threads/block) ===\")\n",
    "print(\"Blocks\\tCalls/sec\\tElapsed(s)\")\n",
    "for b in packs_test:\n",
    "    cps, t = run_trial(b, iters_small, xyt_small, xyt_small)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    print(f\"{b}\\t{int(cps)}\\t\\t{t:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "215cb002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparing multi-ensemble vs separate kernel launches (N=20) ===\n",
      "Blocks\tMulti\tSeparate\n",
      "1\t2064\t5483\n",
      "2\t2973\t3530\n",
      "4\t1577\t1681\n",
      "8\t933\t893\n",
      "16\t1304\t444\n",
      "32\t436\t220\n",
      "64\t356\t110\n",
      "64\t356\t110\n"
     ]
    }
   ],
   "source": [
    "# Test: Compare multi_ensemble vs launching separate kernels\n",
    "# To see if the issue is with the multi-ensemble kernel itself\n",
    "\n",
    "def run_separate_kernels(num_blocks, iters_per_call, xyt1_np, xyt2_np):\n",
    "    \"\"\"Launch num_blocks separate single-block kernels instead of one multi-block kernel\"\"\"\n",
    "    pack_cuda._ensure_initialized()\n",
    "    \n",
    "    xyt1_gpu = cp.asarray(xyt1_np, dtype=cp.float32 if pack_cuda.USE_FLOAT32 else cp.float64)\n",
    "    xyt2_gpu = cp.asarray(xyt2_np, dtype=cp.float32 if pack_cuda.USE_FLOAT32 else cp.float64)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(num_blocks):\n",
    "        totals, grads = pack_cuda.overlap_list_total(xyt1_gpu, xyt2_gpu, compute_grad=True)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    \n",
    "    # Timed run\n",
    "    cp.cuda.Device().synchronize()\n",
    "    start = time.perf_counter()\n",
    "    for k in range(iters_per_call):\n",
    "        for _ in range(num_blocks):\n",
    "            totals, grads = pack_cuda.overlap_list_total(xyt1_gpu, xyt2_gpu, compute_grad=True)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    elapsed = end - start\n",
    "    return iters_per_call / elapsed, elapsed\n",
    "\n",
    "print(\"\\n=== Comparing multi-ensemble vs separate kernel launches (N=20) ===\")\n",
    "print(\"Blocks\\tMulti\\tSeparate\")\n",
    "for b in [1, 2, 4, 8, 16, 32, 64]:\n",
    "    cps_multi, _ = run_trial(b, iters, xyt, xyt)\n",
    "    cps_sep, _ = run_separate_kernels(b, iters, xyt, xyt)\n",
    "    print(f\"{b}\\t{int(cps_multi)}\\t{int(cps_sep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ef0cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel attributes:\n",
      "  Max threads per block: 896\n",
      "  Num regs: 72\n",
      "  Shared size bytes: 0\n",
      "  Const size bytes: 144\n",
      "  Local size bytes: 1520\n",
      "\n",
      "With 20 threads/block:\n",
      "  Max blocks/SM (thread limit): 76\n",
      "  Max active threads/SM: 1520\n"
     ]
    }
   ],
   "source": [
    "# Check actual kernel occupancy\n",
    "# Try to determine how many blocks can run per SM\n",
    "\n",
    "# Get the kernel function\n",
    "pack_cuda._ensure_initialized()\n",
    "kernel = pack_cuda._multi_ensemble_kernel\n",
    "\n",
    "# Check kernel attributes\n",
    "print(\"Kernel attributes:\")\n",
    "print(f\"  Max threads per block: {kernel.max_threads_per_block}\")\n",
    "print(f\"  Num regs: {kernel.num_regs}\")\n",
    "print(f\"  Shared size bytes: {kernel.shared_size_bytes}\")\n",
    "print(f\"  Const size bytes: {kernel.const_size_bytes}\")\n",
    "print(f\"  Local size bytes: {kernel.local_size_bytes}\")\n",
    "\n",
    "# Calculate theoretical occupancy\n",
    "threads_per_block = 20  # Our N\n",
    "max_threads_per_sm = props['maxThreadsPerMultiProcessor']\n",
    "max_blocks_per_sm_thread = max_threads_per_sm // threads_per_block\n",
    "\n",
    "print(f\"\\nWith {threads_per_block} threads/block:\")\n",
    "print(f\"  Max blocks/SM (thread limit): {max_blocks_per_sm_thread}\")\n",
    "print(f\"  Max active threads/SM: {min(max_threads_per_sm, max_blocks_per_sm_thread * threads_per_block)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25992d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf33c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Register analysis:\n",
      "  Registers per SM: 65536\n",
      "  Registers per thread: 72\n",
      "  Registers per block (N=20): 1440\n",
      "  Max blocks/SM (register limit): 45\n",
      "  Max blocks/SM (thread limit): 76\n",
      "  Actual max blocks/SM: 45\n",
      "\n",
      "With 60 SMs:\n",
      "  Max concurrent blocks: 2700\n",
      "  Max concurrent threads: 54000\n"
     ]
    }
   ],
   "source": [
    "# Calculate register-limited occupancy\n",
    "# RTX 4070 Ti (Ada Lovelace): 65,536 registers per SM\n",
    "\n",
    "regs_per_sm = 65536\n",
    "regs_per_thread = kernel.num_regs\n",
    "threads_per_block = 20\n",
    "\n",
    "# How many blocks can fit based on registers?\n",
    "regs_per_block = regs_per_thread * threads_per_block\n",
    "max_blocks_per_sm_regs = regs_per_sm // regs_per_block\n",
    "\n",
    "print(f\"Register analysis:\")\n",
    "print(f\"  Registers per SM: {regs_per_sm}\")\n",
    "print(f\"  Registers per thread: {regs_per_thread}\")\n",
    "print(f\"  Registers per block (N={threads_per_block}): {regs_per_block}\")\n",
    "print(f\"  Max blocks/SM (register limit): {max_blocks_per_sm_regs}\")\n",
    "print(f\"  Max blocks/SM (thread limit): {max_blocks_per_sm_thread}\")\n",
    "print(f\"  Actual max blocks/SM: {min(max_blocks_per_sm_regs, max_blocks_per_sm_thread)}\")\n",
    "print(f\"\\nWith 60 SMs:\")\n",
    "print(f\"  Max concurrent blocks: {60 * min(max_blocks_per_sm_regs, max_blocks_per_sm_thread)}\")\n",
    "print(f\"  Max concurrent threads: {60 * min(max_blocks_per_sm_regs, max_blocks_per_sm_thread) * threads_per_block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a422e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f03cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPU kernel execution time (N=20) ===\n",
      "Blocks\tGPU time(s)\tCalls/sec\n",
      "1\t0.033\t\t303\n",
      "2\t0.032\t\t310\n",
      "4\t0.042\t\t239\n",
      "8\t0.043\t\t234\n",
      "16\t0.042\t\t237\n",
      "32\t0.052\t\t191\n",
      "64\t0.035\t\t288\n",
      "128\t0.080\t\t125\n",
      "256\t0.122\t\t81\n",
      "512\t0.338\t\t29\n",
      "1024\t0.559\t\t17\n"
     ]
    }
   ],
   "source": [
    "# Check if this is actually compute-bound or something else\n",
    "# Let's measure kernel execution time vs total time\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "def run_with_events(num_blocks, iters_per_call, xyt1_np, xyt2_np):\n",
    "    \"\"\"Measure GPU execution time using CUDA events\"\"\"\n",
    "    pack_cuda._ensure_initialized()\n",
    "    \n",
    "    xyt1_gpu = cp.asarray(xyt1_np, dtype=cp.float32 if pack_cuda.USE_FLOAT32 else cp.float64)\n",
    "    xyt2_gpu = cp.asarray(xyt2_np, dtype=cp.float32 if pack_cuda.USE_FLOAT32 else cp.float64)\n",
    "    \n",
    "    xyt1_list = [xyt1_gpu for _ in range(num_blocks)]\n",
    "    xyt2_list = [xyt2_gpu for _ in range(num_blocks)]\n",
    "    \n",
    "    # Warmup\n",
    "    totals, grads = pack_cuda.overlap_multi_ensemble(xyt1_list, xyt2_list, compute_grad=True)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    \n",
    "    # Measure with events\n",
    "    start_event = cp.cuda.Event()\n",
    "    end_event = cp.cuda.Event()\n",
    "    \n",
    "    start_event.record()\n",
    "    for k in range(iters_per_call):\n",
    "        totals, grads = pack_cuda.overlap_multi_ensemble(xyt1_list, xyt2_list, compute_grad=True)\n",
    "    end_event.record()\n",
    "    end_event.synchronize()\n",
    "    \n",
    "    gpu_time_ms = cp.cuda.get_elapsed_time(start_event, end_event)\n",
    "    return gpu_time_ms / 1000.0  # Convert to seconds\n",
    "\n",
    "print(\"\\n=== GPU kernel execution time (N=20) ===\")\n",
    "print(\"Blocks\\tGPU time(s)\\tCalls/sec\")\n",
    "for b in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "    gpu_time = run_with_events(b, iters, xyt, xyt)\n",
    "    cps = iters / gpu_time\n",
    "    print(f\"{b}\\t{gpu_time:.3f}\\t\\t{int(cps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21f5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
