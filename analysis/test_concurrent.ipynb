{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb4ff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FRESH START ===\n"
     ]
    }
   ],
   "source": [
    "# Clear all outputs and restart\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)\n",
    "print(\"=== FRESH START ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4106fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocks\tCalls/sec\tElapsed(s)\n",
      "1\t312\t\t0.032\n",
      "2\t351\t\t0.028\n",
      "4\t251\t\t0.040\n",
      "8\t294\t\t0.034\n",
      "16\t277\t\t0.036\n",
      "32\t235\t\t0.043\n",
      "64\t383\t\t0.026\n",
      "128\t202\t\t0.049\n",
      "32\t235\t\t0.043\n",
      "64\t383\t\t0.026\n",
      "128\t202\t\t0.049\n",
      "256\t70\t\t0.142\n",
      "256\t70\t\t0.142\n",
      "512\t28\t\t0.352\n",
      "512\t28\t\t0.352\n"
     ]
    }
   ],
   "source": [
    "# Multi-block kernel concurrency test\n",
    "# Tests the new multi_ensemble_kernel which launches one block per ensemble\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import sys\n",
    "\n",
    "# Clear and set path\n",
    "if '/mnt/d/packing/code/core/' in sys.path:\n",
    "    sys.path.remove('/mnt/d/packing/code/core/')\n",
    "sys.path.insert(0, '/mnt/d/packing/code/core/')\n",
    "\n",
    "import pack_cuda\n",
    "pack_cuda.USE_FLOAT32 = True\n",
    "\n",
    "def make_input(N):\n",
    "    # small random poses in sensible range\n",
    "    rng = np.random.RandomState(123)\n",
    "    x = rng.uniform(-5.0, 5.0, size=N)\n",
    "    y = rng.uniform(-5.0, 5.0, size=N)\n",
    "    t = rng.uniform(-math.pi, math.pi, size=N)\n",
    "    xyt = np.stack([x, y, t], axis=1).astype(np.float64)\n",
    "    return xyt\n",
    "\n",
    "def run_trial(num_blocks, iters_per_call, xyt1_np, xyt2_np):\n",
    "    pack_cuda._ensure_initialized()\n",
    "    \n",
    "    # Convert to GPU once to avoid repeated pinned memory allocations\n",
    "    xyt1_gpu = cp.asarray(xyt1_np, dtype=cp.float32 if pack_cuda.USE_FLOAT32 else cp.float64)\n",
    "    xyt2_gpu = cp.asarray(xyt2_np, dtype=cp.float32 if pack_cuda.USE_FLOAT32 else cp.float64)\n",
    "    \n",
    "    # Create list of ensembles (all identical for this test)\n",
    "    xyt1_list = [xyt1_gpu for _ in range(num_blocks)]\n",
    "    xyt2_list = [xyt2_gpu for _ in range(num_blocks)]\n",
    "    \n",
    "    # Warmup with full synchronization\n",
    "    totals, grads = pack_cuda.overlap_multi_ensemble(xyt1_list, xyt2_list, compute_grad=True)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    \n",
    "    # Timed run with proper synchronization\n",
    "    cp.cuda.Device().synchronize()\n",
    "    start = time.perf_counter()\n",
    "    for k in range(iters_per_call):\n",
    "        totals, grads = pack_cuda.overlap_multi_ensemble(xyt1_list, xyt2_list, compute_grad=True)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    elapsed = end - start\n",
    "    total_calls = iters_per_call\n",
    "    calls_per_sec = total_calls / elapsed\n",
    "    return calls_per_sec, elapsed\n",
    "\n",
    "\n",
    "# Initialize everything first\n",
    "pack_cuda._ensure_initialized()\n",
    "\n",
    "# Parameters to tune\n",
    "N = 20\n",
    "iters = 10\n",
    "xyt = make_input(N)\n",
    "\n",
    "cp.cuda.Device().synchronize()\n",
    "\n",
    "packs = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "print(\"Blocks\\tCalls/sec\\tElapsed(s)\")\n",
    "for b in packs:\n",
    "    cps, t = run_trial(b, iters, xyt, xyt)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    print(f\"{b}\\t{int(cps)}\\t\\t{t:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485d30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87ca493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 Ti\n",
      "Number of SMs: 60\n",
      "Max threads per SM: 1536\n",
      "Max threads per block: 1024\n",
      "Max blocks per SM: 1\n"
     ]
    }
   ],
   "source": [
    "# Check GPU properties\n",
    "props = cp.cuda.runtime.getDeviceProperties(0)\n",
    "print(f\"GPU: {props['name'].decode()}\")\n",
    "print(f\"Number of SMs: {props['multiProcessorCount']}\")\n",
    "print(f\"Max threads per SM: {props['maxThreadsPerMultiProcessor']}\")\n",
    "print(f\"Max threads per block: {props['maxThreadsPerBlock']}\")\n",
    "print(f\"Max blocks per SM: {props['maxThreadsPerMultiProcessor'] // props['maxThreadsPerBlock']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b7f0df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 12080\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA version: {cp.cuda.runtime.runtimeGetVersion()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae781f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac71fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with smaller N to see if occupancy improves\n",
    "N_small = 50\n",
    "iters_small = 40\n",
    "xyt_small = make_input(N_small)\n",
    "\n",
    "packs_test = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "print(\"\\n=== Testing with N=50 (50 threads/block) ===\")\n",
    "print(\"Blocks\\tCalls/sec\\tElapsed(s)\")\n",
    "for b in packs_test:\n",
    "    cps, t = run_trial(b, iters_small, xyt_small, xyt_small)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    print(f\"{b}\\t{int(cps)}\\t\\t{t:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215cb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Compare multi_ensemble vs launching separate kernels\n",
    "# To see if the issue is with the multi-ensemble kernel itself\n",
    "\n",
    "def run_separate_kernels(num_blocks, iters_per_call, xyt1_np, xyt2_np):\n",
    "    \"\"\"Launch num_blocks separate single-block kernels instead of one multi-block kernel\"\"\"\n",
    "    pack_cuda._ensure_initialized()\n",
    "    \n",
    "    xyt1_gpu = cp.asarray(xyt1_np, dtype=cp.float32 if pack_cuda.USE_FLOAT32 else cp.float64)\n",
    "    xyt2_gpu = cp.asarray(xyt2_np, dtype=cp.float32 if pack_cuda.USE_FLOAT32 else cp.float64)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(num_blocks):\n",
    "        totals, grads = pack_cuda.overlap_list_total(xyt1_gpu, xyt2_gpu, compute_grad=True)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    \n",
    "    # Timed run\n",
    "    cp.cuda.Device().synchronize()\n",
    "    start = time.perf_counter()\n",
    "    for k in range(iters_per_call):\n",
    "        for _ in range(num_blocks):\n",
    "            totals, grads = pack_cuda.overlap_list_total(xyt1_gpu, xyt2_gpu, compute_grad=True)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    elapsed = end - start\n",
    "    return iters_per_call / elapsed, elapsed\n",
    "\n",
    "print(\"\\n=== Comparing multi-ensemble vs separate kernel launches (N=20) ===\")\n",
    "print(\"Blocks\\tMulti\\tSeparate\")\n",
    "for b in [1, 2, 4, 8, 16, 32, 64]:\n",
    "    cps_multi, _ = run_trial(b, iters, xyt, xyt)\n",
    "    cps_sep, _ = run_separate_kernels(b, iters, xyt, xyt)\n",
    "    print(f\"{b}\\t{int(cps_multi)}\\t{int(cps_sep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef0cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check actual kernel occupancy\n",
    "# Try to determine how many blocks can run per SM\n",
    "\n",
    "# Get the kernel function\n",
    "pack_cuda._ensure_initialized()\n",
    "kernel = pack_cuda._multi_ensemble_kernel\n",
    "\n",
    "# Check kernel attributes\n",
    "print(\"Kernel attributes:\")\n",
    "print(f\"  Max threads per block: {kernel.max_threads_per_block}\")\n",
    "print(f\"  Num regs: {kernel.num_regs}\")\n",
    "print(f\"  Shared size bytes: {kernel.shared_size_bytes}\")\n",
    "print(f\"  Const size bytes: {kernel.const_size_bytes}\")\n",
    "print(f\"  Local size bytes: {kernel.local_size_bytes}\")\n",
    "\n",
    "# Calculate theoretical occupancy\n",
    "threads_per_block = 20  # Our N\n",
    "max_threads_per_sm = props['maxThreadsPerMultiProcessor']\n",
    "max_blocks_per_sm_thread = max_threads_per_sm // threads_per_block\n",
    "\n",
    "print(f\"\\nWith {threads_per_block} threads/block:\")\n",
    "print(f\"  Max blocks/SM (thread limit): {max_blocks_per_sm_thread}\")\n",
    "print(f\"  Max active threads/SM: {min(max_threads_per_sm, max_blocks_per_sm_thread * threads_per_block)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
