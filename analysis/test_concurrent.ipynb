{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4106fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streams\tKernels/sec\tElapsed(s)\n",
      "1\t45\t\t0.218\n",
      "2\t99\t\t0.200\n",
      "4\t200\t\t0.200\n",
      "8\t400\t\t0.200\n",
      "16\t401\t\t0.399\n",
      "32\t401\t\t0.797\n",
      "64\t399\t\t1.600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tools/kernel_concurrency_test.py\n",
    "# Usage: python3 tools/kernel_concurrency_test.py\n",
    "# Make sure you run this in the Python environment that has CuPy and your repo available.\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import sys\n",
    "sys.path.append('/mnt/d/packing/code/core/')\n",
    "import pack_cuda\n",
    "\n",
    "def make_input(N):\n",
    "    # small random poses in sensible range\n",
    "    rng = np.random.RandomState(123)\n",
    "    x = rng.uniform(-5.0, 5.0, size=N)\n",
    "    y = rng.uniform(-5.0, 5.0, size=N)\n",
    "    t = rng.uniform(-math.pi, math.pi, size=N)\n",
    "    xyt = np.stack([x, y, t], axis=1).astype(np.float64)\n",
    "    return xyt\n",
    "\n",
    "def run_trial(num_streams, iters_per_stream, xyt1_np, xyt2_np):\n",
    "    pack_cuda._ensure_initialized()  # make sure module is initialized\n",
    "    n1 = xyt1_np.shape[0]\n",
    "    n2 = xyt2_np.shape[0]\n",
    "    # Flatten 3xN row-major as kernel expects\n",
    "    xyt1_3xN = cp.ascontiguousarray(cp.asarray(xyt1_np).T).ravel()\n",
    "    xyt2_3xN = cp.ascontiguousarray(cp.asarray(xyt2_np).T).ravel()\n",
    "    # dynamic shared memory size (no shared memory used for large n2)\n",
    "    shared_mem = 0\n",
    "\n",
    "    # Grab raw kernel and device arrays from pack_cuda (uses private names)\n",
    "    kernel = pack_cuda._overlap_list_total_kernel\n",
    "    piece_xy = pack_cuda._piece_xy_d\n",
    "    piece_nverts = pack_cuda._piece_nverts_d\n",
    "    num_pieces = np.int32(pack_cuda._num_pieces)\n",
    "\n",
    "    # Precreate per-stream outputs (with gradient arrays)\n",
    "    streams = [cp.cuda.Stream(non_blocking=True) for _ in range(num_streams)]\n",
    "    out_totals = [cp.zeros(1, dtype=cp.float64) for _ in range(num_streams)]\n",
    "    out_grads = [cp.zeros(n1 * 3, dtype=cp.float64) for _ in range(num_streams)]\n",
    "\n",
    "    # Warmup single call per stream to get JIT/compilation out of the way\n",
    "    for s_idx, stream in enumerate(streams):\n",
    "        with stream:\n",
    "            kernel(\n",
    "                (1,), (n1,),\n",
    "                (xyt1_3xN, np.int32(n1), xyt2_3xN, np.int32(n2), out_totals[s_idx], out_grads[s_idx]),\n",
    "                stream=stream,\n",
    "                shared_mem=shared_mem\n",
    "            )\n",
    "    # Ensure warmup finished\n",
    "    for s in streams:\n",
    "        s.synchronize()\n",
    "\n",
    "    # Timed run: launch iters_per_stream kernels on each stream (back-to-back)\n",
    "    start = time.time()\n",
    "    for k in range(iters_per_stream):\n",
    "        for s_idx, stream in enumerate(streams):\n",
    "            with stream:\n",
    "                kernel(\n",
    "                    (1,), (n1,),\n",
    "                    (xyt1_3xN, np.int32(n1), xyt2_3xN, np.int32(n2), out_totals[s_idx], out_grads[s_idx]),\n",
    "                    stream=stream,\n",
    "                    shared_mem=shared_mem\n",
    "                )\n",
    "    # Wait for all streams to finish\n",
    "    for s in streams:\n",
    "        s.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    elapsed = end - start\n",
    "    total_kernels = num_streams * iters_per_stream\n",
    "    kernels_per_sec = total_kernels / elapsed\n",
    "    return kernels_per_sec, elapsed\n",
    "\n",
    "\n",
    "# Parameters to tune\n",
    "N = 200                       # number of trees (threads per block)\n",
    "iters = 10                    # iterations per stream\n",
    "xyt = make_input(N)\n",
    "\n",
    "pack_cuda._ensure_initialized()\n",
    "packs = [1,2, 4, 8, 16, 32, 64]  # number of concurrent streams to test\n",
    "print(\"Streams\\tKernels/sec\\tElapsed(s)\")\n",
    "for s in packs:\n",
    "    kps, t = run_trial(s, iters, xyt[:200], xyt[:200])\n",
    "    print(f\"{s}\\t{int(kps)}\\t\\t{t:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485d30f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
