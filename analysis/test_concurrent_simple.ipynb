{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4106fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n",
      "============================================================\n",
      "KERNEL TYPE: OVERLAP\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TEST 1: Block Count Scaling (Single Stream, Multiple Blocks)\n",
      "Config: 20 threads/block, 1000000 work iterations\n",
      "============================================================\n",
      "Blocks\tKernels/sec\tElapsed(s)\n",
      "============================================================\n",
      "KERNEL TYPE: OVERLAP\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TEST 1: Block Count Scaling (Single Stream, Multiple Blocks)\n",
      "Config: 20 threads/block, 1000000 work iterations\n",
      "============================================================\n",
      "Blocks\tKernels/sec\tElapsed(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n",
      "============================================================\n",
      "KERNEL TYPE: OVERLAP\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TEST 1: Block Count Scaling (Single Stream, Multiple Blocks)\n",
      "Config: 20 threads/block, 1000000 work iterations\n",
      "============================================================\n",
      "Blocks\tKernels/sec\tElapsed(s)\n",
      "============================================================\n",
      "KERNEL TYPE: OVERLAP\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TEST 1: Block Count Scaling (Single Stream, Multiple Blocks)\n",
      "Config: 20 threads/block, 1000000 work iterations\n",
      "============================================================\n",
      "Blocks\tKernels/sec\tElapsed(s)\n"
     ]
    },
    {
     "ename": "CUDARuntimeError",
     "evalue": "cudaErrorIllegalAddress: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCUDARuntimeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8264/2551565647.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0mblock_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblock_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mkps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_trial_single_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKERNEL_TYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m     \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{b}\\t{int(kps)}\\t\\t{t:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8264/2551565647.py\u001b[0m in \u001b[0;36mrun_trial_single_stream\u001b[0;34m(num_blocks, iters_per_call, n_threads, work_factor, kernel_type)\u001b[0m\n\u001b[1;32m    125\u001b[0m              cp.array([0], dtype=cp.float32), np.int32(num_ensembles))\n\u001b[1;32m    126\u001b[0m         )\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Timed run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/device.pyx\u001b[0m in \u001b[0;36mcupy.cuda.device.Device.synchronize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/device.pyx\u001b[0m in \u001b[0;36mcupy.cuda.device.Device.synchronize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy_backends/cuda/api/runtime.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.api.runtime.deviceSynchronize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy_backends/cuda/api/runtime.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.api.runtime.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCUDARuntimeError\u001b[0m: cudaErrorIllegalAddress: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "# Simple kernel concurrency test - block size scaling and multi-stream parallelism\n",
    "# Supports both simple kernel and overlap kernel from pack_cuda\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '../core'))\n",
    "\n",
    "import pack_cuda\n",
    "import kaggle_support as kgs\n",
    "\n",
    "# ============================================================================\n",
    "# KERNEL SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Define simple work kernel with block indexing for multi-block execution\n",
    "simple_kernel_code = r'''\n",
    "extern \"C\" __global__\n",
    "void simple_work(const float* input, float* output, int n, int work_factor) {\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (idx < n) {\n",
    "        float val = input[idx];\n",
    "        // Do some arithmetic work to make it non-trivial\n",
    "        for (int i = 0; i < work_factor; i++) {\n",
    "            val = val * 1.001f + 0.001f;\n",
    "            val = sqrtf(val * val + 1.0f);\n",
    "        }\n",
    "        output[idx] = val;\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "# Compile the simple kernel\n",
    "simple_kernel = cp.RawKernel(simple_kernel_code, 'simple_work')\n",
    "\n",
    "# Initialize pack_cuda and get the overlap kernel\n",
    "pack_cuda.USE_FLOAT32=True\n",
    "pack_cuda._ensure_initialized()\n",
    "overlap_kernel = pack_cuda._multi_overlap_list_total_kernel\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS FOR DATA PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_simple_data(num_blocks, n_threads):\n",
    "    \"\"\"Prepare data for simple kernel: single contiguous array\"\"\"\n",
    "    total_size = num_blocks * n_threads\n",
    "    input_data = cp.random.randn(total_size, dtype=cp.float32)\n",
    "    output_data = cp.zeros(total_size, dtype=cp.float32)\n",
    "    return input_data, output_data, total_size\n",
    "\n",
    "def prepare_overlap_data(num_ensembles, n_trees):\n",
    "    \"\"\"Prepare data for overlap kernel: arrays of pointers and metadata\"\"\"\n",
    "    xyt1_arrays = []\n",
    "    xyt2_arrays = []\n",
    "    \n",
    "    # Use float32 if USE_FLOAT32 is set, otherwise float64\n",
    "    dtype = cp.float32 if pack_cuda.USE_FLOAT32 else cp.float64\n",
    "    \n",
    "    for _ in range(num_ensembles):\n",
    "        xyt1 = cp.random.randn(n_trees, 3, dtype=dtype)\n",
    "        xyt2 = cp.random.randn(n_trees, 3, dtype=dtype)\n",
    "        \n",
    "        xyt1_3xN = cp.ascontiguousarray(xyt1.T).ravel()\n",
    "        xyt2_3xN = cp.ascontiguousarray(xyt2.T).ravel()\n",
    "        xyt1_arrays.append(xyt1_3xN)\n",
    "        xyt2_arrays.append(xyt2_3xN)\n",
    "    \n",
    "    xyt1_ptrs = cp.array([arr.data.ptr for arr in xyt1_arrays], dtype=cp.float32)\n",
    "    xyt2_ptrs = cp.array([arr.data.ptr for arr in xyt2_arrays], dtype=cp.float32)\n",
    "    n_array = cp.array([n_trees] * num_ensembles, dtype=cp.int32)\n",
    "    out_totals = cp.zeros(num_ensembles, dtype=dtype)\n",
    "    \n",
    "    return xyt1_arrays, xyt2_arrays, xyt1_ptrs, xyt2_ptrs, n_array, out_totals\n",
    "\n",
    "def prepare_simple_data_multi_stream(num_streams, num_blocks_per_stream, n_threads):\n",
    "    \"\"\"Prepare data for simple kernel with multiple streams\"\"\"\n",
    "    total_size_per_stream = num_blocks_per_stream * n_threads\n",
    "    inputs = [cp.random.randn(total_size_per_stream, dtype=cp.float32) for _ in range(num_streams)]\n",
    "    outputs = [cp.zeros(total_size_per_stream, dtype=cp.float32) for _ in range(num_streams)]\n",
    "    return inputs, outputs, total_size_per_stream\n",
    "\n",
    "# ============================================================================\n",
    "# TEST FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def run_trial_single_stream(num_blocks, iters_per_call, n_threads, work_factor, kernel_type):\n",
    "    \"\"\"Test: vary number of blocks (grid size) with same thread count per block\"\"\"\n",
    "    \n",
    "    if kernel_type == 'simple':\n",
    "        input_data, output_data, total_size = prepare_simple_data(num_blocks, n_threads)\n",
    "        \n",
    "        # Warmup\n",
    "        simple_kernel(\n",
    "            (num_blocks,), (n_threads,),\n",
    "            (input_data, output_data, total_size, work_factor)\n",
    "        )\n",
    "        cp.cuda.Device().synchronize()\n",
    "        \n",
    "        # Timed run\n",
    "        cp.cuda.Device().synchronize()\n",
    "        start = time.perf_counter()\n",
    "        for k in range(iters_per_call):\n",
    "            simple_kernel(\n",
    "                (num_blocks,), (n_threads,),\n",
    "                (input_data, output_data, total_size, work_factor)\n",
    "            )\n",
    "        cp.cuda.Device().synchronize()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "    elif kernel_type == 'overlap':\n",
    "        num_ensembles = num_blocks\n",
    "        # For overlap kernel, use n_threads as number of trees per ensemble\n",
    "        # Each tree requires 4 threads (one per polygon piece)\n",
    "        n_trees = n_threads//4\n",
    "        xyt1_arrays, xyt2_arrays, xyt1_ptrs, xyt2_ptrs, n_array, out_totals = prepare_overlap_data(num_ensembles, n_trees)\n",
    "        \n",
    "        # Warmup\n",
    "        overlap_kernel(\n",
    "            (num_ensembles,), (n_threads,),\n",
    "            (xyt1_ptrs, n_array, xyt2_ptrs, n_array, out_totals,\n",
    "             cp.array([0], dtype=cp.float32), np.int32(num_ensembles))\n",
    "        )\n",
    "        cp.cuda.Device().synchronize()\n",
    "        \n",
    "        # Timed run\n",
    "        cp.cuda.Device().synchronize()\n",
    "        start = time.perf_counter()\n",
    "        for k in range(iters_per_call):\n",
    "            overlap_kernel(\n",
    "                (num_ensembles,), (n_threads,),\n",
    "                (xyt1_ptrs, n_array, xyt2_ptrs, n_array, out_totals,\n",
    "                 cp.array([0], dtype=cp.float32), np.int32(num_ensembles))\n",
    "            )\n",
    "        cp.cuda.Device().synchronize()\n",
    "        end = time.perf_counter()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel_type: {kernel_type}\")\n",
    "    \n",
    "    elapsed = end - start\n",
    "    total_kernel_calls = iters_per_call\n",
    "    calls_per_sec = total_kernel_calls / elapsed\n",
    "    return calls_per_sec, elapsed\n",
    "\n",
    "def run_trial_multi_stream(num_streams, iters_per_stream, num_blocks_per_stream, n_threads, work_factor, kernel_type):\n",
    "    \"\"\"Test: launch multiple kernels in parallel via streams (each kernel has multiple blocks)\"\"\"\n",
    "    \n",
    "    if kernel_type == 'simple':\n",
    "        inputs, outputs, total_size_per_stream = prepare_simple_data_multi_stream(num_streams, num_blocks_per_stream, n_threads)\n",
    "        streams = [cp.cuda.Stream(non_blocking=True) for _ in range(num_streams)]\n",
    "        \n",
    "        # Warmup\n",
    "        for s_idx, stream in enumerate(streams):\n",
    "            simple_kernel(\n",
    "                (num_blocks_per_stream,), (n_threads,),\n",
    "                (inputs[s_idx], outputs[s_idx], total_size_per_stream, work_factor),\n",
    "                stream=stream\n",
    "            )\n",
    "        cp.cuda.Device().synchronize()\n",
    "        \n",
    "        # Timed run\n",
    "        cp.cuda.Device().synchronize()\n",
    "        start = time.perf_counter()\n",
    "        for i in range(iters_per_stream):\n",
    "            for s_idx, stream in enumerate(streams):\n",
    "                simple_kernel(\n",
    "                    (num_blocks_per_stream,), (n_threads,),\n",
    "                    (inputs[s_idx], outputs[s_idx], total_size_per_stream, work_factor),\n",
    "                    stream=stream\n",
    "                )\n",
    "        cp.cuda.Device().synchronize()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "    elif kernel_type == 'overlap':\n",
    "        # For overlap kernel, use n_threads as number of trees per ensemble\n",
    "        # Each tree requires 4 threads (one per polygon piece)\n",
    "        num_trees = n_threads\n",
    "        dtype = cp.float32 if pack_cuda.USE_FLOAT32 else cp.float64\n",
    "        \n",
    "        # Total ensembles = num_streams * num_blocks_per_stream\n",
    "        total_ensembles = num_streams * num_blocks_per_stream\n",
    "        \n",
    "        # Create data for all ensembles\n",
    "        xyt1_arrays = []\n",
    "        xyt2_arrays = []\n",
    "        for _ in range(total_ensembles):\n",
    "            xyt1 = cp.random.randn(num_trees, 3, dtype=dtype)\n",
    "            xyt2 = cp.random.randn(num_trees, 3, dtype=dtype)\n",
    "            \n",
    "            xyt1_3xN = cp.ascontiguousarray(xyt1.T).ravel()\n",
    "            xyt2_3xN = cp.ascontiguousarray(xyt2.T).ravel()\n",
    "            xyt1_arrays.append(xyt1_3xN)\n",
    "            xyt2_arrays.append(xyt2_3xN)\n",
    "        \n",
    "        xyt1_ptrs = cp.array([arr.data.ptr for arr in xyt1_arrays], dtype=cp.uint64)\n",
    "        xyt2_ptrs = cp.array([arr.data.ptr for arr in xyt2_arrays], dtype=cp.uint64)\n",
    "        n_array = cp.array([num_trees] * total_ensembles, dtype=cp.int32)\n",
    "        out_totals = cp.zeros(total_ensembles, dtype=dtype)\n",
    "        \n",
    "        # Warmup\n",
    "        overlap_kernel(\n",
    "            (num_blocks_per_stream,), (num_trees * 4,),\n",
    "            (xyt1_ptrs, n_array, xyt2_ptrs, n_array, out_totals,\n",
    "             cp.array([0], dtype=cp.uint64), np.int32(num_blocks_per_stream))\n",
    "        )\n",
    "        cp.cuda.Device().synchronize()\n",
    "        \n",
    "        # Timed run\n",
    "        cp.cuda.Device().synchronize()\n",
    "        start = time.perf_counter()\n",
    "        for i in range(iters_per_stream):\n",
    "            overlap_kernel(\n",
    "                (num_blocks_per_stream,), (num_trees * 4,),\n",
    "                (xyt1_ptrs, n_array, xyt2_ptrs, n_array, out_totals,\n",
    "                 cp.array([0], dtype=cp.uint64), np.int32(num_blocks_per_stream))\n",
    "            )\n",
    "        cp.cuda.Device().synchronize()\n",
    "        end = time.perf_counter()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel_type: {kernel_type}\")\n",
    "    \n",
    "    elapsed = end - start\n",
    "    total_kernel_calls = iters_per_stream * num_streams\n",
    "    calls_per_sec = total_kernel_calls / elapsed\n",
    "    return calls_per_sec, elapsed\n",
    "\n",
    "# ============================================================================\n",
    "# TEST EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "# Test parameters\n",
    "# For simple kernel: n_threads is threads per block\n",
    "# For overlap kernel: n_threads is trees per ensemble (kernel uses n_threads * 4 actual threads)\n",
    "n_threads = 20  # Thread count per block (simple) or trees per ensemble (overlap)\n",
    "work_factor = 1000000  # Work iterations per thread\n",
    "iters = 1\n",
    "\n",
    "# KERNEL TYPE FLAG: 'simple' or 'overlap'\n",
    "KERNEL_TYPE = 'overlap'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"KERNEL TYPE: {KERNEL_TYPE.upper()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST 1: Block Count Scaling (Single Stream, Multiple Blocks)\")\n",
    "print(f\"Config: {n_threads} threads/block, {work_factor} work iterations\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Blocks\\tKernels/sec\\tElapsed(s)\")\n",
    "block_counts = [1, 2, 4]\n",
    "for b in block_counts:\n",
    "    kps, t = run_trial_single_stream(b, iters, n_threads, work_factor, kernel_type=KERNEL_TYPE)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    print(f\"{b}\\t{int(kps)}\\t\\t{t:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST 2: Parallel Kernel Execution (Multiple Streams, Each with Multiple Blocks)\")\n",
    "print(f\"Config: {n_threads} threads/block, {work_factor} work iterations\")\n",
    "if KERNEL_TYPE == 'overlap':\n",
    "    print(\"Note: num_streams must be a multiple of 4\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Streams\\tBlocks/Stream\\tKernels/sec\\tElapsed(s)\")\n",
    "num_blocks_per_stream = 64  # Fixed block count per kernel\n",
    "stream_counts = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "for num_streams in stream_counts:\n",
    "    kps, t = run_trial_multi_stream(num_streams, iters, num_blocks_per_stream, n_threads, work_factor, kernel_type=KERNEL_TYPE)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    print(f\"{num_streams}\\t{num_blocks_per_stream}\\t\\t{int(kps)}\\t\\t{t:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GPU Info\")\n",
    "print(\"=\" * 60)\n",
    "props = cp.cuda.runtime.getDeviceProperties(0)\n",
    "print(f\"GPU: {props['name'].decode()}\")\n",
    "print(f\"Number of SMs: {props['multiProcessorCount']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3cdf69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
