{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1dbf6f-2cea-41fa-98f7-59e96481613d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n",
      "WARNING: CUDA MPS not active\n",
      "Will use 2 parallel processes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../core')\n",
    "sys.path.append('/packing/code/core/')\n",
    "\n",
    "import pack_runner\n",
    "import pack_ga3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import itertools\n",
    "import glob\n",
    "import dill\n",
    "import importlib\n",
    "from multiprocess import Process, Queue, cpu_count\n",
    "import kaggle_support as kgs\n",
    "importlib.reload(pack_runner)\n",
    "\n",
    "# Configuration\n",
    "output_dir = '../../results/many_ga/'\n",
    "os.makedirs(output_dir + 'full/', exist_ok=True)\n",
    "os.makedirs(output_dir + 'abbr/', exist_ok=True)\n",
    "\n",
    "fast_mode = False  # Set to True for quick testing\n",
    "which_runner = pack_runner.baseline_runner  # Which example runner to use\n",
    "n_parallel_processes = 2 if kgs.env=='local' else 4\n",
    "print(f\"Will use {n_parallel_processes} parallel processes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20e071f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "# rr = pack_runner.baseline_runner(fast_mode = fast_mode)\n",
    "# rr.base_ga.ga.ga_base.N_trees_to_do = 40\n",
    "# rr.seed=2002\n",
    "# rr.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "git_commit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git commit: 65f3b229\n"
     ]
    }
   ],
   "source": [
    "# Get git commit ID for tracking\n",
    "try:\n",
    "    import git\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    git_commit_id = repo.head.object.hexsha\n",
    "    print(f\"Git commit: {git_commit_id[:8]}\")\n",
    "except:\n",
    "    git_commit_id = 'no_git'\n",
    "    print(\"Git not available, using 'no_git' as commit ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "main_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pyinstrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 1 GPU(s)\n",
      "Starting parallel execution of 1000 seeds using 2 processes\n",
      "Staggering process startup by 2 seconds each...\n",
      "\n",
      "Starting process for seed 4000...\n",
      "local\n",
      "WARNING: CUDA MPS not active\n",
      "Starting process for seed 4001...\n",
      "\n",
      "All initial processes started. Monitoring for completion...\n",
      "\n",
      "local\n",
      "WARNING: CUDA MPS not active\n",
      "Seed 4000 using GPU 0\n",
      "\n",
      "=== Starting seed 4000 (Process 36255) ===\n",
      "{'seed': 4000, 'make_single': True, 'use_minkowski': True}\n",
      "init CUDA\n",
      "Detected GPU compute capability: 8.9 (arch=sm_89)\n",
      "GPU max threads per block: 1024\n",
      "=== Compiling kernel variant: crystal ===\n",
      "Defines: ENABLE_CRYSTAL_AXES, ENABLE_OVERLAP_AREA, ENABLE_SEPARATION\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -DENABLE_CRYSTAL_AXES -DENABLE_OVERLAP_AREA -DENABLE_SEPARATION -cubin /mnt/d//packing/temp/pack_cuda_saved.cu -o /mnt/d//packing/temp/pack_cuda_crystal.cubin\n",
      "ptxas info    : 0 bytes gmem, 1172 bytes cmem[3]\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers, 404 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compile time = 8.064 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    240 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 43 registers, used 1 barriers, 240 bytes cumulative stack size, 404 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 390.847 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    1264 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 80 registers, used 1 barriers, 1264 bytes cumulative stack size, 420 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 331.686 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [crystal] ---\n",
      "  Max threads per block (kernel): 768\n",
      "  Num registers: 80\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 1264\n",
      "=== Compiling kernel variant: no_sep ===\n",
      "Defines: ENABLE_OVERLAP_AREA\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -DENABLE_OVERLAP_AREA -cubin /mnt/d//packing/temp/pack_cuda_saved.cu -o /mnt/d//packing/temp/pack_cuda_no_sep.cubin\n",
      "ptxas info    : 0 bytes gmem, 1172 bytes cmem[3]\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers, 404 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compile time = 7.827 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    240 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 43 registers, used 1 barriers, 240 bytes cumulative stack size, 404 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 403.488 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    1232 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 64 registers, used 1 barriers, 1232 bytes cumulative stack size, 420 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 68.770 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [no_sep] ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 64\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 1232\n",
      "=== Compiling kernel variant: sep ===\n",
      "Defines: ENABLE_SEPARATION\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -DENABLE_SEPARATION -cubin /mnt/d//packing/temp/pack_cuda_saved.cu -o /mnt/d//packing/temp/pack_cuda_sep.cubin\n",
      "ptxas info    : 0 bytes gmem, 1172 bytes cmem[3]\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers, 404 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compile time = 8.302 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    240 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 43 registers, used 1 barriers, 240 bytes cumulative stack size, 404 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 405.618 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    144 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, used 1 barriers, 144 bytes cumulative stack size, 420 bytes cmem[0], 40 bytes cmem[2]\n",
      "ptxas info    : Compile time = 70.835 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [sep] ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 56\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 144\n",
      "\n",
      "--- Kernel: multi_boundary_list_total ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 43\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 240\n",
      "\n",
      "--- Kernel: multi_boundary_distance_list_total ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 36\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/packing/code/analysis/../core/pack_dynamics.py:60: VisibleDeprecationWarning: This function is deprecated and will be removed in a future release. Use the cupy.from_dlpack() array constructor instead.\n",
      "  x0 = from_dlpack(x0.toDlpack())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building lookup table for CollisionCostExactSeparation using Minkowski difference...\n",
      "Building LUT: 451 x 900 x 900 = 365,310,000 grid points\n",
      "  Processing theta 1/900\n",
      "Seed 4001 using GPU 0\n",
      "\n",
      "=== Starting seed 4001 (Process 36304) ===\n",
      "{'seed': 4001, 'make_single': True, 'use_minkowski': True}\n",
      "init CUDA\n",
      "Detected GPU compute capability: 8.9 (arch=sm_89)\n",
      "GPU max threads per block: 1024\n",
      "=== Compiling kernel variant: crystal ===\n",
      "Defines: ENABLE_CRYSTAL_AXES, ENABLE_OVERLAP_AREA, ENABLE_SEPARATION\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -DENABLE_CRYSTAL_AXES -DENABLE_OVERLAP_AREA -DENABLE_SEPARATION -cubin /mnt/d//packing/temp/pack_cuda_saved.cu -o /mnt/d//packing/temp/pack_cuda_crystal.cubin\n",
      "ptxas info    : 0 bytes gmem, 1172 bytes cmem[3]\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers, 404 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compile time = 8.330 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    240 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 43 registers, used 1 barriers, 240 bytes cumulative stack size, 404 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 414.781 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    1264 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 80 registers, used 1 barriers, 1264 bytes cumulative stack size, 420 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 348.718 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [crystal] ---\n",
      "  Max threads per block (kernel): 768\n",
      "  Num registers: 80\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 1264\n",
      "=== Compiling kernel variant: no_sep ===\n",
      "Defines: ENABLE_OVERLAP_AREA\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -DENABLE_OVERLAP_AREA -cubin /mnt/d//packing/temp/pack_cuda_saved.cu -o /mnt/d//packing/temp/pack_cuda_no_sep.cubin\n",
      "ptxas info    : 0 bytes gmem, 1172 bytes cmem[3]\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers, 404 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compile time = 8.392 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    240 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 43 registers, used 1 barriers, 240 bytes cumulative stack size, 404 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 427.862 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    1232 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 64 registers, used 1 barriers, 1232 bytes cumulative stack size, 420 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 71.827 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [no_sep] ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 64\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 1232\n",
      "=== Compiling kernel variant: sep ===\n",
      "Defines: ENABLE_SEPARATION\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -DENABLE_SEPARATION -cubin /mnt/d//packing/temp/pack_cuda_saved.cu -o /mnt/d//packing/temp/pack_cuda_sep.cubin\n",
      "ptxas info    : 0 bytes gmem, 1172 bytes cmem[3]\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers, 404 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compile time = 8.277 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    240 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 43 registers, used 1 barriers, 240 bytes cumulative stack size, 404 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 411.919 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    144 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, used 1 barriers, 144 bytes cumulative stack size, 420 bytes cmem[0], 40 bytes cmem[2]\n",
      "ptxas info    : Compile time = 69.766 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [sep] ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 56\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 144\n",
      "\n",
      "--- Kernel: multi_boundary_list_total ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 43\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 240\n",
      "\n",
      "--- Kernel: multi_boundary_distance_list_total ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 36\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/packing/code/analysis/../core/pack_dynamics.py:60: VisibleDeprecationWarning: This function is deprecated and will be removed in a future release. Use the cupy.from_dlpack() array constructor instead.\n",
      "  x0 = from_dlpack(x0.toDlpack())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building lookup table for CollisionCostExactSeparation using Minkowski difference...\n",
      "Building LUT: 451 x 900 x 900 = 365,310,000 grid points\n",
      "  Processing theta 1/900\n",
      "  Processing theta 51/900\n",
      "  Processing theta 51/900\n",
      "  Processing theta 101/900\n",
      "  Processing theta 101/900\n",
      "  Processing theta 151/900\n",
      "  Processing theta 151/900\n",
      "  Processing theta 201/900\n",
      "  Processing theta 201/900\n",
      "  Processing theta 251/900\n",
      "  Processing theta 251/900\n",
      "  Processing theta 301/900\n",
      "  Processing theta 301/900\n",
      "  Processing theta 351/900\n",
      "  Processing theta 351/900\n",
      "  Processing theta 401/900\n",
      "  Processing theta 401/900\n",
      "  Processing theta 451/900\n",
      "  Processing theta 451/900\n",
      "  Processing theta 501/900\n",
      "  Processing theta 501/900\n",
      "  Processing theta 551/900\n",
      "  Processing theta 551/900\n",
      "  Processing theta 601/900\n",
      "  Processing theta 601/900\n",
      "  Processing theta 651/900\n",
      "  Processing theta 651/900\n",
      "  Processing theta 701/900\n",
      "  Processing theta 701/900\n",
      "  Processing theta 751/900\n",
      "  Processing theta 751/900\n",
      "  Processing theta 801/900\n",
      "  Processing theta 801/900\n",
      "  Processing theta 851/900\n",
      "  Processing theta 851/900\n",
      "Cost range: [-1.093621, 0.630274]\n",
      "Cost range: [-1.093621, 0.630274]\n",
      "Trimming zero edges:\n",
      "  X: 451 -> 451 (removed 0)\n",
      "  Y: 900 -> 900 (removed 0)\n",
      "  Theta: 900 -> 900 (removed 0)\n",
      "  Total reduction: 0.0% (0 points)\n",
      "Trimming zero edges:\n",
      "  X: 451 -> 451 (removed 0)\n",
      "  Y: 900 -> 900 (removed 0)\n",
      "  Theta: 900 -> 900 (removed 0)\n",
      "  Total reduction: 0.0% (0 points)\n",
      "Compiling CUDA LUT kernel one-time only)\n",
      "Detected GPU compute capability: 89 (arch=sm_89)\n",
      "Compiling: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -lineinfo -arch=sm_89 -cubin /mnt/d//packing/temp/pack_cuda_lut_saved.cu -o /mnt/d//packing/temp/pack_cuda_lut.cubin\n",
      "/mnt/d//packing/temp/pack_cuda_lut_saved.cu:5: warning: \"M_PI\" redefined\n",
      "    5 | #define M_PI 3.14159265358979323846f\n",
      "      | \n",
      "In file included from /usr/include/c++/13/cmath:47,\n",
      "                 from /usr/include/c++/13/math.h:36,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/crt/math_functions.h:4577,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/crt/common_functions.h:303,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:117,\n",
      "                 from <command-line>:\n",
      "/usr/include/math.h:1152: note: this is the location of the previous definition\n",
      " 1152 | # define M_PI           3.14159265358979323846  /* pi */\n",
      "      | \n",
      "ptxas info    : 16 bytes gmem, 48 bytes cmem[3], 16 bytes cmem[4]\n",
      "ptxas info    : Compiling entry function 'multi_overlap_lut_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_lut_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, used 1 barriers, 396 bytes cmem[0]\n",
      "ptxas info    : Compile time = 22.060 ms\n",
      "\n",
      "Kernel multi_overlap_lut_total:\n",
      "  Registers: 40\n",
      "  Shared mem: 0 bytes\n",
      "  Max threads/block: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/packing/code/analysis/../core/pack_dynamics.py:92: VisibleDeprecationWarning: This function is deprecated and will be removed in a future release. Use the cupy.from_dlpack() array constructor instead.\n",
      "  return from_dlpack(tmp_cost[:N].toDlpack()), from_dlpack(res.toDlpack())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling CUDA LUT kernel one-time only)\n",
      "Detected GPU compute capability: 89 (arch=sm_89)\n",
      "Compiling: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -lineinfo -arch=sm_89 -cubin /mnt/d//packing/temp/pack_cuda_lut_saved.cu -o /mnt/d//packing/temp/pack_cuda_lut.cubin\n",
      "/mnt/d//packing/temp/pack_cuda_lut_saved.cu:5: warning: \"M_PI\" redefined\n",
      "    5 | #define M_PI 3.14159265358979323846f\n",
      "      | \n",
      "In file included from /usr/include/c++/13/cmath:47,\n",
      "                 from /usr/include/c++/13/math.h:36,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/crt/math_functions.h:4577,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/crt/common_functions.h:303,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:117,\n",
      "                 from <command-line>:\n",
      "/usr/include/math.h:1152: note: this is the location of the previous definition\n",
      " 1152 | # define M_PI           3.14159265358979323846  /* pi */\n",
      "      | \n",
      "ptxas info    : 16 bytes gmem, 48 bytes cmem[3], 16 bytes cmem[4]\n",
      "ptxas info    : Compiling entry function 'multi_overlap_lut_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_lut_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, used 1 barriers, 396 bytes cmem[0]\n",
      "ptxas info    : Compile time = 23.524 ms\n",
      "\n",
      "Kernel multi_overlap_lut_total:\n",
      "  Registers: 40\n",
      "  Shared mem: 0 bytes\n",
      "  Max threads/block: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/packing/code/analysis/../core/pack_dynamics.py:92: VisibleDeprecationWarning: This function is deprecated and will be removed in a future release. Use the cupy.from_dlpack() array constructor instead.\n",
      "  return from_dlpack(tmp_cost[:N].toDlpack()), from_dlpack(res.toDlpack())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LAP CUDA\n",
      "Detected GPU compute capability: 8.9 (arch=sm_89)\n",
      "GPU max threads per block: 1024\n",
      "=== Compiling LAP kernels ===\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -cubin /mnt/d//packing/temp/lap_batch_saved.cu -o /mnt/d//packing/temp/lap_batch.cubin\n",
      "ptxas info    : 0 bytes gmem\n",
      "ptxas info    : Compiling entry function 'diversity_shortcut_kernel' for 'sm_89'\n",
      "ptxas info    : Function properties for diversity_shortcut_kernel\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 29 registers, used 1 barriers, 400 bytes cmem[0]\n",
      "ptxas info    : Compile time = 16.379 ms\n",
      "ptxas info    : Compiling entry function 'compute_costs' for 'sm_89'\n",
      "ptxas info    : Function properties for compute_costs\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 29 registers, used 0 barriers, 384 bytes cmem[0]\n",
      "ptxas info    : Compile time = 11.322 ms\n",
      "ptxas info    : Compiling entry function 'auction_kernel' for 'sm_89'\n",
      "ptxas info    : Function properties for auction_kernel\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 35 registers, used 0 barriers, 412 bytes cmem[0]\n",
      "ptxas info    : Compile time = 24.804 ms\n",
      "ptxas info    : Compiling entry function 'hungarian_kernel' for 'sm_89'\n",
      "ptxas info    : Function properties for hungarian_kernel\n",
      "    1808 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, used 0 barriers, 1808 bytes cumulative stack size, 408 bytes cmem[0]\n",
      "ptxas info    : Compile time = 34.709 ms\n",
      "\n",
      "\n",
      "--- Kernel: hungarian_kernel ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 40\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 1808\n",
      "\n",
      "--- Kernel: auction_kernel ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 35\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 0\n",
      "\n",
      "--- Kernel: compute_costs ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 29\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 0\n",
      "\n",
      "--- Kernel: diversity_shortcut_kernel ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 29\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 0\n",
      "Generation 0: Best costs = [[0.371981, 1.18495]]\n",
      "Generation 1: Best costs = [[0.371981, 1.162363]]\n",
      "init LAP CUDA\n",
      "Detected GPU compute capability: 8.9 (arch=sm_89)\n",
      "GPU max threads per block: 1024\n",
      "=== Compiling LAP kernels ===\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -cubin /mnt/d//packing/temp/lap_batch_saved.cu -o /mnt/d//packing/temp/lap_batch.cubin\n",
      "Generation 2: Best costs = [[0.371981, 1.080628]]\n",
      "ptxas info    : 0 bytes gmem\n",
      "ptxas info    : Compiling entry function 'diversity_shortcut_kernel' for 'sm_89'\n",
      "ptxas info    : Function properties for diversity_shortcut_kernel\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 29 registers, used 1 barriers, 400 bytes cmem[0]\n",
      "ptxas info    : Compile time = 15.596 ms\n",
      "ptxas info    : Compiling entry function 'compute_costs' for 'sm_89'\n",
      "ptxas info    : Function properties for compute_costs\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 29 registers, used 0 barriers, 384 bytes cmem[0]\n",
      "ptxas info    : Compile time = 10.027 ms\n",
      "ptxas info    : Compiling entry function 'auction_kernel' for 'sm_89'\n",
      "ptxas info    : Function properties for auction_kernel\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 35 registers, used 0 barriers, 412 bytes cmem[0]\n",
      "ptxas info    : Compile time = 20.901 ms\n",
      "ptxas info    : Compiling entry function 'hungarian_kernel' for 'sm_89'\n",
      "ptxas info    : Function properties for hungarian_kernel\n",
      "    1808 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, used 0 barriers, 1808 bytes cumulative stack size, 408 bytes cmem[0]\n",
      "ptxas info    : Compile time = 14.381 ms\n",
      "\n",
      "\n",
      "--- Kernel: hungarian_kernel ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 40\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 1808\n",
      "\n",
      "--- Kernel: auction_kernel ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 35\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 0\n",
      "\n",
      "--- Kernel: compute_costs ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 29\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 0\n",
      "\n",
      "--- Kernel: diversity_shortcut_kernel ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 29\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 0\n",
      "Generation 0: Best costs = [[0.371981, 1.521454]]\n",
      "Generation 3: Best costs = [[0.371981, 0.937042]]\n",
      "Generation 4: Best costs = [[0.371981, 0.821161]]\n",
      "Generation 1: Best costs = [[0.371981, 1.214501]]\n",
      "Generation 5: Best costs = [[0.371981, 0.703193]]\n",
      "Generation 2: Best costs = [[0.371981, 1.022838]]\n",
      "Generation 6: Best costs = [[0.371981, 0.668623]]\n",
      "Generation 3: Best costs = [[0.371981, 0.824725]]\n",
      "Generation 7: Best costs = [[0.371981, 0.604545]]\n",
      "Generation 4: Best costs = [[0.371981, 0.723468]]\n",
      "Generation 8: Best costs = [[0.371981, 0.514495]]\n",
      "Generation 5: Best costs = [[0.371981, 0.717693]]\n",
      "Generation 9: Best costs = [[0.371981, 0.451791]]\n",
      "Generation 6: Best costs = [[0.371981, 0.589639]]\n",
      "Generation 10: Best costs = [[0.371981, 0.447517]]\n",
      "Generation 7: Best costs = [[0.371981, 0.584587]]\n",
      "Generation 11: Best costs = [[0.371981, 0.404033]]\n",
      "Generation 8: Best costs = [[0.371981, 0.539406]]\n",
      "Generation 12: Best costs = [[0.371981, 0.373174]]\n",
      "Generation 9: Best costs = [[0.371981, 0.512545]]\n",
      "Generation 13: Best costs = [[0.371981, 0.352163]]\n",
      "Generation 10: Best costs = [[0.371981, 0.478942]]\n",
      "Generation 14: Best costs = [[0.371981, 0.343196]]\n",
      "Generation 11: Best costs = [[0.371981, 0.459332]]\n",
      "Generation 15: Best costs = [[0.371981, 0.282511]]\n",
      "Generation 12: Best costs = [[0.371981, 0.417519]]\n",
      "Generation 16: Best costs = [[0.371981, 0.272778]]\n",
      "Generation 13: Best costs = [[0.371981, 0.417519]]\n",
      "Generation 17: Best costs = [[0.371981, 0.272778]]\n",
      "Generation 14: Best costs = [[0.371981, 0.400246]]\n",
      "Generation 15: Best costs = [[0.371981, 0.385242]]\n",
      "Generation 18: Best costs = [[0.371981, 0.246687]]\n",
      "Generation 16: Best costs = [[0.371981, 0.362644]]\n",
      "Generation 19: Best costs = [[0.371981, 0.242521]]\n",
      "Generation 17: Best costs = [[0.371981, 0.336741]]\n",
      "Generation 20: Best costs = [[0.371981, 0.240772]]\n",
      "Generation 18: Best costs = [[0.371981, 0.32807]]\n",
      "Generation 21: Best costs = [[0.371981, 0.224568]]\n",
      "Generation 19: Best costs = [[0.371981, 0.312613]]\n",
      "Generation 22: Best costs = [[0.371981, 0.224568]]\n",
      "Generation 20: Best costs = [[0.371981, 0.297815]]\n",
      "Generation 23: Best costs = [[0.371981, 0.217269]]\n",
      "Generation 21: Best costs = [[0.371981, 0.286493]]\n",
      "Generation 24: Best costs = [[0.371981, 0.206112]]\n",
      "Generation 22: Best costs = [[0.371981, 0.2841]]\n",
      "Generation 25: Best costs = [[0.371981, 0.195718]]\n",
      "Generation 23: Best costs = [[0.371981, 0.266917]]\n",
      "Generation 26: Best costs = [[0.371981, 0.191419]]\n",
      "Generation 24: Best costs = [[0.371981, 0.256229]]\n",
      "Generation 27: Best costs = [[0.371981, 0.18227]]\n",
      "Generation 25: Best costs = [[0.371981, 0.227423]]\n",
      "Generation 28: Best costs = [[0.371981, 0.169233]]\n",
      "Generation 29: Best costs = [[0.371981, 0.161324]]\n",
      "Generation 26: Best costs = [[0.371981, 0.223569]]\n",
      "Generation 30: Best costs = [[0.371981, 0.158611]]\n",
      "Generation 27: Best costs = [[0.371981, 0.205323]]\n",
      "Generation 31: Best costs = [[0.371981, 0.158611]]\n",
      "Generation 28: Best costs = [[0.371981, 0.194364]]\n",
      "Generation 32: Best costs = [[0.371981, 0.15717]]\n",
      "Generation 29: Best costs = [[0.371981, 0.193005]]\n",
      "Generation 33: Best costs = [[0.371981, 0.152629]]\n",
      "Generation 30: Best costs = [[0.371981, 0.193005]]\n",
      "Generation 34: Best costs = [[0.371981, 0.148824]]\n",
      "Generation 31: Best costs = [[0.371981, 0.18828]]\n",
      "Generation 35: Best costs = [[0.371981, 0.148079]]\n",
      "Generation 32: Best costs = [[0.371981, 0.178458]]\n",
      "Generation 36: Best costs = [[0.371981, 0.147372]]\n",
      "Generation 33: Best costs = [[0.371981, 0.168296]]\n",
      "Generation 37: Best costs = [[0.371981, 0.145789]]\n",
      "Generation 34: Best costs = [[0.371981, 0.168296]]\n",
      "Generation 38: Best costs = [[0.371981, 0.14555]]\n",
      "Generation 35: Best costs = [[0.371981, 0.168296]]\n",
      "Generation 39: Best costs = [[0.371981, 0.132955]]\n",
      "Generation 36: Best costs = [[0.371981, 0.168296]]\n",
      "Generation 40: Best costs = [[0.371981, 0.132955]]\n",
      "Generation 37: Best costs = [[0.371981, 0.168296]]\n",
      "Generation 41: Best costs = [[0.371981, 0.132955]]\n",
      "Generation 38: Best costs = [[0.371981, 0.168296]]\n",
      "Generation 42: Best costs = [[0.371981, 0.132955]]\n",
      "Generation 39: Best costs = [[0.371981, 0.166693]]\n",
      "Generation 43: Best costs = [[0.371981, 0.132955]]\n",
      "Generation 40: Best costs = [[0.371981, 0.162227]]\n",
      "Generation 44: Best costs = [[0.371981, 0.130737]]\n",
      "Generation 45: Best costs = [[0.371981, 0.130737]]\n",
      "Generation 41: Best costs = [[0.371981, 0.162227]]\n",
      "Generation 46: Best costs = [[0.371981, 0.130737]]\n",
      "Generation 42: Best costs = [[0.371981, 0.160317]]\n",
      "Generation 47: Best costs = [[0.371981, 0.130737]]\n",
      "Generation 43: Best costs = [[0.371981, 0.155942]]\n",
      "Generation 48: Best costs = [[0.371981, 0.130737]]\n",
      "Generation 44: Best costs = [[0.371981, 0.154525]]\n",
      "Generation 45: Best costs = [[0.371981, 0.153979]]\n",
      "Generation 49: Best costs = [[0.371981, 0.119357]]\n",
      "Generation 50: Best costs = [[0.371981, 0.114396]]\n",
      "Generation 46: Best costs = [[0.371981, 0.152957]]\n",
      "Generation 51: Best costs = [[0.371981, 0.114396]]\n",
      "Generation 47: Best costs = [[0.371981, 0.147805]]\n",
      "Generation 52: Best costs = [[0.371981, 0.114396]]\n",
      "Generation 48: Best costs = [[0.371981, 0.143909]]\n",
      "Generation 53: Best costs = [[0.371981, 0.111945]]\n",
      "Generation 49: Best costs = [[0.371981, 0.140341]]\n",
      "Generation 50: Best costs = [[0.371981, 0.138245]]\n",
      "Generation 54: Best costs = [[0.371981, 0.108111]]\n",
      "Generation 51: Best costs = [[0.371981, 0.138245]]\n",
      "Generation 55: Best costs = [[0.371981, 0.106936]]\n",
      "Generation 52: Best costs = [[0.371981, 0.128883]]\n",
      "Generation 56: Best costs = [[0.371981, 0.099653]]\n",
      "Generation 53: Best costs = [[0.371981, 0.128883]]\n",
      "Generation 57: Best costs = [[0.371981, 0.093083]]\n",
      "Generation 54: Best costs = [[0.371981, 0.125334]]\n",
      "Generation 58: Best costs = [[0.371981, 0.093083]]\n",
      "Generation 55: Best costs = [[0.371981, 0.120218]]\n",
      "Generation 59: Best costs = [[0.371981, 0.093083]]\n",
      "Generation 56: Best costs = [[0.371981, 0.114027]]\n",
      "Generation 60: Best costs = [[0.371981, 0.085486]]\n",
      "Generation 57: Best costs = [[0.371981, 0.11268]]\n",
      "Generation 61: Best costs = [[0.371981, 0.083879]]\n",
      "Generation 58: Best costs = [[0.371981, 0.108253]]\n",
      "Generation 62: Best costs = [[0.371981, 0.083839]]\n",
      "Generation 59: Best costs = [[0.371981, 0.101362]]\n",
      "Generation 63: Best costs = [[0.371981, 0.083839]]\n",
      "Generation 64: Best costs = [[0.371981, 0.080005]]\n",
      "Generation 60: Best costs = [[0.371981, 0.101362]]\n",
      "Generation 65: Best costs = [[0.371981, 0.080005]]\n",
      "Generation 61: Best costs = [[0.371981, 0.098927]]\n",
      "Generation 62: Best costs = [[0.371981, 0.097759]]\n",
      "Generation 66: Best costs = [[0.371981, 0.080005]]\n",
      "Generation 63: Best costs = [[0.371981, 0.092213]]\n",
      "Generation 67: Best costs = [[0.371981, 0.080005]]\n",
      "Generation 64: Best costs = [[0.371981, 0.092065]]\n",
      "Generation 68: Best costs = [[0.371981, 0.079568]]\n",
      "Generation 65: Best costs = [[0.371981, 0.090078]]\n",
      "Generation 69: Best costs = [[0.371981, 0.07898]]\n",
      "Generation 66: Best costs = [[0.371981, 0.086752]]\n",
      "Generation 70: Best costs = [[0.371981, 0.075468]]\n",
      "Generation 67: Best costs = [[0.371981, 0.082888]]\n",
      "Generation 71: Best costs = [[0.371981, 0.07411]]\n",
      "Generation 68: Best costs = [[0.371981, 0.082888]]\n",
      "Generation 72: Best costs = [[0.371981, 0.071116]]\n",
      "Generation 69: Best costs = [[0.371981, 0.082888]]\n",
      "Generation 73: Best costs = [[0.371981, 0.071116]]\n",
      "Generation 70: Best costs = [[0.371981, 0.082646]]\n",
      "Generation 74: Best costs = [[0.371981, 0.069562]]\n",
      "Generation 71: Best costs = [[0.371981, 0.082112]]\n",
      "Generation 75: Best costs = [[0.371981, 0.069562]]\n",
      "Generation 72: Best costs = [[0.371981, 0.082112]]\n",
      "Generation 76: Best costs = [[0.371981, 0.069427]]\n",
      "Generation 73: Best costs = [[0.371981, 0.07933]]\n",
      "Generation 77: Best costs = [[0.371981, 0.069064]]\n",
      "Generation 74: Best costs = [[0.371981, 0.07933]]\n",
      "Generation 78: Best costs = [[0.371981, 0.069064]]\n",
      "Generation 75: Best costs = [[0.371981, 0.077673]]\n",
      "Generation 79: Best costs = [[0.371981, 0.069064]]\n",
      "Generation 76: Best costs = [[0.371981, 0.077673]]\n",
      "Generation 80: Best costs = [[0.371981, 0.066393]]\n",
      "Generation 77: Best costs = [[0.371981, 0.077003]]\n",
      "Generation 81: Best costs = [[0.371981, 0.065102]]\n",
      "Generation 78: Best costs = [[0.371981, 0.077003]]\n",
      "Generation 82: Best costs = [[0.371981, 0.062917]]\n",
      "Generation 79: Best costs = [[0.371981, 0.075486]]\n",
      "Generation 83: Best costs = [[0.371981, 0.062917]]\n",
      "Generation 80: Best costs = [[0.371981, 0.071683]]\n",
      "Generation 81: Best costs = [[0.371981, 0.070929]]\n",
      "Generation 84: Best costs = [[0.371981, 0.062917]]\n",
      "Generation 85: Best costs = [[0.371981, 0.060324]]\n",
      "Generation 82: Best costs = [[0.371981, 0.067908]]\n",
      "Generation 86: Best costs = [[0.371981, 0.059168]]\n",
      "Generation 83: Best costs = [[0.371981, 0.067908]]\n",
      "Generation 87: Best costs = [[0.371981, 0.056143]]\n",
      "Generation 84: Best costs = [[0.371981, 0.067908]]\n",
      "Generation 88: Best costs = [[0.371981, 0.056143]]\n",
      "Generation 85: Best costs = [[0.371981, 0.062819]]\n",
      "Generation 89: Best costs = [[0.371981, 0.056143]]\n",
      "Generation 86: Best costs = [[0.371981, 0.059556]]\n",
      "Generation 90: Best costs = [[0.371981, 0.056143]]\n",
      "Generation 87: Best costs = [[0.371981, 0.055903]]\n",
      "Generation 91: Best costs = [[0.371981, 0.055103]]\n",
      "Generation 88: Best costs = [[0.371981, 0.052781]]\n",
      "Generation 92: Best costs = [[0.371981, 0.054901]]\n",
      "Generation 93: Best costs = [[0.371981, 0.054611]]\n",
      "Generation 89: Best costs = [[0.371981, 0.05004]]\n",
      "Generation 90: Best costs = [[0.371981, 0.047759]]\n",
      "Generation 94: Best costs = [[0.371981, 0.053501]]\n",
      "Generation 95: Best costs = [[0.371981, 0.052973]]\n",
      "Generation 91: Best costs = [[0.371981, 0.044896]]\n",
      "Generation 92: Best costs = [[0.371981, 0.044811]]\n",
      "Generation 96: Best costs = [[0.371981, 0.052914]]\n",
      "Generation 97: Best costs = [[0.371981, 0.0512]]\n",
      "Generation 93: Best costs = [[0.371981, 0.044811]]\n",
      "Generation 98: Best costs = [[0.371981, 0.0512]]\n",
      "Generation 94: Best costs = [[0.371981, 0.044811]]\n",
      "Generation 99: Best costs = [[0.371981, 0.0512]]\n",
      "Generation 95: Best costs = [[0.371981, 0.044811]]\n",
      "Generation 100: Best costs = [[0.371981, 0.0512]]\n",
      "Generation 96: Best costs = [[0.371981, 0.038364]]\n",
      "Generation 101: Best costs = [[0.371981, 0.0512]]\n",
      "Generation 97: Best costs = [[0.371981, 0.037733]]\n",
      "Generation 102: Best costs = [[0.371981, 0.0512]]\n",
      "Generation 98: Best costs = [[0.371981, 0.037733]]\n",
      "Generation 103: Best costs = [[0.371981, 0.051029]]\n",
      "Generation 99: Best costs = [[0.371981, 0.037733]]\n",
      "Generation 104: Best costs = [[0.371981, 0.051029]]\n",
      "Generation 100: Best costs = [[0.371981, 0.036915]]\n",
      "Generation 105: Best costs = [[0.371981, 0.05097]]\n",
      "Generation 101: Best costs = [[0.371981, 0.035802]]\n",
      "Generation 106: Best costs = [[0.371981, 0.05097]]\n",
      "Generation 102: Best costs = [[0.371981, 0.03356]]\n",
      "Generation 107: Best costs = [[0.371981, 0.050119]]\n",
      "Generation 103: Best costs = [[0.371981, 0.03356]]\n",
      "Generation 108: Best costs = [[0.371981, 0.050119]]\n",
      "Generation 104: Best costs = [[0.371981, 0.03356]]\n",
      "Generation 109: Best costs = [[0.371981, 0.049481]]\n",
      "Generation 105: Best costs = [[0.371981, 0.031239]]\n",
      "Generation 110: Best costs = [[0.371981, 0.049481]]\n",
      "Generation 106: Best costs = [[0.371981, 0.031239]]\n",
      "Generation 111: Best costs = [[0.371981, 0.049481]]\n",
      "Generation 107: Best costs = [[0.371981, 0.031239]]\n",
      "Generation 112: Best costs = [[0.371981, 0.048662]]\n",
      "Generation 108: Best costs = [[0.371981, 0.028915]]\n",
      "Generation 113: Best costs = [[0.371981, 0.047548]]\n",
      "Generation 109: Best costs = [[0.371981, 0.028915]]\n",
      "Generation 114: Best costs = [[0.371981, 0.045705]]\n",
      "Generation 110: Best costs = [[0.371981, 0.027909]]\n",
      "Generation 115: Best costs = [[0.371981, 0.045705]]\n",
      "Generation 111: Best costs = [[0.371981, 0.02778]]\n",
      "Generation 116: Best costs = [[0.371981, 0.044976]]\n",
      "Generation 112: Best costs = [[0.371981, 0.027086]]\n",
      "Generation 117: Best costs = [[0.371981, 0.044626]]\n",
      "Generation 113: Best costs = [[0.371981, 0.027086]]\n",
      "Generation 118: Best costs = [[0.371981, 0.043567]]\n",
      "Generation 114: Best costs = [[0.371981, 0.026602]]\n",
      "Generation 119: Best costs = [[0.371981, 0.043567]]\n",
      "Generation 115: Best costs = [[0.371981, 0.025619]]\n",
      "Generation 120: Best costs = [[0.371981, 0.042565]]\n",
      "Generation 116: Best costs = [[0.371981, 0.025334]]\n",
      "Generation 121: Best costs = [[0.371981, 0.042565]]\n",
      "Generation 117: Best costs = [[0.371981, 0.024848]]\n",
      "Generation 122: Best costs = [[0.371981, 0.042495]]\n",
      "Generation 118: Best costs = [[0.371981, 0.024848]]\n",
      "Generation 123: Best costs = [[0.371981, 0.040853]]\n",
      "Generation 119: Best costs = [[0.371981, 0.024848]]\n",
      "Generation 124: Best costs = [[0.371981, 0.040853]]\n",
      "Generation 120: Best costs = [[0.371981, 0.024848]]\n",
      "Generation 125: Best costs = [[0.371981, 0.039656]]\n",
      "Generation 121: Best costs = [[0.371981, 0.024848]]\n",
      "Generation 126: Best costs = [[0.371981, 0.039656]]\n",
      "Generation 122: Best costs = [[0.371981, 0.023768]]\n",
      "Generation 127: Best costs = [[0.371981, 0.038383]]\n",
      "Generation 123: Best costs = [[0.371981, 0.023768]]\n",
      "Generation 128: Best costs = [[0.371981, 0.038305]]\n",
      "Generation 124: Best costs = [[0.371981, 0.023768]]\n",
      "Generation 129: Best costs = [[0.371981, 0.038305]]\n",
      "Generation 125: Best costs = [[0.371981, 0.023768]]\n",
      "Generation 130: Best costs = [[0.371981, 0.038305]]\n",
      "Generation 126: Best costs = [[0.371981, 0.023768]]\n",
      "Generation 127: Best costs = [[0.371981, 0.02284]]\n",
      "Generation 131: Best costs = [[0.371981, 0.038305]]\n",
      "Generation 128: Best costs = [[0.371981, 0.022812]]\n",
      "Generation 132: Best costs = [[0.371981, 0.037764]]\n",
      "Generation 129: Best costs = [[0.371981, 0.022599]]\n",
      "Generation 133: Best costs = [[0.371981, 0.036947]]\n",
      "Generation 130: Best costs = [[0.371981, 0.022471]]\n",
      "Generation 134: Best costs = [[0.371981, 0.036947]]\n",
      "Generation 131: Best costs = [[0.371981, 0.02144]]\n",
      "Generation 135: Best costs = [[0.371981, 0.036206]]\n",
      "Generation 132: Best costs = [[0.371981, 0.020732]]\n",
      "Generation 136: Best costs = [[0.371981, 0.035546]]\n",
      "Generation 133: Best costs = [[0.371981, 0.019959]]\n",
      "Generation 137: Best costs = [[0.371981, 0.035546]]\n",
      "Generation 134: Best costs = [[0.371981, 0.019879]]\n",
      "Generation 138: Best costs = [[0.371981, 0.035546]]\n",
      "Generation 135: Best costs = [[0.371981, 0.019879]]\n",
      "Generation 139: Best costs = [[0.371981, 0.035402]]\n",
      "Generation 136: Best costs = [[0.371981, 0.019879]]\n",
      "Generation 140: Best costs = [[0.371981, 0.03508]]\n",
      "Generation 137: Best costs = [[0.371981, 0.019879]]\n",
      "Generation 141: Best costs = [[0.371981, 0.034207]]\n",
      "Generation 138: Best costs = [[0.371981, 0.019634]]\n",
      "Generation 142: Best costs = [[0.371981, 0.033968]]\n",
      "Generation 139: Best costs = [[0.371981, 0.018759]]\n",
      "Generation 143: Best costs = [[0.371981, 0.033968]]\n",
      "Generation 140: Best costs = [[0.371981, 0.018759]]\n",
      "Generation 144: Best costs = [[0.371981, 0.033968]]\n",
      "Generation 141: Best costs = [[0.371981, 0.018353]]\n",
      "Generation 145: Best costs = [[0.371981, 0.033231]]\n",
      "Generation 146: Best costs = [[0.371981, 0.033231]]\n",
      "Generation 142: Best costs = [[0.371981, 0.018353]]\n",
      "Generation 143: Best costs = [[0.371981, 0.018353]]\n",
      "Generation 147: Best costs = [[0.371981, 0.033187]]\n",
      "Generation 144: Best costs = [[0.371981, 0.018353]]\n",
      "Generation 148: Best costs = [[0.371981, 0.03308]]\n",
      "Generation 145: Best costs = [[0.371981, 0.01827]]\n",
      "Generation 149: Best costs = [[0.371981, 0.032764]]\n",
      "Generation 150: Best costs = [[0.371981, 0.032271]]\n",
      "Generation 146: Best costs = [[0.371981, 0.01827]]\n",
      "Generation 147: Best costs = [[0.371981, 0.017859]]\n",
      "Generation 151: Best costs = [[0.371981, 0.032271]]\n",
      "Generation 152: Best costs = [[0.371981, 0.032271]]\n",
      "Generation 148: Best costs = [[0.371981, 0.017859]]\n",
      "Generation 153: Best costs = [[0.371981, 0.032271]]\n",
      "Generation 149: Best costs = [[0.371981, 0.017859]]\n",
      "Generation 150: Best costs = [[0.371981, 0.017556]]\n",
      "Generation 154: Best costs = [[0.371981, 0.032217]]\n",
      "Generation 151: Best costs = [[0.371981, 0.017556]]\n",
      "Generation 155: Best costs = [[0.371981, 0.031987]]\n",
      "Generation 152: Best costs = [[0.371981, 0.017194]]\n",
      "Generation 156: Best costs = [[0.371981, 0.031904]]\n",
      "Generation 153: Best costs = [[0.371981, 0.016479]]\n",
      "Generation 157: Best costs = [[0.371981, 0.031904]]\n",
      "Generation 154: Best costs = [[0.371981, 0.016479]]\n",
      "Generation 158: Best costs = [[0.371981, 0.031758]]\n",
      "Generation 155: Best costs = [[0.371981, 0.016479]]\n",
      "Generation 159: Best costs = [[0.371981, 0.031758]]\n",
      "Generation 156: Best costs = [[0.371981, 0.016479]]\n",
      "Generation 157: Best costs = [[0.371981, 0.016446]]\n",
      "Generation 160: Best costs = [[0.371981, 0.031372]]\n",
      "Generation 158: Best costs = [[0.371981, 0.016446]]\n",
      "Generation 161: Best costs = [[0.371981, 0.031035]]\n",
      "Generation 159: Best costs = [[0.371981, 0.016217]]\n",
      "Generation 162: Best costs = [[0.371981, 0.028829]]\n",
      "Generation 160: Best costs = [[0.371981, 0.016217]]\n",
      "Generation 163: Best costs = [[0.371981, 0.028766]]\n",
      "Generation 161: Best costs = [[0.371981, 0.016217]]\n",
      "Generation 164: Best costs = [[0.371981, 0.028201]]\n",
      "Generation 162: Best costs = [[0.371981, 0.016217]]\n",
      "Generation 165: Best costs = [[0.371981, 0.028201]]\n",
      "Generation 166: Best costs = [[0.371981, 0.027842]]\n",
      "Generation 163: Best costs = [[0.371981, 0.016217]]\n",
      "Generation 167: Best costs = [[0.371981, 0.027842]]\n",
      "Generation 164: Best costs = [[0.371981, 0.016217]]\n",
      "Generation 168: Best costs = [[0.371981, 0.027573]]\n",
      "Generation 165: Best costs = [[0.371981, 0.016217]]\n",
      "Generation 169: Best costs = [[0.371981, 0.027573]]\n",
      "Generation 166: Best costs = [[0.371981, 0.016217]]\n",
      "Generation 167: Best costs = [[0.371981, 0.016217]]\n",
      "Generation 170: Best costs = [[0.371981, 0.02731]]\n",
      "Generation 168: Best costs = [[0.371981, 0.016169]]\n",
      "Generation 171: Best costs = [[0.371981, 0.027271]]\n",
      "Generation 169: Best costs = [[0.371981, 0.016169]]\n",
      "Generation 172: Best costs = [[0.371981, 0.027271]]\n",
      "Generation 170: Best costs = [[0.371981, 0.016169]]\n",
      "Generation 173: Best costs = [[0.371981, 0.027196]]\n",
      "Generation 171: Best costs = [[0.371981, 0.016163]]\n",
      "Generation 174: Best costs = [[0.371981, 0.027196]]\n",
      "Generation 172: Best costs = [[0.371981, 0.015713]]\n",
      "Generation 175: Best costs = [[0.371981, 0.026904]]\n",
      "Generation 173: Best costs = [[0.371981, 0.015713]]\n",
      "Generation 176: Best costs = [[0.371981, 0.026904]]\n",
      "Generation 177: Best costs = [[0.371981, 0.026904]]\n",
      "Generation 174: Best costs = [[0.371981, 0.015713]]\n",
      "Generation 178: Best costs = [[0.371981, 0.026904]]\n",
      "Generation 175: Best costs = [[0.371981, 0.015713]]\n",
      "Generation 179: Best costs = [[0.371981, 0.026838]]\n",
      "Generation 176: Best costs = [[0.371981, 0.015713]]\n",
      "Generation 180: Best costs = [[0.371981, 0.026838]]\n",
      "Generation 177: Best costs = [[0.371981, 0.01559]]\n",
      "Generation 181: Best costs = [[0.371981, 0.026838]]\n",
      "Generation 178: Best costs = [[0.371981, 0.01559]]\n",
      "Generation 182: Best costs = [[0.371981, 0.025025]]\n",
      "Generation 179: Best costs = [[0.371981, 0.015183]]\n",
      "Generation 183: Best costs = [[0.371981, 0.025025]]\n",
      "Generation 180: Best costs = [[0.371981, 0.015183]]\n",
      "Generation 181: Best costs = [[0.371981, 0.015183]]\n",
      "Generation 184: Best costs = [[0.371981, 0.025025]]\n",
      "Generation 182: Best costs = [[0.371981, 0.015141]]\n",
      "Generation 185: Best costs = [[0.371981, 0.023183]]\n",
      "Generation 183: Best costs = [[0.371981, 0.014846]]\n",
      "Generation 186: Best costs = [[0.371981, 0.023183]]\n",
      "Generation 184: Best costs = [[0.371981, 0.014846]]\n",
      "Generation 187: Best costs = [[0.371981, 0.022795]]\n",
      "Generation 185: Best costs = [[0.371981, 0.014181]]\n",
      "Generation 188: Best costs = [[0.371981, 0.022795]]\n",
      "Generation 186: Best costs = [[0.371981, 0.014181]]\n",
      "Generation 189: Best costs = [[0.371981, 0.022795]]\n",
      "Generation 187: Best costs = [[0.371981, 0.013579]]\n",
      "Generation 190: Best costs = [[0.371981, 0.021324]]\n",
      "Generation 188: Best costs = [[0.371981, 0.013579]]\n",
      "Generation 191: Best costs = [[0.371981, 0.021324]]\n",
      "Generation 189: Best costs = [[0.371981, 0.013579]]\n",
      "Generation 192: Best costs = [[0.371981, 0.021324]]\n",
      "Generation 190: Best costs = [[0.371981, 0.013579]]\n",
      "Generation 193: Best costs = [[0.371981, 0.021324]]\n",
      "Generation 191: Best costs = [[0.371981, 0.013004]]\n",
      "Generation 194: Best costs = [[0.371981, 0.021324]]\n",
      "Generation 192: Best costs = [[0.371981, 0.013004]]\n",
      "Generation 195: Best costs = [[0.371981, 0.021306]]\n",
      "Generation 193: Best costs = [[0.371981, 0.013004]]\n",
      "Generation 196: Best costs = [[0.371981, 0.021306]]\n",
      "Generation 194: Best costs = [[0.371981, 0.012905]]\n",
      "Generation 197: Best costs = [[0.371981, 0.021306]]\n",
      "Generation 195: Best costs = [[0.371981, 0.012905]]\n",
      "Generation 198: Best costs = [[0.371981, 0.021144]]\n",
      "Generation 196: Best costs = [[0.371981, 0.012831]]\n",
      "Generation 199: Best costs = [[0.371981, 0.018143]]\n",
      "Generation 197: Best costs = [[0.371981, 0.012831]]\n",
      "Generation 200: Best costs = [[0.371981, 0.018143]]\n",
      "Generation 198: Best costs = [[0.371981, 0.012661]]\n",
      "Generation 201: Best costs = [[0.371981, 0.018143]]\n",
      "Generation 199: Best costs = [[0.371981, 0.012661]]\n",
      "Generation 202: Best costs = [[0.371981, 0.018143]]\n",
      "Generation 200: Best costs = [[0.371981, 0.012661]]\n",
      "Generation 203: Best costs = [[0.371981, 0.01813]]\n",
      "Generation 201: Best costs = [[0.371981, 0.012661]]\n",
      "Generation 204: Best costs = [[0.371981, 0.01813]]\n",
      "Generation 202: Best costs = [[0.371981, 0.012661]]\n",
      "Generation 205: Best costs = [[0.371981, 0.018117]]\n",
      "Generation 203: Best costs = [[0.371981, 0.012661]]\n",
      "Generation 206: Best costs = [[0.371981, 0.018117]]\n",
      "Generation 204: Best costs = [[0.371981, 0.012581]]\n",
      "Generation 207: Best costs = [[0.371981, 0.017893]]\n",
      "Generation 205: Best costs = [[0.371981, 0.012581]]\n",
      "Generation 208: Best costs = [[0.371981, 0.017893]]\n",
      "Generation 206: Best costs = [[0.371981, 0.012581]]\n",
      "Generation 209: Best costs = [[0.371981, 0.017392]]\n",
      "Generation 207: Best costs = [[0.371981, 0.012307]]\n",
      "Generation 210: Best costs = [[0.371981, 0.017306]]\n",
      "Generation 208: Best costs = [[0.371981, 0.012037]]\n",
      "Generation 211: Best costs = [[0.371981, 0.01702]]\n",
      "Generation 209: Best costs = [[0.371981, 0.012037]]\n",
      "Generation 210: Best costs = [[0.371981, 0.011933]]\n",
      "Generation 212: Best costs = [[0.371981, 0.01702]]\n",
      "Generation 211: Best costs = [[0.371981, 0.011933]]\n",
      "Generation 213: Best costs = [[0.371981, 0.016797]]\n",
      "Generation 212: Best costs = [[0.371981, 0.011933]]\n",
      "Generation 214: Best costs = [[0.371981, 0.016352]]\n",
      "Generation 213: Best costs = [[0.371981, 0.011491]]\n",
      "Generation 215: Best costs = [[0.371981, 0.016352]]\n",
      "Generation 216: Best costs = [[0.371981, 0.016352]]\n",
      "Generation 214: Best costs = [[0.371981, 0.011491]]\n",
      "Generation 217: Best costs = [[0.371981, 0.016352]]\n",
      "Generation 215: Best costs = [[0.371981, 0.011416]]\n",
      "Generation 218: Best costs = [[0.371981, 0.016292]]\n",
      "Generation 216: Best costs = [[0.371981, 0.011416]]\n",
      "Generation 217: Best costs = [[0.371981, 0.011416]]\n",
      "Generation 219: Best costs = [[0.371981, 0.015398]]\n",
      "Generation 218: Best costs = [[0.371981, 0.011367]]\n",
      "Generation 220: Best costs = [[0.371981, 0.014818]]\n",
      "Generation 219: Best costs = [[0.371981, 0.010813]]\n",
      "Generation 221: Best costs = [[0.371981, 0.014688]]\n",
      "Generation 220: Best costs = [[0.371981, 0.010813]]\n",
      "Generation 222: Best costs = [[0.371981, 0.014688]]\n",
      "Generation 221: Best costs = [[0.371981, 0.010813]]\n",
      "Generation 223: Best costs = [[0.371981, 0.014688]]\n",
      "Generation 222: Best costs = [[0.371981, 0.010813]]\n",
      "Generation 224: Best costs = [[0.371981, 0.014195]]\n",
      "Generation 223: Best costs = [[0.371981, 0.010776]]\n",
      "Generation 225: Best costs = [[0.371981, 0.014195]]\n",
      "Generation 224: Best costs = [[0.371981, 0.010776]]\n",
      "Generation 226: Best costs = [[0.371981, 0.013515]]\n",
      "Generation 227: Best costs = [[0.371981, 0.013515]]\n",
      "Generation 225: Best costs = [[0.371981, 0.010776]]\n",
      "Generation 226: Best costs = [[0.371981, 0.010776]]\n",
      "Generation 228: Best costs = [[0.371981, 0.013382]]\n",
      "Generation 229: Best costs = [[0.371981, 0.013382]]\n",
      "Generation 227: Best costs = [[0.371981, 0.010588]]\n",
      "Generation 230: Best costs = [[0.371981, 0.013382]]\n",
      "Generation 228: Best costs = [[0.371981, 0.010588]]\n",
      "Generation 231: Best costs = [[0.371981, 0.013382]]\n",
      "Generation 229: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 232: Best costs = [[0.371981, 0.013266]]\n",
      "Generation 230: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 233: Best costs = [[0.371981, 0.013266]]\n",
      "Generation 231: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 234: Best costs = [[0.371981, 0.013157]]\n",
      "Generation 232: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 235: Best costs = [[0.371981, 0.012974]]\n",
      "Generation 233: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 236: Best costs = [[0.371981, 0.012974]]\n",
      "Generation 234: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 237: Best costs = [[0.371981, 0.012333]]\n",
      "Generation 235: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 238: Best costs = [[0.371981, 0.012285]]\n",
      "Generation 236: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 239: Best costs = [[0.371981, 0.012285]]\n",
      "Generation 237: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 240: Best costs = [[0.371981, 0.012285]]\n",
      "Generation 238: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 241: Best costs = [[0.371981, 0.012285]]\n",
      "Generation 239: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 242: Best costs = [[0.371981, 0.012285]]\n",
      "Generation 240: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 243: Best costs = [[0.371981, 0.012285]]\n",
      "Generation 241: Best costs = [[0.371981, 0.010546]]\n",
      "Generation 244: Best costs = [[0.371981, 0.012095]]\n",
      "Generation 245: Best costs = [[0.371981, 0.011781]]\n",
      "Generation 242: Best costs = [[0.371981, 0.010461]]\n",
      "Generation 246: Best costs = [[0.371981, 0.011781]]\n",
      "Generation 243: Best costs = [[0.371981, 0.010461]]\n",
      "Generation 244: Best costs = [[0.371981, 0.010249]]\n",
      "Generation 247: Best costs = [[0.371981, 0.011781]]\n",
      "Generation 245: Best costs = [[0.371981, 0.010249]]\n",
      "Generation 248: Best costs = [[0.371981, 0.011704]]\n",
      "Generation 246: Best costs = [[0.371981, 0.010171]]\n",
      "Generation 249: Best costs = [[0.371981, 0.011704]]\n",
      "Generation 247: Best costs = [[0.371981, 0.010171]]\n",
      "Generation 250: Best costs = [[0.371981, 0.011704]]\n",
      "Generation 248: Best costs = [[0.371981, 0.010171]]\n",
      "Generation 251: Best costs = [[0.371981, 0.011511]]\n",
      "Generation 252: Best costs = [[0.371981, 0.011361]]\n",
      "Generation 249: Best costs = [[0.371981, 0.010171]]\n",
      "Generation 253: Best costs = [[0.371981, 0.011361]]\n",
      "Generation 250: Best costs = [[0.371981, 0.010171]]\n",
      "Generation 254: Best costs = [[0.371981, 0.011361]]\n",
      "Generation 251: Best costs = [[0.371981, 0.010057]]\n",
      "Generation 255: Best costs = [[0.371981, 0.011194]]\n",
      "Generation 252: Best costs = [[0.371981, 0.009596]]\n",
      "Generation 256: Best costs = [[0.371981, 0.011194]]\n",
      "Generation 253: Best costs = [[0.371981, 0.009596]]\n",
      "Generation 257: Best costs = [[0.371981, 0.011019]]\n",
      "Generation 254: Best costs = [[0.371981, 0.009596]]\n",
      "Generation 258: Best costs = [[0.371981, 0.011019]]\n",
      "Generation 255: Best costs = [[0.371981, 0.009596]]\n",
      "Generation 256: Best costs = [[0.371981, 0.009596]]\n",
      "Generation 259: Best costs = [[0.371981, 0.010738]]\n",
      "Generation 257: Best costs = [[0.371981, 0.009596]]\n",
      "Generation 260: Best costs = [[0.371981, 0.010738]]\n",
      "Generation 258: Best costs = [[0.371981, 0.009534]]\n",
      "Generation 261: Best costs = [[0.371981, 0.010428]]\n",
      "Generation 259: Best costs = [[0.371981, 0.009534]]\n",
      "Generation 262: Best costs = [[0.371981, 0.010428]]\n",
      "Generation 260: Best costs = [[0.371981, 0.009291]]\n",
      "Generation 263: Best costs = [[0.371981, 0.010428]]\n",
      "Generation 261: Best costs = [[0.371981, 0.009028]]\n",
      "Generation 264: Best costs = [[0.371981, 0.010428]]\n",
      "Generation 262: Best costs = [[0.371981, 0.009028]]\n",
      "Generation 265: Best costs = [[0.371981, 0.010428]]\n",
      "Generation 266: Best costs = [[0.371981, 0.010401]]\n",
      "Generation 263: Best costs = [[0.371981, 0.009028]]\n",
      "Generation 264: Best costs = [[0.371981, 0.008949]]\n",
      "Generation 267: Best costs = [[0.371981, 0.010401]]\n",
      "Generation 265: Best costs = [[0.371981, 0.008949]]\n",
      "Generation 268: Best costs = [[0.371981, 0.010401]]\n",
      "Generation 269: Best costs = [[0.371981, 0.010375]]\n",
      "Generation 266: Best costs = [[0.371981, 0.008949]]\n",
      "Generation 270: Best costs = [[0.371981, 0.010375]]\n",
      "Generation 267: Best costs = [[0.371981, 0.008865]]\n",
      "Generation 271: Best costs = [[0.371981, 0.010375]]\n",
      "Generation 268: Best costs = [[0.371981, 0.008865]]\n",
      "Generation 272: Best costs = [[0.371981, 0.010375]]\n",
      "Generation 269: Best costs = [[0.371981, 0.008865]]\n",
      "Generation 273: Best costs = [[0.371981, 0.010303]]\n",
      "Generation 270: Best costs = [[0.371981, 0.008865]]\n",
      "Generation 274: Best costs = [[0.371981, 0.010069]]\n",
      "Generation 275: Best costs = [[0.371981, 0.010069]]\n",
      "Generation 271: Best costs = [[0.371981, 0.008865]]\n",
      "Generation 276: Best costs = [[0.371981, 0.010069]]\n",
      "Generation 272: Best costs = [[0.371981, 0.008865]]\n",
      "Generation 273: Best costs = [[0.371981, 0.008865]]\n",
      "Generation 277: Best costs = [[0.371981, 0.010069]]\n",
      "Generation 278: Best costs = [[0.371981, 0.010069]]\n",
      "Generation 274: Best costs = [[0.371981, 0.008865]]\n",
      "Generation 279: Best costs = [[0.371981, 0.010069]]\n",
      "Generation 275: Best costs = [[0.371981, 0.008865]]\n",
      "Generation 280: Best costs = [[0.371981, 0.010069]]\n",
      "Generation 276: Best costs = [[0.371981, 0.008865]]\n",
      "Generation 281: Best costs = [[0.371981, 0.010069]]\n",
      "Generation 277: Best costs = [[0.371981, 0.008695]]\n",
      "Generation 282: Best costs = [[0.371981, 0.010069]]\n",
      "Generation 278: Best costs = [[0.371981, 0.008695]]\n",
      "Generation 283: Best costs = [[0.371981, 0.010069]]\n",
      "Generation 279: Best costs = [[0.371981, 0.008695]]\n",
      "Generation 284: Best costs = [[0.371981, 0.010069]]\n",
      "Generation 280: Best costs = [[0.371981, 0.008695]]\n",
      "Generation 285: Best costs = [[0.371981, 0.010069]]\n",
      "Generation 281: Best costs = [[0.371981, 0.008695]]\n",
      "Generation 286: Best costs = [[0.371981, 0.010011]]\n",
      "Generation 282: Best costs = [[0.371981, 0.008695]]\n",
      "Generation 287: Best costs = [[0.371981, 0.009926]]\n",
      "Generation 283: Best costs = [[0.371981, 0.008695]]\n",
      "Generation 288: Best costs = [[0.371981, 0.009926]]\n",
      "Generation 284: Best costs = [[0.371981, 0.008626]]\n",
      "Generation 289: Best costs = [[0.371981, 0.009926]]\n",
      "Generation 285: Best costs = [[0.371981, 0.008626]]\n",
      "Generation 290: Best costs = [[0.371981, 0.00933]]\n",
      "Generation 291: Best costs = [[0.371981, 0.00933]]\n",
      "Generation 286: Best costs = [[0.371981, 0.008626]]\n",
      "Generation 287: Best costs = [[0.371981, 0.008466]]\n",
      "Generation 292: Best costs = [[0.371981, 0.00933]]\n",
      "Generation 288: Best costs = [[0.371981, 0.008466]]\n",
      "Generation 293: Best costs = [[0.371981, 0.00933]]\n",
      "Generation 289: Best costs = [[0.371981, 0.008466]]\n",
      "Generation 294: Best costs = [[0.371981, 0.00933]]\n",
      "Generation 295: Best costs = [[0.371981, 0.009064]]\n",
      "Generation 290: Best costs = [[0.371981, 0.008466]]\n",
      "Generation 296: Best costs = [[0.371981, 0.008923]]\n",
      "Generation 291: Best costs = [[0.371981, 0.008419]]\n",
      "Generation 297: Best costs = [[0.371981, 0.008923]]\n",
      "Generation 292: Best costs = [[0.371981, 0.008419]]\n",
      "Generation 293: Best costs = [[0.371981, 0.008393]]\n",
      "Generation 298: Best costs = [[0.371981, 0.008923]]\n",
      "Generation 294: Best costs = [[0.371981, 0.008393]]\n",
      "Generation 299: Best costs = [[0.371981, 0.008923]]\n",
      "Before optimization:  0.15821176767349243 0.009419502690434456 3.8573601\n",
      "Generation 295: Best costs = [[0.371981, 0.008393]]\n",
      "Generation 296: Best costs = [[0.371981, 0.008393]]\n",
      "Generation 297: Best costs = [[0.371981, 0.00827]]\n",
      "Generation 298: Best costs = [[0.371981, 0.00827]]\n",
      "Generation 299: Best costs = [[0.371981, 0.00827]]\n",
      "Before optimization:  0.1580449491739273 0.009252682328224182 3.8573601\n",
      "After optimization:  0.1538403332233429 0.0004644639266189188 3.9163232\n",
      "After optimization:  0.1543109118938446 0.0001573607005411759 3.9262395\n",
      "After optimization:  0.15314489603042603 0.001254999777302146 3.8973055\n",
      "After optimization:  0.1547420620918274 3.0670555133838207e-05 3.933337\n",
      "After optimization:  0.15488888323307037 1.3994238543091342e-05 3.9354148\n",
      "After optimization:  0.15508019924163818 2.606688667583512e-06 3.9379892\n",
      "After optimization:  0.1551610827445984 6.848031262052245e-07 3.9390404\n",
      "After optimization:  0.15362514555454254 0.00026196279213763773 3.9161613\n",
      "After optimization:  0.15520241856575012 1.9372296833353175e-07 3.9395714\n",
      "After optimization:  0.15521538257598877 1.1399423982538792e-07 3.9397368\n",
      "After optimization:  0.1552302986383438 5.4923887660152104e-08 3.939927\n",
      "After optimization:  0.15524978935718536 9.441261994425076e-09 3.9401748\n",
      "After optimization:  0.1552540808916092 2.1595902843785098e-09 3.9402294\n",
      "After optimization:  0.1552559733390808 1.0636077574233127e-09 3.9402535\n",
      "After optimization:  0.15525753796100616 4.8366688432111e-10 3.9402733\n",
      "After optimization:  0.1552588939666748 1.4868437037129922e-10 3.9402907\n",
      "After optimization:  0.15525951981544495 6.788532236345901e-11 3.9402986\n",
      "Score of generated dataframe: 0.3881477104685439\n",
      "Runtime: 640.3s\n",
      "\n",
      "Seed 4000 completed in 640.3s\n",
      "Best final costs: [0.37198067]\n",
      "Modifier values: {'seed': 4000, 'make_single': True, 'use_minkowski': True}\n",
      "Saved full: ../../results/many_ga/full/Baseline_4000_65f3b229_f.pkl\n",
      "Saved abbr: ../../results/many_ga/abbr/Baseline_4000_65f3b229_a.pkl\n",
      "Seed 4000 finished successfully!\n",
      "============================================================\n",
      "\n",
      "*** Result received for seed 4000: avg_cost = 0.371981, time = 640.3s ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization:  0.1541217565536499 4.553089092951268e-05 3.9252546\n",
      "Starting process for seed 4002...\n",
      "After optimization:  0.15440332889556885 7.299903700186405e-06 3.929326\n",
      "local\n",
      "WARNING: CUDA MPS not active\n",
      "After optimization:  0.1545010358095169 2.3679879177507246e-06 3.9306319\n",
      "After optimization:  0.15454387664794922 1.2441940953067387e-06 3.9311912\n",
      "After optimization:  0.15460878610610962 1.7536611096602428e-07 3.9320302\n",
      "After optimization:  0.15462659299373627 5.734285224434643e-08 3.9322581\n",
      "After optimization:  0.15463386476039886 3.5313071578002564e-08 3.9323509\n",
      "After optimization:  0.15463729202747345 2.737250071049857e-08 3.9323945\n",
      "After optimization:  0.15465112030506134 5.207262621098607e-09 3.9325707\n",
      "After optimization:  0.15465286374092102 2.4719009061868746e-09 3.9325929\n",
      "After optimization:  0.15465351939201355 2.1473343103650677e-09 3.9326012\n",
      "After optimization:  0.1546545922756195 1.6250962797670354e-09 3.9326148\n",
      "After optimization:  0.15465900301933289 2.2505380892212656e-10 3.932671\n",
      "After optimization:  0.15465939044952393 1.6968226823621535e-10 3.9326758\n",
      "After optimization:  0.15465976297855377 1.2537326732342535e-10 3.9326806\n",
      "After optimization:  0.1546602100133896 8.463922618329178e-11 3.9326863\n",
      "Score of generated dataframe: 0.3866489880436265\n",
      "Runtime: 648.1s\n",
      "\n",
      "Seed 4001 completed in 648.1s\n",
      "Best final costs: [0.37198067]\n",
      "Modifier values: {'seed': 4001, 'make_single': True, 'use_minkowski': True}\n",
      "Saved full: ../../results/many_ga/full/Baseline_4001_65f3b229_f.pkl\n",
      "Saved abbr: ../../results/many_ga/abbr/Baseline_4001_65f3b229_a.pkl\n",
      "Seed 4001 finished successfully!\n",
      "============================================================\n",
      "\n",
      "*** Result received for seed 4001: avg_cost = 0.371981, time = 648.1s ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process for seed 4003...\n",
      "local\n",
      "WARNING: CUDA MPS not active\n",
      "Seed 4002 using GPU 0\n",
      "\n",
      "=== Starting seed 4002 (Process 38431) ===\n",
      "{'seed': 4002, 'make_single': True, 'use_minkowski_rough': False}\n",
      "init CUDA\n",
      "Detected GPU compute capability: 8.9 (arch=sm_89)\n",
      "GPU max threads per block: 1024\n",
      "=== Compiling kernel variant: crystal ===\n",
      "Defines: ENABLE_CRYSTAL_AXES, ENABLE_OVERLAP_AREA, ENABLE_SEPARATION\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -DENABLE_CRYSTAL_AXES -DENABLE_OVERLAP_AREA -DENABLE_SEPARATION -cubin /mnt/d//packing/temp/pack_cuda_saved.cu -o /mnt/d//packing/temp/pack_cuda_crystal.cubin\n",
      "ptxas info    : 0 bytes gmem, 1172 bytes cmem[3]\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers, 404 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compile time = 18.450 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    240 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 43 registers, used 1 barriers, 240 bytes cumulative stack size, 404 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 468.885 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    1264 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 80 registers, used 1 barriers, 1264 bytes cumulative stack size, 420 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 436.711 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [crystal] ---\n",
      "  Max threads per block (kernel): 768\n",
      "  Num registers: 80\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 1264\n",
      "=== Compiling kernel variant: no_sep ===\n",
      "Defines: ENABLE_OVERLAP_AREA\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -DENABLE_OVERLAP_AREA -cubin /mnt/d//packing/temp/pack_cuda_saved.cu -o /mnt/d//packing/temp/pack_cuda_no_sep.cubin\n",
      "ptxas info    : 0 bytes gmem, 1172 bytes cmem[3]\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers, 404 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compile time = 8.277 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    240 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 43 registers, used 1 barriers, 240 bytes cumulative stack size, 404 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 435.082 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    1232 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 64 registers, used 1 barriers, 1232 bytes cumulative stack size, 420 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 73.186 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [no_sep] ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 64\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 1232\n",
      "=== Compiling kernel variant: sep ===\n",
      "Defines: ENABLE_SEPARATION\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -DENABLE_SEPARATION -cubin /mnt/d//packing/temp/pack_cuda_saved.cu -o /mnt/d//packing/temp/pack_cuda_sep.cubin\n",
      "ptxas info    : 0 bytes gmem, 1172 bytes cmem[3]\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers, 404 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compile time = 9.080 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    240 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 43 registers, used 1 barriers, 240 bytes cumulative stack size, 404 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 476.589 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    144 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, used 1 barriers, 144 bytes cumulative stack size, 420 bytes cmem[0], 40 bytes cmem[2]\n",
      "ptxas info    : Compile time = 78.959 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [sep] ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 56\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 144\n",
      "\n",
      "--- Kernel: multi_boundary_list_total ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 43\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 240\n",
      "\n",
      "--- Kernel: multi_boundary_distance_list_total ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 36\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/packing/code/analysis/../core/pack_dynamics.py:60: VisibleDeprecationWarning: This function is deprecated and will be removed in a future release. Use the cupy.from_dlpack() array constructor instead.\n",
      "  x0 = from_dlpack(x0.toDlpack())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building lookup table for CollisionCostOverlappingArea...\n",
      "Building LUT: 451 x 900 x 900 = 365,310,000 grid points\n",
      "  Processing theta 1/900\n",
      "Seed 4003 using GPU 0\n",
      "\n",
      "=== Starting seed 4003 (Process 38565) ===\n",
      "{'seed': 4003, 'make_single': True, 'use_minkowski_rough': False}\n",
      "init CUDA\n",
      "Detected GPU compute capability: 8.9 (arch=sm_89)\n",
      "GPU max threads per block: 1024\n",
      "=== Compiling kernel variant: crystal ===\n",
      "Defines: ENABLE_CRYSTAL_AXES, ENABLE_OVERLAP_AREA, ENABLE_SEPARATION\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -DENABLE_CRYSTAL_AXES -DENABLE_OVERLAP_AREA -DENABLE_SEPARATION -cubin /mnt/d//packing/temp/pack_cuda_saved.cu -o /mnt/d//packing/temp/pack_cuda_crystal.cubin\n",
      "  Processing theta 51/900\n",
      "  Processing theta 101/900\n",
      "  Processing theta 151/900\n",
      "ptxas info    : 0 bytes gmem, 1172 bytes cmem[3]\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers, 404 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compile time = 10.029 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    240 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 43 registers, used 1 barriers, 240 bytes cumulative stack size, 404 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 560.570 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    1264 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 80 registers, used 1 barriers, 1264 bytes cumulative stack size, 420 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 459.025 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [crystal] ---\n",
      "  Max threads per block (kernel): 768\n",
      "  Num registers: 80\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 1264\n",
      "=== Compiling kernel variant: no_sep ===\n",
      "Defines: ENABLE_OVERLAP_AREA\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -DENABLE_OVERLAP_AREA -cubin /mnt/d//packing/temp/pack_cuda_saved.cu -o /mnt/d//packing/temp/pack_cuda_no_sep.cubin\n",
      "  Processing theta 201/900\n",
      "  Processing theta 251/900\n",
      "  Processing theta 301/900\n",
      "ptxas info    : 0 bytes gmem, 1172 bytes cmem[3]\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers, 404 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compile time = 15.342 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    240 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 43 registers, used 1 barriers, 240 bytes cumulative stack size, 404 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 607.307 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    1232 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 64 registers, used 1 barriers, 1232 bytes cumulative stack size, 420 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 79.028 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [no_sep] ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 64\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 1232\n",
      "=== Compiling kernel variant: sep ===\n",
      "Defines: ENABLE_SEPARATION\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -DENABLE_SEPARATION -cubin /mnt/d//packing/temp/pack_cuda_saved.cu -o /mnt/d//packing/temp/pack_cuda_sep.cubin\n",
      "  Processing theta 351/900\n",
      "  Processing theta 401/900\n",
      "ptxas info    : 0 bytes gmem, 1172 bytes cmem[3]\n",
      "ptxas info    : Compiling entry function 'multi_boundary_distance_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_distance_list_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, used 1 barriers, 404 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compile time = 8.640 ms\n",
      "ptxas info    : Compiling entry function 'multi_boundary_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_boundary_list_total\n",
      "    240 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 43 registers, used 1 barriers, 240 bytes cumulative stack size, 404 bytes cmem[0], 32 bytes cmem[2]\n",
      "ptxas info    : Compile time = 513.186 ms\n",
      "ptxas info    : Compiling entry function 'multi_overlap_list_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_list_total\n",
      "    144 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, used 1 barriers, 144 bytes cumulative stack size, 420 bytes cmem[0], 40 bytes cmem[2]\n",
      "ptxas info    : Compile time = 85.520 ms\n",
      "\n",
      "\n",
      "--- Kernel: multi_overlap_list_total [sep] ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 56\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 144\n",
      "\n",
      "--- Kernel: multi_boundary_list_total ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 43\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 240\n",
      "\n",
      "--- Kernel: multi_boundary_distance_list_total ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 36\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 1172\n",
      "  Local memory (bytes): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/packing/code/analysis/../core/pack_dynamics.py:60: VisibleDeprecationWarning: This function is deprecated and will be removed in a future release. Use the cupy.from_dlpack() array constructor instead.\n",
      "  x0 = from_dlpack(x0.toDlpack())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building lookup table for CollisionCostOverlappingArea...\n",
      "Building LUT: 451 x 900 x 900 = 365,310,000 grid points\n",
      "  Processing theta 1/900\n",
      "  Processing theta 451/900\n",
      "  Processing theta 501/900\n",
      "  Processing theta 51/900\n",
      "  Processing theta 551/900\n",
      "  Processing theta 101/900\n",
      "  Processing theta 601/900\n",
      "  Processing theta 151/900\n",
      "  Processing theta 651/900\n",
      "  Processing theta 201/900\n",
      "  Processing theta 701/900\n",
      "  Processing theta 251/900\n",
      "  Processing theta 751/900\n",
      "  Processing theta 301/900\n",
      "  Processing theta 801/900\n",
      "  Processing theta 351/900\n",
      "  Processing theta 851/900\n",
      "  Processing theta 401/900\n",
      "Cost range: [0.000000, 0.243503]\n",
      "  Processing theta 451/900\n",
      "  Processing theta 501/900\n",
      "  Processing theta 551/900\n",
      "  Processing theta 601/900\n",
      "Trimming zero edges:\n",
      "  X: 451 -> 362 (removed 89)\n",
      "  Y: 900 -> 839 (removed 61)\n",
      "  Theta: 900 -> 900 (removed 0)\n",
      "  Total reduction: 25.2% (91,963,800 points)\n",
      "  Processing theta 651/900\n",
      "  Processing theta 701/900\n",
      "  Processing theta 751/900\n",
      "  Processing theta 801/900\n",
      "Compiling CUDA LUT kernel one-time only)\n",
      "Detected GPU compute capability: 89 (arch=sm_89)\n",
      "Compiling: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -lineinfo -arch=sm_89 -cubin /mnt/d//packing/temp/pack_cuda_lut_saved.cu -o /mnt/d//packing/temp/pack_cuda_lut.cubin\n",
      "/mnt/d//packing/temp/pack_cuda_lut_saved.cu:5: warning: \"M_PI\" redefined\n",
      "    5 | #define M_PI 3.14159265358979323846f\n",
      "      | \n",
      "In file included from /usr/include/c++/13/cmath:47,\n",
      "                 from /usr/include/c++/13/math.h:36,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/crt/math_functions.h:4577,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/crt/common_functions.h:303,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:117,\n",
      "                 from <command-line>:\n",
      "/usr/include/math.h:1152: note: this is the location of the previous definition\n",
      " 1152 | # define M_PI           3.14159265358979323846  /* pi */\n",
      "      | \n",
      "ptxas info    : 16 bytes gmem, 48 bytes cmem[3], 16 bytes cmem[4]\n",
      "ptxas info    : Compiling entry function 'multi_overlap_lut_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_lut_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, used 1 barriers, 396 bytes cmem[0]\n",
      "ptxas info    : Compile time = 11.545 ms\n",
      "\n",
      "Kernel multi_overlap_lut_total:\n",
      "  Registers: 40\n",
      "  Shared mem: 0 bytes\n",
      "  Max threads/block: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/packing/code/analysis/../core/pack_dynamics.py:92: VisibleDeprecationWarning: This function is deprecated and will be removed in a future release. Use the cupy.from_dlpack() array constructor instead.\n",
      "  return from_dlpack(tmp_cost[:N].toDlpack()), from_dlpack(res.toDlpack())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing theta 851/900\n",
      "Cost range: [0.000000, 0.243503]\n",
      "init LAP CUDA\n",
      "Detected GPU compute capability: 8.9 (arch=sm_89)\n",
      "GPU max threads per block: 1024\n",
      "=== Compiling LAP kernels ===\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -cubin /mnt/d//packing/temp/lap_batch_saved.cu -o /mnt/d//packing/temp/lap_batch.cubin\n",
      "ptxas info    : 0 bytes gmem\n",
      "ptxas info    : Compiling entry function 'diversity_shortcut_kernel' for 'sm_89'\n",
      "ptxas info    : Function properties for diversity_shortcut_kernel\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 29 registers, used 1 barriers, 400 bytes cmem[0]\n",
      "ptxas info    : Compile time = 10.224 ms\n",
      "ptxas info    : Compiling entry function 'compute_costs' for 'sm_89'\n",
      "ptxas info    : Function properties for compute_costs\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 29 registers, used 0 barriers, 384 bytes cmem[0]\n",
      "ptxas info    : Compile time = 3.217 ms\n",
      "ptxas info    : Compiling entry function 'auction_kernel' for 'sm_89'\n",
      "ptxas info    : Function properties for auction_kernel\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 35 registers, used 0 barriers, 412 bytes cmem[0]\n",
      "ptxas info    : Compile time = 7.099 ms\n",
      "ptxas info    : Compiling entry function 'hungarian_kernel' for 'sm_89'\n",
      "ptxas info    : Function properties for hungarian_kernel\n",
      "    1808 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, used 0 barriers, 1808 bytes cumulative stack size, 408 bytes cmem[0]\n",
      "ptxas info    : Compile time = 15.577 ms\n",
      "\n",
      "\n",
      "--- Kernel: hungarian_kernel ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 40\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 1808\n",
      "\n",
      "--- Kernel: auction_kernel ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 35\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 0\n",
      "\n",
      "--- Kernel: compute_costs ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 29\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 0\n",
      "\n",
      "--- Kernel: diversity_shortcut_kernel ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 29\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 0\n",
      "Generation 0: Best costs = [[0.371981, 0.165717]]\n",
      "Generation 1: Best costs = [[0.371981, 0.077269]]\n",
      "Generation 2: Best costs = [[0.371981, 0.043813]]\n",
      "Generation 3: Best costs = [[0.371981, 0.033755]]\n",
      "Trimming zero edges:\n",
      "  X: 451 -> 362 (removed 89)\n",
      "  Y: 900 -> 839 (removed 61)\n",
      "  Theta: 900 -> 900 (removed 0)\n",
      "  Total reduction: 25.2% (91,963,800 points)\n",
      "Generation 4: Best costs = [[0.371981, 0.027985]]\n",
      "Generation 5: Best costs = [[0.371981, 0.021111]]\n",
      "Generation 6: Best costs = [[0.371981, 0.017641]]\n",
      "Generation 7: Best costs = [[0.371981, 0.016141]]\n",
      "Generation 8: Best costs = [[0.371981, 0.013831]]\n",
      "Generation 9: Best costs = [[0.371981, 0.012474]]\n",
      "Generation 10: Best costs = [[0.371981, 0.011769]]\n",
      "Generation 11: Best costs = [[0.371981, 0.010911]]\n",
      "Compiling CUDA LUT kernel one-time only)\n",
      "Detected GPU compute capability: 89 (arch=sm_89)\n",
      "Compiling: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -lineinfo -arch=sm_89 -cubin /mnt/d//packing/temp/pack_cuda_lut_saved.cu -o /mnt/d//packing/temp/pack_cuda_lut.cubin\n",
      "Generation 12: Best costs = [[0.371981, 0.009974]]\n",
      "/mnt/d//packing/temp/pack_cuda_lut_saved.cu:5: warning: \"M_PI\" redefined\n",
      "    5 | #define M_PI 3.14159265358979323846f\n",
      "      | \n",
      "In file included from /usr/include/c++/13/cmath:47,\n",
      "                 from /usr/include/c++/13/math.h:36,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/crt/math_functions.h:4577,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/crt/common_functions.h:303,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:117,\n",
      "                 from <command-line>:\n",
      "/usr/include/math.h:1152: note: this is the location of the previous definition\n",
      " 1152 | # define M_PI           3.14159265358979323846  /* pi */\n",
      "      | \n",
      "ptxas info    : 16 bytes gmem, 48 bytes cmem[3], 16 bytes cmem[4]\n",
      "ptxas info    : Compiling entry function 'multi_overlap_lut_total' for 'sm_89'\n",
      "ptxas info    : Function properties for multi_overlap_lut_total\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, used 1 barriers, 396 bytes cmem[0]\n",
      "ptxas info    : Compile time = 30.326 ms\n",
      "\n",
      "Kernel multi_overlap_lut_total:\n",
      "  Registers: 40\n",
      "  Shared mem: 0 bytes\n",
      "  Max threads/block: 1024\n",
      "Generation 13: Best costs = [[0.371981, 0.00794]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/packing/code/analysis/../core/pack_dynamics.py:92: VisibleDeprecationWarning: This function is deprecated and will be removed in a future release. Use the cupy.from_dlpack() array constructor instead.\n",
      "  return from_dlpack(tmp_cost[:N].toDlpack()), from_dlpack(res.toDlpack())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 14: Best costs = [[0.371981, 0.007772]]\n",
      "init LAP CUDA\n",
      "Detected GPU compute capability: 8.9 (arch=sm_89)\n",
      "GPU max threads per block: 1024\n",
      "=== Compiling LAP kernels ===\n",
      "Command: /usr/local/cuda/bin/nvcc -O3 -use_fast_math --extra-device-vectorization --ptxas-options=-v,--warn-on-spills -arch=sm_89 -cubin /mnt/d//packing/temp/lap_batch_saved.cu -o /mnt/d//packing/temp/lap_batch.cubin\n",
      "Generation 15: Best costs = [[0.371981, 0.00716]]\n",
      "ptxas info    : 0 bytes gmem\n",
      "ptxas info    : Compiling entry function 'diversity_shortcut_kernel' for 'sm_89'\n",
      "ptxas info    : Function properties for diversity_shortcut_kernel\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 29 registers, used 1 barriers, 400 bytes cmem[0]\n",
      "ptxas info    : Compile time = 18.959 ms\n",
      "ptxas info    : Compiling entry function 'compute_costs' for 'sm_89'\n",
      "ptxas info    : Function properties for compute_costs\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 29 registers, used 0 barriers, 384 bytes cmem[0]\n",
      "ptxas info    : Compile time = 3.115 ms\n",
      "ptxas info    : Compiling entry function 'auction_kernel' for 'sm_89'\n",
      "ptxas info    : Function properties for auction_kernel\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 35 registers, used 0 barriers, 412 bytes cmem[0]\n",
      "ptxas info    : Compile time = 6.581 ms\n",
      "ptxas info    : Compiling entry function 'hungarian_kernel' for 'sm_89'\n",
      "ptxas info    : Function properties for hungarian_kernel\n",
      "    1808 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, used 0 barriers, 1808 bytes cumulative stack size, 408 bytes cmem[0]\n",
      "ptxas info    : Compile time = 15.221 ms\n",
      "\n",
      "\n",
      "--- Kernel: hungarian_kernel ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 40\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 1808\n",
      "\n",
      "--- Kernel: auction_kernel ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 35\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 0\n",
      "\n",
      "--- Kernel: compute_costs ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 29\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 0\n",
      "\n",
      "--- Kernel: diversity_shortcut_kernel ---\n",
      "  Max threads per block (kernel): 1024\n",
      "  Num registers: 29\n",
      "  Shared memory (bytes): 0\n",
      "  Const memory (bytes): 0\n",
      "  Local memory (bytes): 0\n",
      "Generation 0: Best costs = [[0.371981, 0.187509]]\n",
      "Generation 1: Best costs = [[0.371981, 0.08586]]\n",
      "Generation 16: Best costs = [[0.371981, 0.007031]]\n",
      "Generation 2: Best costs = [[0.371981, 0.051677]]\n",
      "Generation 17: Best costs = [[0.371981, 0.005812]]\n",
      "Generation 18: Best costs = [[0.371981, 0.005569]]\n",
      "Generation 3: Best costs = [[0.371981, 0.031264]]\n",
      "Generation 19: Best costs = [[0.371981, 0.005365]]\n",
      "Generation 4: Best costs = [[0.371981, 0.026268]]\n",
      "Generation 5: Best costs = [[0.371981, 0.023418]]\n",
      "Generation 20: Best costs = [[0.371981, 0.005345]]\n",
      "Generation 21: Best costs = [[0.371981, 0.005174]]\n",
      "Generation 6: Best costs = [[0.371981, 0.018907]]\n",
      "Generation 22: Best costs = [[0.371981, 0.004787]]\n",
      "Generation 7: Best costs = [[0.371981, 0.017373]]\n",
      "Generation 8: Best costs = [[0.371981, 0.013515]]\n",
      "Generation 23: Best costs = [[0.371981, 0.004787]]\n",
      "Generation 9: Best costs = [[0.371981, 0.011277]]\n",
      "Generation 24: Best costs = [[0.371981, 0.004412]]\n",
      "Generation 10: Best costs = [[0.371981, 0.008614]]\n",
      "Generation 25: Best costs = [[0.371981, 0.003951]]\n",
      "Generation 11: Best costs = [[0.371981, 0.006913]]\n",
      "Generation 26: Best costs = [[0.371981, 0.003207]]\n",
      "Generation 12: Best costs = [[0.371981, 0.005265]]\n",
      "Generation 27: Best costs = [[0.371981, 0.002955]]\n",
      "Generation 13: Best costs = [[0.371981, 0.004394]]\n",
      "Generation 28: Best costs = [[0.371981, 0.002746]]\n",
      "Generation 14: Best costs = [[0.371981, 0.003455]]\n",
      "Generation 29: Best costs = [[0.371981, 0.002658]]\n",
      "Generation 15: Best costs = [[0.371981, 0.003167]]\n",
      "Generation 30: Best costs = [[0.371981, 0.002361]]\n",
      "Generation 16: Best costs = [[0.371981, 0.003015]]\n",
      "Generation 31: Best costs = [[0.371981, 0.002139]]\n",
      "Generation 17: Best costs = [[0.371981, 0.002047]]\n",
      "Generation 32: Best costs = [[0.371981, 0.002055]]\n",
      "Generation 18: Best costs = [[0.371981, 0.002047]]\n",
      "Generation 33: Best costs = [[0.371981, 0.001989]]\n",
      "Generation 19: Best costs = [[0.371981, 0.001765]]\n",
      "Generation 34: Best costs = [[0.371981, 0.001908]]\n",
      "Generation 20: Best costs = [[0.371981, 0.001392]]\n",
      "Generation 35: Best costs = [[0.371981, 0.001416]]\n",
      "Generation 21: Best costs = [[0.371981, 0.001241]]\n",
      "Generation 36: Best costs = [[0.371981, 0.001416]]\n",
      "Generation 22: Best costs = [[0.371981, 0.00124]]\n",
      "Generation 37: Best costs = [[0.371981, 0.001287]]\n",
      "Generation 23: Best costs = [[0.371981, 0.001137]]\n",
      "Generation 38: Best costs = [[0.371981, 0.001216]]\n",
      "Generation 24: Best costs = [[0.371981, 0.001137]]\n",
      "Generation 39: Best costs = [[0.371981, 0.001216]]\n",
      "Generation 25: Best costs = [[0.371981, 0.000959]]\n",
      "Generation 40: Best costs = [[0.371981, 0.001216]]\n",
      "Generation 26: Best costs = [[0.371981, 0.000892]]\n",
      "Generation 41: Best costs = [[0.371981, 0.001216]]\n",
      "Generation 27: Best costs = [[0.371981, 0.000889]]\n",
      "Generation 42: Best costs = [[0.371981, 0.001126]]\n",
      "Generation 28: Best costs = [[0.371981, 0.000807]]\n",
      "Generation 43: Best costs = [[0.371981, 0.001121]]\n",
      "Generation 29: Best costs = [[0.371981, 0.000796]]\n",
      "Generation 30: Best costs = [[0.371981, 0.000789]]\n",
      "Generation 44: Best costs = [[0.371981, 0.001121]]\n",
      "Generation 31: Best costs = [[0.371981, 0.000782]]\n",
      "Generation 45: Best costs = [[0.371981, 0.001094]]\n",
      "Generation 32: Best costs = [[0.371981, 0.000766]]\n",
      "Generation 46: Best costs = [[0.371981, 0.001088]]\n",
      "Generation 33: Best costs = [[0.371981, 0.000762]]\n",
      "Generation 47: Best costs = [[0.371981, 0.001044]]\n",
      "Generation 34: Best costs = [[0.371981, 0.000757]]\n",
      "Generation 48: Best costs = [[0.371981, 0.001037]]\n"
     ]
    }
   ],
   "source": [
    "import kaggle_support as kgs\n",
    "kgs.profiling = False\n",
    "\n",
    "def run_single_seed(seed, which_runner, fast_mode, git_commit_id, output_dir, result_queue):\n",
    "    \"\"\"Worker function to run a single seed in a separate process\"\"\"\n",
    "    # Import everything needed in this worker process\n",
    "    import sys\n",
    "    sys.path.append('../core')\n",
    "    sys.path.append('/packing/code/core/')\n",
    "    \n",
    "    import numpy as np\n",
    "    import time\n",
    "    import copy\n",
    "    import os\n",
    "    import dill\n",
    "    \n",
    "    try:\n",
    "        # Set CUDA device based on process to avoid conflicts\n",
    "        # This helps if you have multiple GPUs\n",
    "        try:\n",
    "            import cupy as cp\n",
    "            # Use modulo to cycle through available GPUs\n",
    "            n_gpus = cp.cuda.runtime.getDeviceCount()\n",
    "            device_id = seed % n_gpus\n",
    "            cp.cuda.Device(device_id).use()\n",
    "            # Small delay to stagger CUDA initialization\n",
    "            time.sleep(0.5)\n",
    "            print(f'Seed {seed} using GPU {device_id}')\n",
    "        except Exception as e:\n",
    "            print(f'Warning: Could not set CUDA device for seed {seed}: {e}')\n",
    "        \n",
    "        print(f'\\n=== Starting seed {seed} (Process {os.getpid()}) ===')\n",
    "        \n",
    "        r = which_runner(fast_mode=fast_mode)\n",
    "        r.seed = seed\n",
    "        r.base_ga.ga.ga_base.N_trees_to_do = 40\n",
    "        \n",
    "        # Check if this experiment already exists\n",
    "        base_filename = f\"{r.label}_{r.seed}_{git_commit_id[:8]}\"\n",
    "        if fast_mode:\n",
    "            base_filename += '_fast'\n",
    "        \n",
    "        # Run the experiment\n",
    "        start_time = time.time()\n",
    "        r.run()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        if r.exception is not None:\n",
    "            print(f\"ERROR occurred in seed {seed}: {r.exception[:200]}\")\n",
    "            result_queue.put({'seed': seed, 'success': False, 'error': r.exception[:200]})\n",
    "            return\n",
    "        \n",
    "        # Get best costs for each configuration (tree sizes, etc.)\n",
    "        # r.best_costs is now shape (n_generations, n_configs) from pack_ga2 structure\n",
    "        best_costs_final = r.best_costs[-1, :]\n",
    "        print(f\"\\nSeed {seed} completed in {elapsed_time:.1f}s\")\n",
    "        print(f\"Best final costs: {best_costs_final}\")\n",
    "        print(f\"Modifier values: {r.modifier_values}\")\n",
    "        \n",
    "        # Create score string from average best cost\n",
    "        avg_cost = np.mean(best_costs_final)\n",
    "        score_str = f\"{avg_cost:.4f}\".replace('.', '_')\n",
    "        \n",
    "        # Save full version (with populations)\n",
    "        output_file_full = output_dir + 'full/' + base_filename + '_f.pkl'\n",
    "        with open(output_file_full, 'wb') as f:\n",
    "            dill.dump(r, f)\n",
    "        print(f\"Saved full: {output_file_full}\")\n",
    "        \n",
    "        # Save abbreviated version (without populations to save space)\n",
    "        r_abbr = copy.deepcopy(r)\n",
    "        #r_abbr.result_ga.ga.champions = []  # Clear champions to save space\n",
    "        r_abbr.result_ga.ga.abbreviate()\n",
    "        output_file_abbr = output_dir + 'abbr/' + base_filename + '_a.pkl'\n",
    "        with open(output_file_abbr, 'wb') as f:\n",
    "            dill.dump(r_abbr, f)\n",
    "        print(f\"Saved abbr: {output_file_abbr}\")\n",
    "        \n",
    "        # Put result data in queue for plotting\n",
    "        result_queue.put({\n",
    "            'seed': seed,\n",
    "            'success': True,\n",
    "            'label': r.label,\n",
    "            'best_costs': r.best_costs.copy(),\n",
    "            'n_configs': r.best_costs.shape[1],  # Number of configurations (tree sizes, etc.)\n",
    "            'avg_cost': avg_cost,\n",
    "            'elapsed_time': elapsed_time\n",
    "        })\n",
    "        \n",
    "        print(f\"Seed {seed} finished successfully!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_msg = traceback.format_exc()\n",
    "        print(f\"EXCEPTION in seed {seed}:\\n{error_msg}\")\n",
    "        result_queue.put({'seed': seed, 'success': False, 'error': str(e)})\n",
    "\n",
    "\n",
    "# Main parallel execution\n",
    "n_seeds = 1000 if not fast_mode else 10\n",
    "result_queue = Queue()\n",
    "active_processes = []\n",
    "completed_results = []\n",
    "seeds_to_run = [a+4000 for a in list(range(n_seeds))]\n",
    "next_seed_idx = 0\n",
    "\n",
    "# Limit parallel processes if using GPU (to avoid memory issues)\n",
    "# You may want to adjust this based on your GPU memory\n",
    "import cupy as cp\n",
    "n_gpus = cp.cuda.runtime.getDeviceCount()\n",
    "print(f\"Detected {n_gpus} GPU(s)\")\n",
    "\n",
    "# Adjust parallel processes - too many can overwhelm GPU memory\n",
    "# Start with 1-2 processes per GPU, adjust based on your GPU memory\n",
    "max_parallel = n_parallel_processes\n",
    "print(f\"Starting parallel execution of {n_seeds} seeds using {max_parallel} processes\")\n",
    "print(f\"Staggering process startup by 2 seconds each...\\n\")\n",
    "\n",
    "# Start initial batch of processes with staggered startup\n",
    "stagger_delay = 5.0  # seconds between each process start\n",
    "for i in range(min(max_parallel, n_seeds)):\n",
    "    seed = seeds_to_run[next_seed_idx]\n",
    "    print(f\"Starting process for seed {seed}...\")\n",
    "    p = Process(target=run_single_seed, args=(seed, which_runner, fast_mode, git_commit_id, output_dir, result_queue))\n",
    "    p.start()\n",
    "    active_processes.append((p, seed))\n",
    "    next_seed_idx += 1\n",
    "    \n",
    "    # Stagger the startup to avoid CUDA initialization conflicts\n",
    "    if i < min(max_parallel, n_seeds) - 1:  # Don't sleep after the last one\n",
    "        time.sleep(stagger_delay)\n",
    "\n",
    "print(f\"\\nAll initial processes started. Monitoring for completion...\\n\")\n",
    "\n",
    "# Track when we last started a process to stagger new starts\n",
    "last_process_start_time = time.time()\n",
    "\n",
    "# Main loop: wait for processes to complete and start new ones\n",
    "while active_processes or not result_queue.empty():\n",
    "    # Check for completed results in queue\n",
    "    while not result_queue.empty():\n",
    "        result = result_queue.get()\n",
    "        completed_results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"\\n*** Result received for seed {result['seed']}: avg_cost = {result['avg_cost']:.6f}, time = {result['elapsed_time']:.1f}s ***\\n\")\n",
    "            \n",
    "            # Plot convergence for this seed immediately\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            for i_config in range(result['n_configs']):\n",
    "                ax.plot(result['best_costs'][:, i_config], label=f'Config {i_config}')\n",
    "            ax.set_xlabel('Generation')\n",
    "            ax.set_ylabel('Best Cost')\n",
    "            ax.set_title(f\"Seed {result['seed']} - Final cost: {result['avg_cost']:.6f} (Time: {result['elapsed_time']:.1f}s)\")\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\n*** Seed {result['seed']} failed: {result.get('error', 'Unknown error')} ***\\n\")\n",
    "    \n",
    "    # Check for finished processes and start new ones\n",
    "    for i in range(len(active_processes) - 1, -1, -1):\n",
    "        p, seed = active_processes[i]\n",
    "        if not p.is_alive():\n",
    "            p.join()\n",
    "            active_processes.pop(i)\n",
    "            \n",
    "            # Start a new process if there are more seeds to run\n",
    "            if next_seed_idx < n_seeds:\n",
    "                new_seed = seeds_to_run[next_seed_idx]\n",
    "                \n",
    "                # Stagger new process starts to avoid CUDA conflicts\n",
    "                time_since_last_start = time.time() - last_process_start_time\n",
    "                if time_since_last_start < stagger_delay:\n",
    "                    sleep_time = stagger_delay - time_since_last_start\n",
    "                    print(f\"Waiting {sleep_time:.1f}s before starting seed {new_seed}...\")\n",
    "                    time.sleep(sleep_time)\n",
    "                \n",
    "                print(f\"Starting process for seed {new_seed}...\")\n",
    "                new_p = Process(target=run_single_seed, args=(new_seed, which_runner, fast_mode, git_commit_id, output_dir, result_queue))\n",
    "                new_p.start()\n",
    "                active_processes.append((new_p, new_seed))\n",
    "                last_process_start_time = time.time()\n",
    "                next_seed_idx += 1\n",
    "    \n",
    "    # Small sleep to avoid busy waiting\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"All {n_seeds} seeds completed!\")\n",
    "print(f\"Successful runs: {sum(1 for r in completed_results if r['success'])}\")\n",
    "print(f\"Failed runs: {sum(1 for r in completed_results if not r['success'])}\")\n",
    "\n",
    "# Create summary plots for all successful runs\n",
    "successful_results = [r for r in completed_results if r['success']]\n",
    "if successful_results:\n",
    "    print(f\"\\nCreating summary plots for {len(successful_results)} successful runs...\")\n",
    "    \n",
    "    # Plot all convergence curves overlaid\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    for result in successful_results:\n",
    "        for i_config in range(result['n_configs']):\n",
    "            ax.plot(result['best_costs'][:, i_config], alpha=0.3, linewidth=0.5, color='blue')\n",
    "    \n",
    "    ax.set_xlabel('Generation')\n",
    "    ax.set_ylabel('Best Cost')\n",
    "    ax.set_title(f'GA Convergence - All {len(successful_results)} Runs (Overlaid)')\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot distribution of final costs\n",
    "    final_costs = [r['avg_cost'] for r in successful_results]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.hist(final_costs, bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(np.mean(final_costs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(final_costs):.6f}')\n",
    "    ax.axvline(np.min(final_costs), color='green', linestyle='--', linewidth=2, label=f'Best: {np.min(final_costs):.6f}')\n",
    "    ax.set_xlabel('Average Final Cost')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'Distribution of Final Costs ({len(successful_results)} runs)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal Statistics:\")\n",
    "    print(f\"  Best cost:  {np.min(final_costs):.6f}\")\n",
    "    print(f\"  Worst cost: {np.max(final_costs):.6f}\")\n",
    "    print(f\"  Mean cost:  {np.mean(final_costs):.6f}\")\n",
    "    print(f\"  Std cost:   {np.std(final_costs):.6f}\")\n",
    "    print(f\"  Median cost: {np.median(final_costs):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load and analyze results\n",
    "results_files = sorted(glob.glob(output_dir + 'abbr/*.pkl'))\n",
    "print(f\"Found {len(results_files)} result files\")\n",
    "\n",
    "if len(results_files) > 0:\n",
    "    # Load all results\n",
    "    results = []\n",
    "    for f in results_files:\n",
    "        with open(f, 'rb') as fp:\n",
    "            results.append(dill.load(fp))\n",
    "    \n",
    "    # Extract hyperparameters and final costs\n",
    "    hyperparams = []\n",
    "    final_costs = []\n",
    "    \n",
    "    for r in results:\n",
    "        if r.exception is None and r.best_costs is not None:\n",
    "            hyperparams.append(r.modifier_values)\n",
    "            # r.best_costs is shape (n_generations, n_configs)\n",
    "            # Average across all configs for overall score\n",
    "            final_costs.append(np.mean(r.best_costs[-1, :]))\n",
    "    \n",
    "    print(f\"\\nSuccessfully completed runs: {len(final_costs)}\")\n",
    "    if len(final_costs) > 0:\n",
    "        print(f\"Best average cost: {np.min(final_costs):.6f}\")\n",
    "        print(f\"Worst average cost: {np.max(final_costs):.6f}\")\n",
    "        print(f\"Mean average cost: {np.mean(final_costs):.6f}\")\n",
    "        print(f\"Std average cost: {np.std(final_costs):.6f}\")\n",
    "        \n",
    "        # Plot distribution of final costs\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(final_costs, bins=20, edgecolor='black')\n",
    "        plt.xlabel('Average Final Cost')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Distribution of Final Costs Across Hyperparameter Settings')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
